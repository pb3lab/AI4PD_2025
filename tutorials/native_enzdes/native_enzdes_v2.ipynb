{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4yBrceuFbf3"
      },
      "source": [
        "# **Native sequence redesign with ProteinMPNN and evolutionary information (v2)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pb3lab/AI4PD_2025/blob/main/tutorials/native_enzdes/native_enzdes_v2.ipynb)"
      ],
      "metadata": {
        "id": "_49KeNzjry_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we will exemplify how to combine ProteinMPNN with experimental information about active site residues and evolutionary information â€“ in the form conservation of residues at difference sequence identity percentages from a multiple sequence alignment (MSA) â€“ to perform **native sequence redesign of natural enzymes**.\n",
        "\n",
        "This tutorial is largely based on the breakthough article by **Kiera Sumida**, published in [JACS in 2024](https://pubs.acs.org/doi/10.1021/jacs.3c10941), with some key differences and similarities:\n",
        "1) As in the original article, active site residues are defined as residues containing backbone atoms within 7 Ã… or sidechains atoms within 6 Ã… of the ligand.\n",
        "2) We are using an MSA generated using [MMseqs2](https://github.com/soedinglab/MMseqs2) instead of [HHblits](https://github.com/soedinglab/hh-suite) for retrieven homologous enzymes to generate an MSA. The primary reason for using MMseqs2 over HHblits is its massive speed advantage.\n",
        "3) In the original article, four iterative HHblits searches were performed against the UniRef30 database at E-value cutoffs of 1e-50, 1e-30, 1e-10 and 1e-4. Here, the database being used by MMseqs2 are built from extensive sequence sets like UniRef30 and other environmental sequences\n",
        "4) We maintained the filtering of the sequences in the MSA at 90% sequence identity, 50% coverage and 30% minimum query identity.\n",
        "5) In the original article, each position in the MSA was ranked based on how highly conserved the most frequent amino acid identity was, selecting the top 30%, 50%, and 70% most conserved positions to fix. Here, we are only fixing the top 70% most conserved positions.\n",
        "6) We are using the same ProteinMPNN model, trained with 0.2 Ã… applied to the training set of protein bacbones.\n",
        "7) Three sampling temperatures were tried for ProteinMPNN during the protein sequence generation stage (0.1, 0.2, 0.3), whereas only one 0.2 is used in this tutorial.\n",
        "8) Only 4 sequences are being generated, whereas 144 sequences were generated in the original article.\n",
        "9) Only model 4 is being used for the AlphaFold predictions, as in the original article, but we are only filtering candidates based on pLDDT >85, and not on CÎ± RMSD < 2 Ã…. This will be added in the future."
      ],
      "metadata": {
        "id": "7NPqtfGm5jgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0. Install the different packages required to run this tutorial\n",
        "\n",
        "### **Please install all the different dependencies at the beginning of the tutorial in the order they are indicated in this notebook.**"
      ],
      "metadata": {
        "id": "bB9h6Pn8BAzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Install ProteinMPNN\n",
        "!pip install jupyter_bokeh --quiet\n",
        "import json, time, os, sys, glob\n",
        "\n",
        "if not os.path.isdir(\"ProteinMPNN\"):\n",
        "  os.system(\"git clone -q https://github.com/dauparas/ProteinMPNN.git\")\n",
        "sys.path.append('/content/ProteinMPNN')"
      ],
      "metadata": {
        "id": "sEi7JwX4dWaV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Setup ProteinMPNN model\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split, Subset\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import os.path\n",
        "from protein_mpnn_utils import loss_nll, loss_smoothed, gather_edges, gather_nodes, gather_nodes_t, cat_neighbors_nodes, _scores, _S_to_seq, tied_featurize, parse_PDB\n",
        "from protein_mpnn_utils import StructureDataset, StructureDatasetPDB, ProteinMPNN\n",
        "\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "#v_48_010=version with 48 edges 0.10A noise\n",
        "model_name = \"v_48_020\" #@param [\"v_48_002\", \"v_48_010\", \"v_48_020\", \"v_48_030\"]\n",
        "\n",
        "\n",
        "backbone_noise=0.00               # Standard deviation of Gaussian noise to add to backbone atoms\n",
        "\n",
        "path_to_model_weights='/content/ProteinMPNN/vanilla_model_weights'\n",
        "hidden_dim = 128\n",
        "num_layers = 3\n",
        "model_folder_path = path_to_model_weights\n",
        "if model_folder_path[-1] != '/':\n",
        "    model_folder_path = model_folder_path + '/'\n",
        "checkpoint_path = model_folder_path + f'{model_name}.pt'\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "print('Number of edges:', checkpoint['num_edges'])\n",
        "noise_level_print = checkpoint['noise_level']\n",
        "print(f'Training noise level: {noise_level_print}A')\n",
        "model = ProteinMPNN(num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
        "model.to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(\"Model loaded\")"
      ],
      "metadata": {
        "id": "HNvZ8PSydflj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Helper functions for ProteinMPNN\n",
        "def make_tied_positions_for_homomers(pdb_dict_list):\n",
        "    my_dict = {}\n",
        "    for result in pdb_dict_list:\n",
        "        all_chain_list = sorted([item[-1:] for item in list(result) if item[:9]=='seq_chain']) #A, B, C, ...\n",
        "        tied_positions_list = []\n",
        "        chain_length = len(result[f\"seq_chain_{all_chain_list[0]}\"])\n",
        "        for i in range(1,chain_length+1):\n",
        "            temp_dict = {}\n",
        "            for j, chain in enumerate(all_chain_list):\n",
        "                temp_dict[chain] = [i] #needs to be a list\n",
        "            tied_positions_list.append(temp_dict)\n",
        "        my_dict[result['name']] = tied_positions_list\n",
        "    return my_dict"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p8UCmlCnvIah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4) Setup ColabFold without AMBER (i.e. without relax)\n",
        "\n",
        "from sys import version_info\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "use_amber = False\n",
        "use_templates = True\n",
        "python_version = python_version"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uubxVRG9JjHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5) Install dependencies for running MMseqs2 and ColabFold\n",
        "%%time\n",
        "%%bash -s $use_amber $use_templates $python_version\n",
        "\n",
        "set -e\n",
        "\n",
        "USE_AMBER=$1\n",
        "USE_TEMPLATES=$2\n",
        "PYTHON_VERSION=$3\n",
        "\n",
        "if [ ! -f COLABFOLD_READY ]; then\n",
        "  # install dependencies\n",
        "  # We have to use \"--no-warn-conflicts\" because colab already has a lot preinstalled with requirements different to ours\n",
        "  pip install -q --no-warn-conflicts \"colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold\"\n",
        "  if [ -n \"${TPU_NAME}\" ]; then\n",
        "    pip install -q --no-warn-conflicts -U dm-haiku==0.0.10 jax==0.3.25\n",
        "  fi\n",
        "  ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\n",
        "  ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\n",
        "  # hack to fix TF crash\n",
        "  rm -f /usr/local/lib/python3.*/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so\n",
        "  touch COLABFOLD_READY\n",
        "fi\n",
        "\n",
        "# Download params (~1min)\n",
        "python -m colabfold.download\n",
        "\n",
        "# setup conda\n",
        "if [ ${USE_AMBER} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
        "  if [ ! -f CONDA_READY ]; then\n",
        "    wget -qnc https://github.com/conda-forge/miniforge/releases/download/25.3.1-0/Miniforge3-25.3.1-0-Linux-x86_64.sh\n",
        "    bash Miniforge3-25.3.1-0-Linux-x86_64.sh -bfp /usr/local 2>&1 1>/dev/null\n",
        "    rm Miniforge3-25.3.1-0-Linux-x86_64.sh\n",
        "    conda config --set auto_update_conda false\n",
        "    touch CONDA_READY\n",
        "  fi\n",
        "fi\n",
        "# setup template search\n",
        "if [ ${USE_TEMPLATES} == \"True\" ] && [ ! -f HH_READY ]; then\n",
        "  conda install -y -q -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python=\"${PYTHON_VERSION}\" 2>&1 1>/dev/null\n",
        "  touch HH_READY\n",
        "fi\n",
        "# setup openmm for amber refinement\n",
        "if [ ${USE_AMBER} == \"True\" ] && [ ! -f AMBER_READY ]; then\n",
        "  conda install -y -q -c conda-forge openmm=8.2.0 python=\"${PYTHON_VERSION}\" pdbfixer 2>&1 1>/dev/null\n",
        "  touch AMBER_READY\n",
        "fi"
      ],
      "metadata": {
        "id": "AzIKiDiCaHAn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Once you have finished installing these dependencies, we are ready to perform the tutorial**"
      ],
      "metadata": {
        "id": "zps1RWW0BlKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. Determine the active site residues and highly conserved residues of the enzyme to fix during sequence redesign\n",
        "\n",
        "### Please follow the steps in the order they are presented in this tutorial"
      ],
      "metadata": {
        "id": "7WZL0b3nCZ4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Load Structure, Extract Sequence, and Split Protein and Ligand Chains\n",
        "#@markdown ---\n",
        "#@markdown ### 1. PDB Input\n",
        "#@markdown Choose to download from RCSB or upload your own file.\n",
        "pdb_id = 'AZO1' #@param {type:\"string\"}\n",
        "use_upload = True #@param {type:\"boolean\"}\n",
        "#@markdown > If uploading, the script will assume the first chain is the protein.\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Protein (Target) Chain\n",
        "#@markdown Provide the chain letter for your main protein target.\n",
        "target_chain_letter = 'A' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 3. Ligand\n",
        "#@markdown Choose your method for extracting the ligand.\n",
        "ligand_extraction_method = \"By HETATM Residue Name\" #@param [\"By Chain\", \"By HETATM Residue Name\"]\n",
        "#@markdown **If 'By Chain':** Provide chain letter.\n",
        "#@markdown **If 'By HETATM Residue Name':** Provide the chain letter where the ligand(s) are located, and their 3-letter name(s).\n",
        "ligand_chain_letter = 'C' #@param {type:\"string\"}\n",
        "ligand_residue_name = 'NAP' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import warnings\n",
        "\n",
        "try:\n",
        "    from Bio.PDB import (\n",
        "        PDBParser, PDBIO, Select,\n",
        "        Polypeptide, CaPPBuilder, PDBExceptions\n",
        "    )\n",
        "    from Bio.Seq import Seq\n",
        "    from Bio.SeqRecord import SeqRecord\n",
        "    from Bio import SeqIO\n",
        "except ImportError:\n",
        "    print(\"Biopython not found. Installing...\")\n",
        "    !pip install biopython\n",
        "    from Bio.PDB import (\n",
        "        PDBParser, PDBIO, Select,\n",
        "        Polypeptide, CaPPBuilder, PDBExceptions\n",
        "    )\n",
        "    from Bio.Seq import Seq\n",
        "    from Bio.SeqRecord import SeqRecord\n",
        "    from Bio import SeqIO\n",
        "\n",
        "# Suppress Biopython warnings\n",
        "warnings.filterwarnings(\"ignore\", category=PDBExceptions.PDBConstructionWarning)\n",
        "\n",
        "# --- 0A. Python-based renumbering function for the protein ---\n",
        "def renumber_protein_pdb(filename):\n",
        "    \"\"\"\n",
        "    Reads a PDB file, keeps only ATOM and TER lines, and renumbers\n",
        "    residues sequentially from 1, incrementing on 'N' atoms.\n",
        "    \"\"\"\n",
        "    renumbered_lines = []\n",
        "    current_res_num = 0\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            if line.startswith(\"ATOM\"):\n",
        "                atom_name = line[12:16].strip()\n",
        "\n",
        "                # Increment residue number if this is a Nitrogen atom\n",
        "                # This mimics the `if( $3==\"N\" ) ++num;` logic\n",
        "                if atom_name == \"N\":\n",
        "                    current_res_num += 1\n",
        "\n",
        "                # Safety check if PDB doesn't start with N\n",
        "                if current_res_num == 0:\n",
        "                    current_res_num = 1\n",
        "\n",
        "                new_res_num_str = str(current_res_num).rjust(4)\n",
        "\n",
        "                # Splice in the new res num\n",
        "                new_line = line[:22] + new_res_num_str + line[26:]\n",
        "                renumbered_lines.append(new_line)\n",
        "\n",
        "            elif line.startswith(\"TER\"):\n",
        "                renumbered_lines.append(line)\n",
        "\n",
        "        # Overwrite the original file\n",
        "        with open(filename, 'w') as f:\n",
        "            f.writelines(renumbered_lines)\n",
        "\n",
        "        print(f\"âœ… Successfully filtered and renumbered {filename}\")\n",
        "        return current_res_num # Return the last residue number used\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸ”¥ Error renumbering protein {filename}: {e}\")\n",
        "        return 0\n",
        "\n",
        "# --- 0B. Python-based renumbering function for ligands ---\n",
        "def renumber_and_filter_ligand(filename, start_res_num, new_chain_letter, is_last_file=False):\n",
        "    \"\"\"\n",
        "    Reads a PDB file, filters for ATOM/HETATM/TER, renumbers residues,\n",
        "    and sets a new chain ID. Adds END instead of TER if it's the last file.\n",
        "    \"\"\"\n",
        "    renumbered_lines = []\n",
        "    current_offset = -1 # Will be 0 on the first new residue\n",
        "    last_original_res_num_str = None\n",
        "\n",
        "    # Ensure chain letter is a single character\n",
        "    if not isinstance(new_chain_letter, str) or len(new_chain_letter) != 1:\n",
        "        print(f\"âš ï¸ Invalid chain letter '{new_chain_letter}'. Defaulting to 'X'.\")\n",
        "        new_chain_letter = 'X'\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            if line.startswith(\"ATOM\") or line.startswith(\"HETATM\"):\n",
        "                original_res_num_str = line[22:26] # Keep as string for comparison\n",
        "\n",
        "                if original_res_num_str != last_original_res_num_str:\n",
        "                    current_offset += 1 # Increment for each new residue\n",
        "                    last_original_res_num_str = original_res_num_str\n",
        "\n",
        "                new_res_num = start_res_num + current_offset\n",
        "                new_res_num_str = str(new_res_num).rjust(4)\n",
        "\n",
        "                # Splice in the new chain ID and new res num\n",
        "                new_line = line[:21] + new_chain_letter + new_res_num_str + line[26:]\n",
        "                renumbered_lines.append(new_line)\n",
        "\n",
        "            elif line.startswith(\"TER\"):\n",
        "                # Only append TER if it's NOT the last file\n",
        "                if not is_last_file:\n",
        "                    renumbered_lines.append(line)\n",
        "\n",
        "        # After processing all lines, add END if it IS the last file\n",
        "        if is_last_file:\n",
        "            renumbered_lines.append(\"END\\n\")\n",
        "\n",
        "        # Overwrite the original file\n",
        "        with open(filename, 'w') as f:\n",
        "            f.writelines(renumbered_lines)\n",
        "\n",
        "        final_res_num = start_res_num + current_offset\n",
        "        print(f\"âœ… Successfully filtered and renumbered {filename} (Chain {new_chain_letter}, Res {start_res_num}-{final_res_num})\")\n",
        "        return final_res_num # Return the last residue number used\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸ”¥ Error renumbering {filename}: {e}\")\n",
        "        return start_res_num - 1\n",
        "\n",
        "# --- Biopython Selectors ---\n",
        "class ProteinSelect(Select):\n",
        "    \"\"\" Selects only the ATOM records for a specific protein chain. \"\"\"\n",
        "    def __init__(self, chain_id):\n",
        "        self.chain_id = chain_id\n",
        "\n",
        "    def accept_chain(self, chain):\n",
        "        return chain.id == self.chain_id\n",
        "\n",
        "    def accept_residue(self, residue):\n",
        "        # Only accept standard residues (no HETATMs)\n",
        "        return residue.id[0] == ' '\n",
        "\n",
        "    def accept_atom(self, atom):\n",
        "        return True\n",
        "\n",
        "class LigandChainSelect(Select):\n",
        "    \"\"\" Selects all ATOM/HETATM records for a specific ligand chain. \"\"\"\n",
        "    def __init__(self, chain_id):\n",
        "        self.chain_id = chain_id\n",
        "\n",
        "    def accept_chain(self, chain):\n",
        "        return chain.id == self.chain_id\n",
        "\n",
        "    def accept_residue(self, residue):\n",
        "        return True # Accept both ATOM and HETATM\n",
        "\n",
        "    def accept_atom(self, atom):\n",
        "        return True\n",
        "\n",
        "class HetatmResidueSelect(Select):\n",
        "    \"\"\" Selects only a specific HETATM residue. \"\"\"\n",
        "    def __init__(self, target_residue):\n",
        "        self.target_residue = target_residue\n",
        "\n",
        "    def accept_chain(self, chain):\n",
        "        return chain.id == self.target_residue.get_parent().id\n",
        "\n",
        "    def accept_residue(self, residue):\n",
        "        return residue == self.target_residue\n",
        "\n",
        "    def accept_atom(self, atom):\n",
        "        return True\n",
        "\n",
        "# --- 1. Handle PDB Input (Upload vs. Download) ---\n",
        "full_pdb_filename = \"\"\n",
        "pdb_id_base = \"\"\n",
        "\n",
        "if use_upload:\n",
        "    print(\"Please upload your PDB file...\")\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            raise Exception(\"File upload cancelled.\")\n",
        "\n",
        "        uploaded_filename = next(iter(uploaded)) # Get the first filename\n",
        "        pdb_content = uploaded[uploaded_filename].decode('utf-8') # Get the file content\n",
        "\n",
        "        full_pdb_filename = \"uploaded_file.pdb\"\n",
        "        with open(full_pdb_filename, 'w') as f:\n",
        "            f.write(pdb_content) # Write the content to a local file\n",
        "\n",
        "        pdb_id_base = os.path.splitext(uploaded_filename)[0] # Use file name as base\n",
        "        print(f\"Using uploaded file: {uploaded_filename} (saved as {full_pdb_filename})\")\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸ”¥ Error during file upload: {e}\")\n",
        "        raise SystemExit(\"File upload failed.\")\n",
        "else:\n",
        "    pdb_id_base = pdb_id\n",
        "    full_pdb_filename = f\"{pdb_id_base}.pdb\"\n",
        "    if not os.path.isfile(full_pdb_filename):\n",
        "        print(f\"Downloading {full_pdb_filename} from RCSB PDB...\")\n",
        "        !wget -q -O {full_pdb_filename} https://files.rcsb.org/download/{full_pdb_filename}\n",
        "        print(f\"Successfully downloaded {full_pdb_filename}.\")\n",
        "    else:\n",
        "        print(f\"{full_pdb_filename} already exists. Using local file.\")\n",
        "\n",
        "# --- 2. Define Output Filenames ---\n",
        "target_fasta_filename = \"protein.fasta\"\n",
        "target_pdb_filename = \"protein.pdb\"\n",
        "\n",
        "# --- 3. Load Structure and Process Chains ---\n",
        "if full_pdb_filename and os.path.exists(full_pdb_filename):\n",
        "    try:\n",
        "        print(f\"\\nLoading full structure from {full_pdb_filename}...\")\n",
        "        parser = PDBParser(QUIET=True)\n",
        "        structure = parser.get_structure(pdb_id_base, full_pdb_filename)\n",
        "        model = structure[0] # Assume first model\n",
        "        last_protein_res_num = 0\n",
        "\n",
        "        # --- 3A. Process Target Chain (Protein) ---\n",
        "        print(f\"\\nProcessing target chain: {target_chain_letter}\")\n",
        "\n",
        "        if target_chain_letter not in model:\n",
        "            if use_upload:\n",
        "                original_letter = target_chain_letter\n",
        "                target_chain_letter = next(iter(model.child_dict.keys()))\n",
        "                print(f\"Warning: Chain '{original_letter}' not found. Using first chain '{target_chain_letter}' as protein.\")\n",
        "            else:\n",
        "                print(f\"âš ï¸ Error: Target protein chain '{target_chain_letter}' not found in PDB. Stopping.\")\n",
        "                raise SystemExit()\n",
        "\n",
        "        protein_chain = model[target_chain_letter]\n",
        "\n",
        "        # Get sequence\n",
        "        sequence = \"\"\n",
        "        for res in protein_chain.get_residues():\n",
        "            # Check if it's a standard amino acid\n",
        "            if res.id[0] == ' ' and Polypeptide.is_aa(res.get_resname(), standard=True):\n",
        "                try:\n",
        "                    # FIX: Use the protein_letters_3to1 dictionary instead of the old function\n",
        "                    sequence += Polypeptide.protein_letters_3to1[res.get_resname()]\n",
        "                except KeyError:\n",
        "                    sequence += 'X' # Unknown residue\n",
        "\n",
        "        # MODIFIED: Set description to \"\" to only get \">protein\"\n",
        "        seq_record = SeqRecord(Seq(sequence), id=\"protein\", description=\"\")\n",
        "        with open(target_fasta_filename, 'w') as f:\n",
        "            SeqIO.write(seq_record, f, \"fasta\")\n",
        "        print(f\"âœ… Successfully saved sequence to {target_fasta_filename}\")\n",
        "\n",
        "        # Save protein structure (ATOM lines only)\n",
        "        io = PDBIO()\n",
        "        io.set_structure(structure)\n",
        "        io.save(target_pdb_filename, ProteinSelect(target_chain_letter))\n",
        "        print(f\"âœ… Successfully saved structure to {target_pdb_filename}\")\n",
        "\n",
        "        # Renumber protein PDB\n",
        "        last_protein_res_num = renumber_protein_pdb(target_pdb_filename)\n",
        "        if last_protein_res_num > 0:\n",
        "            print(f\"Protein renumbered. Last residue number is: {last_protein_res_num}\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Warning: Protein renumbering failed or protein was empty.\")\n",
        "\n",
        "        # --- 3B. Process Ligand(s) ---\n",
        "        current_ligand_res_start = last_protein_res_num + 1\n",
        "        start_chain_ord = ord(target_chain_letter.upper())\n",
        "        ligand_chain_counter = 1 # To assign B, C, D...\n",
        "\n",
        "        if ligand_extraction_method == \"By Chain\":\n",
        "            print(f\"\\nProcessing ligand by chain: {ligand_chain_letter}\")\n",
        "\n",
        "            if ligand_chain_letter not in model:\n",
        "                print(f\"âš ï¸ Error: Ligand chain '{ligand_chain_letter}' not found.\")\n",
        "            else:\n",
        "                ligand_pdb_filename = f\"{ligand_chain_letter}_ligand.pdb\"\n",
        "                io.save(ligand_pdb_filename, LigandChainSelect(ligand_chain_letter))\n",
        "                print(f\"âœ… Successfully saved structure to {ligand_pdb_filename}\")\n",
        "\n",
        "                # Renumber this ligand chain\n",
        "                current_new_chain_letter = chr(start_chain_ord + ligand_chain_counter)\n",
        "                renumber_and_filter_ligand(\n",
        "                    ligand_pdb_filename,\n",
        "                    current_ligand_res_start,\n",
        "                    current_new_chain_letter,\n",
        "                    is_last_file=True # Assume this is the only ligand\n",
        "                )\n",
        "\n",
        "        elif ligand_extraction_method == \"By HETATM Residue Name\":\n",
        "            if not ligand_residue_name:\n",
        "                print(\"âš ï¸ Error: Please provide a HETATM residue name to search for.\")\n",
        "            else:\n",
        "                ligand_names_to_find = [name.strip().upper() for name in ligand_residue_name.split(',')]\n",
        "                print(f\"\\nProcessing ligands by HETATM name(s): {ligand_names_to_find}\")\n",
        "\n",
        "                if ligand_chain_letter not in model:\n",
        "                     print(f\"âš ï¸ Error: Specified ligand chain '{ligand_chain_letter}' not found. Cannot search for HETATMs.\")\n",
        "                else:\n",
        "                    print(f\"Searching for HETATMs on chain '{ligand_chain_letter}'...\")\n",
        "\n",
        "                    search_chain = model[ligand_chain_letter]\n",
        "                    matching_residues = []\n",
        "\n",
        "                    for res in search_chain.get_residues():\n",
        "                        res_name_upper = res.get_resname().strip().upper()\n",
        "                        # Check if it's a HETATM and matches one of the names\n",
        "                        if res.id[0] != ' ' and res_name_upper in ligand_names_to_find:\n",
        "                            matching_residues.append(res)\n",
        "\n",
        "                    found_ligands = len(matching_residues)\n",
        "                    ligand_file_counts = {}\n",
        "\n",
        "                    if found_ligands == 0:\n",
        "                        print(f\"âš ï¸ No HETATM residues matching {ligand_names_to_find} found on chain {ligand_chain_letter}.\")\n",
        "                    else:\n",
        "                        print(f\"Found {found_ligands} matching HETATM residues. Processing...\")\n",
        "\n",
        "                        for i, res in enumerate(matching_residues):\n",
        "                            is_last = (i == found_ligands - 1) # Check if this is the last item\n",
        "                            res_name_upper = res.get_resname().strip().upper()\n",
        "\n",
        "                            # Determine chain letter\n",
        "                            current_new_chain_letter = chr(start_chain_ord + ligand_chain_counter)\n",
        "                            ligand_chain_counter += 1 # Increment for the *next* ligand\n",
        "\n",
        "                            # Handle file naming to prevent overwrites\n",
        "                            count = ligand_file_counts.get(res_name_upper, 0) + 1\n",
        "                            ligand_file_counts[res_name_upper] = count\n",
        "\n",
        "                            ligand_pdb_filename = f\"{res_name_upper}.pdb\"\n",
        "                            if count > 1:\n",
        "                                ligand_pdb_filename = f\"{res_name_upper}_{count}.pdb\"\n",
        "\n",
        "                            # Save this single residue to a file\n",
        "                            io.save(ligand_pdb_filename, HetatmResidueSelect(res))\n",
        "                            print(f\"âœ… Extracted HETATM {i+1}/{found_ligands} ({res_name_upper}) to {ligand_pdb_filename}\")\n",
        "\n",
        "                            # Call renumbering with the new chain letter and is_last flag\n",
        "                            last_num_used = renumber_and_filter_ligand(\n",
        "                                ligand_pdb_filename,\n",
        "                                current_ligand_res_start,\n",
        "                                current_new_chain_letter,\n",
        "                                is_last_file=is_last\n",
        "                            )\n",
        "\n",
        "                            # Update the *next* starting number\n",
        "                            current_ligand_res_start = last_num_used + 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸ”¥ Error during PDB processing: {e}\")\n",
        "else:\n",
        "    print(\"ðŸ”¥ Error: PDB file not found or specified. Halting.\")\n",
        "\n",
        "print(\"\\nProcess complete.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Kt2nJuU0HbOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import requests\n",
        "import subprocess\n",
        "from glob import glob\n",
        "import warnings\n",
        "\n",
        "# --- Configuration ---\n",
        "#@title 2) Define Active Site Residues Around Ligand\n",
        "#@markdown ### 1. Input Files\n",
        "protein_pdb_file = \"protein.pdb\"  #@param {type:\"string\"}\n",
        "ligand_pdb_files_str = \"NAP.pdb\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### 2. Ligand Parameters\n",
        "#@markdown **Important:** The chains here MUST match the chains you assigned\n",
        "#@markdown in the previous PDB splitting step (e.g., 'B', 'C', etc.)\n",
        "ligand_chains_str = \"B\"   #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### 3. Output File\n",
        "merged_pdb_file = \"protein_ligand_complex.pdb\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### 4. Active Site Definition\n",
        "#@markdown ---\n",
        "#@markdown Define the active site by two criteria:\n",
        "#@markdown 1.  Protein **backbone** atoms ('N', 'CA', 'C', 'O') within X angstroms of the ligand.\n",
        "#@markdown 2.  Protein **sidechain** atoms (all others) within Y angstroms of the ligand.\n",
        "cutoff_backbone = 7.0  #@param {type:\"number\"}\n",
        "cutoff_sidechain = 6.0  #@param {type:\"number\"}\n",
        "\n",
        "# --- 1. Install/Import Biopython ---\n",
        "try:\n",
        "    from Bio.PDB import PDBParser, NeighborSearch, PDBExceptions\n",
        "    from Bio.PDB.Atom import Atom\n",
        "except ImportError:\n",
        "    print(\"Biopython not found. Installing...\")\n",
        "    !pip install biopython\n",
        "    from Bio.PDB import PDBParser, NeighborSearch, PDBExceptions\n",
        "    from Bio.PDB.Atom import Atom\n",
        "\n",
        "# Suppress Biopython warnings\n",
        "warnings.filterwarnings(\"ignore\", category=PDBExceptions.PDBConstructionWarning)\n",
        "\n",
        "# --- 2. Merge Protein and Ligand PDBs ---\n",
        "# This step is necessary to create a single complex for Biopython to analyze\n",
        "print(\"\\n--- Merging PDB Files ---\")\n",
        "\n",
        "processed_ligand_pdbs = [f.strip() for f in ligand_pdb_files_str.split(',')]\n",
        "\n",
        "with open(merged_pdb_file, 'w') as outfile:\n",
        "    if os.path.exists(protein_pdb_file):\n",
        "        with open(protein_pdb_file, 'r') as infile:\n",
        "            for line in infile:\n",
        "                # Filter out CRYST1, REMARK, and any existing END\n",
        "                if not line.startswith((\"CRYST1\", \"REMARK\", \"END\")):\n",
        "                    outfile.write(line)\n",
        "        print(f\"Added {protein_pdb_file} to {merged_pdb_file}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Warning: Protein file {protein_pdb_file} not found. Merged file will only contain ligands.\")\n",
        "\n",
        "    # Add a TER record if protein was added and it didn't end with one\n",
        "    if os.path.exists(protein_pdb_file):\n",
        "         with open(protein_pdb_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            if lines and not lines[-1].startswith(\"TER\"):\n",
        "                outfile.write(\"TER\\n\")\n",
        "\n",
        "    for ligand_file in processed_ligand_pdbs:\n",
        "        if not os.path.exists(ligand_file):\n",
        "            print(f\"âš ï¸ Warning: Ligand file {ligand_file} not found. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        with open(ligand_file, 'r') as infile:\n",
        "            for line in infile:\n",
        "                 # Don't add the \"END\" line from the ligand, we'll add one at the very end\n",
        "                 if not line.startswith(\"END\"):\n",
        "                    outfile.write(line)\n",
        "        print(f\"Added {ligand_file} to {merged_pdb_file}\")\n",
        "\n",
        "    outfile.write(\"END\\n\")\n",
        "\n",
        "print(f\"âœ… Merged file created: {merged_pdb_file}\")\n",
        "\n",
        "\n",
        "# --- 3. Load Structure and Prepare Atoms ---\n",
        "print(\"\\n--- Loading Structure with Biopython ---\")\n",
        "try:\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"complex\", merged_pdb_file)\n",
        "    model = structure[0] # Assume first model\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ”¥ Error parsing merged PDB file: {e}\")\n",
        "    print(\"   Please check the merged file for errors.\")\n",
        "    raise SystemExit\n",
        "\n",
        "ligand_chains = [c.strip() for c in ligand_chains_str.split(',')]\n",
        "protein_chains = []\n",
        "for chain in model:\n",
        "    if chain.id not in ligand_chains:\n",
        "        protein_chains.append(chain.id)\n",
        "\n",
        "print(f\"Protein chains identified: {protein_chains}\")\n",
        "print(f\"Ligand chains identified: {ligand_chains}\")\n",
        "\n",
        "# Define backbone atom names\n",
        "backbone_atoms = {'N', 'CA', 'C', 'O'}\n",
        "\n",
        "ligand_atoms = []\n",
        "protein_bb_atoms = []\n",
        "protein_sc_atoms = []\n",
        "\n",
        "print(\"Separating protein (backbone/sidechain) and ligand atoms...\")\n",
        "for chain in model:\n",
        "    if chain.id in ligand_chains:\n",
        "        for atom in chain.get_atoms():\n",
        "            ligand_atoms.append(atom)\n",
        "\n",
        "    elif chain.id in protein_chains:\n",
        "        for residue in chain.get_residues():\n",
        "            # Only process standard protein residues\n",
        "            if residue.id[0] == ' ': # ' ' indicates a standard residue\n",
        "                for atom in residue.get_atoms():\n",
        "                    if atom.name in backbone_atoms:\n",
        "                        protein_bb_atoms.append(atom)\n",
        "                    else:\n",
        "                        protein_sc_atoms.append(atom)\n",
        "\n",
        "print(f\"Found {len(ligand_atoms)} ligand atoms.\")\n",
        "print(f\"Found {len(protein_bb_atoms)} protein backbone atoms.\")\n",
        "print(f\"Found {len(protein_sc_atoms)} protein sidechain atoms.\")\n",
        "\n",
        "if not ligand_atoms:\n",
        "    print(\"ðŸ”¥ Error: No ligand atoms found. Cannot proceed.\")\n",
        "    raise SystemExit\n",
        "\n",
        "# --- 4. Perform Neighbor Search ---\n",
        "print(\"Finding nearby residues...\")\n",
        "\n",
        "# Create the NeighborSearch object from all ligand atoms\n",
        "ns = NeighborSearch(ligand_atoms)\n",
        "\n",
        "# Find all protein backbone atoms within the backbone cutoff\n",
        "nearby_bb_atoms = set()\n",
        "for atom in protein_bb_atoms:\n",
        "    nearby = ns.search(atom.coord, cutoff_backbone, 'A') # 'A' = atom level\n",
        "    if nearby:\n",
        "        nearby_bb_atoms.add(atom)\n",
        "\n",
        "# Find all protein sidechain atoms within the sidechain cutoff\n",
        "nearby_sc_atoms = set()\n",
        "for atom in protein_sc_atoms:\n",
        "    nearby = ns.search(atom.coord, cutoff_sidechain, 'A') # 'A' = atom level\n",
        "    if nearby:\n",
        "        nearby_sc_atoms.add(atom)\n",
        "\n",
        "print(f\"Found {len(nearby_bb_atoms)} backbone atoms within {cutoff_backbone} Ã….\")\n",
        "print(f\"Found {len(nearby_sc_atoms)} sidechain atoms within {cutoff_sidechain} Ã….\")\n",
        "\n",
        "# --- 5. Map Atoms to Residues and Finalize List ---\n",
        "active_site_residues = set()\n",
        "\n",
        "# Get parent residues for backbone atoms\n",
        "for atom in nearby_bb_atoms:\n",
        "    active_site_residues.add(atom.get_parent())\n",
        "\n",
        "# Get parent residues for sidechain atoms\n",
        "for atom in nearby_sc_atoms:\n",
        "    active_site_residues.add(atom.get_parent())\n",
        "\n",
        "print(\"\\n--- Active Site Analysis Complete ---\")\n",
        "if not active_site_residues:\n",
        "    print(\"âš ï¸ No active site residues found with the given cutoffs.\")\n",
        "else:\n",
        "    print(f\"âœ… Found {len(active_site_residues)} unique residues in the active site.\")\n",
        "\n",
        "    # Sort residues by chain and residue number\n",
        "    sorted_residues = sorted(list(active_site_residues),\n",
        "                             key=lambda res: (res.get_parent().id, res.id[1]))\n",
        "\n",
        "    active_site_pdb_names = []\n",
        "\n",
        "    # CRITICAL: Create the global variable for the next script\n",
        "    # This must be a list of 1-indexed residue numbers\n",
        "    global active_site_residue_numbers\n",
        "    active_site_residue_numbers = []\n",
        "\n",
        "    print(\"Active Site Residues (Chain + PDB Number):\")\n",
        "    for res in sorted_residues:\n",
        "        chain_id = res.get_parent().id\n",
        "        res_num = res.id[1]\n",
        "        res_name = res.get_resname()\n",
        "\n",
        "        active_site_pdb_names.append(f\"{chain_id}{res_num} ({res_name})\")\n",
        "        active_site_residue_numbers.append(res_num)\n",
        "\n",
        "    print(\", \".join(active_site_pdb_names))\n",
        "\n",
        "    print(\"\\nCorresponding Residue Numbers (for next script):\")\n",
        "    print(active_site_residue_numbers)\n",
        "\n",
        "print(\"\\nðŸŽ¯ Script finished successfully.\")"
      ],
      "metadata": {
        "id": "7Q3yqLnD0o_M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Generate MSA (.a3m) and optionally filter based on different parameters\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Specify Input\n",
        "#@markdown Enter the name of the FASTA file you generated.\n",
        "input_fasta_file = 'protein.fasta' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 2. HHfilter Options\n",
        "#@markdown Check the box to run hhfilter on the generated .a3m file.\n",
        "run_hhfilter = True #@param {type:\"boolean\"}\n",
        "id_redundancy = 90 #@param {type:\"integer\"}\n",
        "coverage = 50 #@param {type:\"integer\"}\n",
        "query_identity = 30 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. Check if the input FASTA file exists\n",
        "if not os.path.isfile(input_fasta_file):\n",
        "    print(f\"ðŸ”¥ Error: Input file not found: {input_fasta_file}\")\n",
        "    print(\"Please make sure the filename matches the one from the previous step.\")\n",
        "    sys.exit(f\"File not found: {input_fasta_file}\")\n",
        "else:\n",
        "    print(f\"Found input file: {input_fasta_file}\")\n",
        "\n",
        "# 2. Define the output directory as the current folder\n",
        "output_dir = \".\" # This will save files to /content/\n",
        "\n",
        "# 3. Run the colabfold_batch command\n",
        "print(f\"Running colabfold_batch on {input_fasta_file}...\")\n",
        "print(f\"This will generate the .a3m file and then stop.\")\n",
        "\n",
        "# Run the alignment generation\n",
        "!colabfold_batch {input_fasta_file} {output_dir} --msa-mode \"mmseqs2_uniref_env\" --msa-only\n",
        "\n",
        "# 4. Check the output files from colabfold_batch\n",
        "print(\"\\nAlignment generation complete.\")\n",
        "\n",
        "# --- 5. Run HHfilter (New Step) ---\n",
        "base_name = os.path.splitext(input_fasta_file)[0]\n",
        "original_a3m = f\"{base_name}.a3m\"\n",
        "filtered_a3m = f\"{base_name}.filtered.a3m\"\n",
        "\n",
        "if run_hhfilter:\n",
        "    print(f\"\\nRunning hhfilter on {original_a3m}...\")\n",
        "    if not os.path.isfile(original_a3m):\n",
        "        print(f\"ðŸ”¥ Error: The original alignment file {original_a3m} was not found. Skipping filter.\")\n",
        "    else:\n",
        "        # Build and run the hhfilter command\n",
        "        !hhfilter -i {original_a3m} -o {filtered_a3m} -id {id_redundancy} -cov {coverage} -qid {query_identity}\n",
        "        print(f\"Filtering complete. Filtered file saved as: {filtered_a3m}\")\n",
        "else:\n",
        "    print(\"\\nSkipping hhfilter step.\")\n",
        "\n",
        "# --- 6. Find and report the final .a3m file ---\n",
        "print(\"\\n--- Final Output ---\")\n",
        "try:\n",
        "    if run_hhfilter and os.path.isfile(filtered_a3m):\n",
        "        print(f\"âœ… Your FINAL filtered alignment file is ready: {filtered_a3m}\")\n",
        "    elif os.path.isfile(original_a3m):\n",
        "        print(f\"âœ… Your original (unfiltered) alignment file is ready: {original_a3m}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Error: No .a3m file ({original_a3m} or {filtered_a3m}) was found.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâš ï¸ Error finding .a3m file: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SFSdQBD4Aw3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4) Find Conserved Residues based on the MSA\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Specify Input Files\n",
        "#@markdown Enter the name of the filtered .a3m file (from previous step).\n",
        "filtered_a3m_file = 'protein.filtered.a3m' #@param {type:\"string\"}\n",
        "#@markdown\n",
        "#@markdown ### 2. Conservation Settings\n",
        "#@markdown Fraction of most-conserved residues to select (e.g., 0.5 = 50%).\n",
        "frac_conserved = 0.5 #@param {type:\"number\"}\n",
        "#@markdown\n",
        "#@markdown ### 3. Output File\n",
        "#@markdown Name for the final .txt file containing the fixed positions.\n",
        "output_txt_file = \"fixed_positions.txt\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Self-contained A3M Parser (Replaces tools.py) ---\n",
        "def robust_parse_a3m(a3m_file_path):\n",
        "    \"\"\"\n",
        "    Parses an .a3m file, removes insertions (lowercase), and converts to numbers.\n",
        "    This version correctly skips header lines before the first sequence.\n",
        "    \"\"\"\n",
        "    # Mapping from AA to number (0-19 are AA, 20 is gap)\n",
        "    aa_to_num = {\n",
        "        'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9,\n",
        "        'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19,\n",
        "        '-': 20, 'X': 20, 'B': 20, 'Z': 20 # Treat unknowns as gaps\n",
        "    }\n",
        "\n",
        "    msa_sequences = []\n",
        "    seq = \"\"\n",
        "    found_first_header = False # Flag to skip junk lines at the start\n",
        "\n",
        "    try:\n",
        "        with open(a3m_file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line.startswith('>'):\n",
        "                    found_first_header = True # We've found the first sequence, start parsing\n",
        "                    if seq: # save previous sequence\n",
        "                        msa_sequences.append(seq)\n",
        "                    seq = \"\" # start new sequence\n",
        "                elif found_first_header and line: # Only append if we're parsing and line is not empty\n",
        "                    seq += line\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ðŸ”¥ Error: The alignment file '{a3m_file_path}' was not found.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸ”¥ Error reading {a3m_file_path}: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "    if seq: # save last sequence\n",
        "        msa_sequences.append(seq)\n",
        "\n",
        "    if not msa_sequences:\n",
        "        raise ValueError(f\"No sequences found in {a3m_file_path}. File might be malformed.\")\n",
        "\n",
        "    # Get query length from the first sequence\n",
        "    query_seq = msa_sequences[0]\n",
        "    query_L = len([c for c in query_seq if c.isupper() or c == '-'])\n",
        "\n",
        "    if query_L == 0:\n",
        "        raise ValueError(f\"Query sequence found, but it has no match/delete characters (Length is 0).\")\n",
        "\n",
        "    msa_aligned = []\n",
        "    for seq in msa_sequences:\n",
        "        aligned_seq = \"\"\n",
        "        for char in seq:\n",
        "            # Keep only uppercase (match) and gaps (delete)\n",
        "            if char.isupper() or char == '-':\n",
        "                aligned_seq += char\n",
        "\n",
        "        # Ensure all sequences have the same length as the query\n",
        "        if len(aligned_seq) == query_L:\n",
        "            msa_aligned.append(aligned_seq)\n",
        "\n",
        "    if not msa_aligned:\n",
        "         raise ValueError(f\"No valid, aligned sequences found in {a3m_file_path}.\")\n",
        "\n",
        "    # Convert to numbers\n",
        "    msa_numeric = []\n",
        "    for seq in msa_aligned:\n",
        "        num_seq = [aa_to_num.get(char.upper(), 20) for char in seq] # .upper() for safety\n",
        "        msa_numeric.append(num_seq)\n",
        "\n",
        "    return {\n",
        "        'msa': np.array(msa_aligned),\n",
        "        'msa_num': np.array(msa_numeric, dtype=int)\n",
        "    }\n",
        "# --- End of Parser ---\n",
        "\n",
        "\n",
        "# --- 2. Check for 'active_site_residue_numbers' variable ---\n",
        "# This variable should be created by the define_active_site_biopython.py script\n",
        "if 'active_site_residue_numbers' not in locals() and 'active_site_residue_numbers' not in globals():\n",
        "    print(\"ðŸ”¥ Error: The 'active_site_residue_numbers' list was not found.\")\n",
        "    print(\"   Please re-run the 'Define Active Site (Biopython)' script first.\")\n",
        "    sys.exit(1)\n",
        "else:\n",
        "    # Ensure it's a numpy array for union1d\n",
        "    active_site_np = np.array(active_site_residue_numbers)\n",
        "    print(f\"Found active_site_residue_numbers list with {len(active_site_np)} residues.\")\n",
        "\n",
        "# --- 3. Run Conservation Analysis ---\n",
        "try:\n",
        "    if not os.path.isfile(filtered_a3m_file):\n",
        "        print(f\"ðŸ”¥ Error: The alignment file '{filtered_a3m_file}' was not found.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Parsing alignment file: {filtered_a3m_file}...\")\n",
        "    # Use our new robust parser\n",
        "    aln = robust_parse_a3m(filtered_a3m_file)\n",
        "\n",
        "    msa_num = aln['msa_num']\n",
        "    L = msa_num.shape[1] # Get length (L) from the numeric array\n",
        "    num_seqs = msa_num.shape[0]\n",
        "\n",
        "    if num_seqs == 0 or L == 0:\n",
        "        print(f\"ðŸ”¥ Error: The alignment file '{filtered_a3m_file}' contains no valid sequences or has length 0.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Alignment loaded. Length: {L}, Number of sequences: {num_seqs}\")\n",
        "\n",
        "    # Calculate conservation\n",
        "    counts = np.stack([np.bincount(column, minlength=21) for column in msa_num.T]).T\n",
        "    max_count = np.max(counts, axis=0)\n",
        "\n",
        "    freq = counts / num_seqs\n",
        "\n",
        "    # Handle columns that are 100% gaps (to avoid divide-by-zero)\n",
        "    freq_sum = freq[:20].sum(axis=0)\n",
        "    freq_sum[freq_sum == 0] = 1.0 # Set sum to 1.0 to prevent error\n",
        "\n",
        "    freq_norm = freq[:20] / freq_sum\n",
        "    max_freq_norm = np.max(freq_norm, axis=0)\n",
        "\n",
        "    # Only apply low-count penalty if we have more than one sequence\n",
        "    if num_seqs > 1:\n",
        "        max_freq_norm[max_count < 10] = 0 # Don't choose positions with low counts\n",
        "    else:\n",
        "        print(\"Only 1 sequence found, skipping low-count filter.\")\n",
        "\n",
        "    # Save the conserved residues as a list\n",
        "    num_conserved = int(L * frac_conserved)\n",
        "    conserved_residues = np.argsort(max_freq_norm)[::-1][:num_conserved] + 1 # make 1-indexed\n",
        "    conserved_residues.sort()\n",
        "\n",
        "    print(f\"Found {len(conserved_residues)} conserved residues (top {frac_conserved*100}%).\")\n",
        "\n",
        "    # --- 4. Intersect with Active Site ---\n",
        "    # Use the active_site_np array we defined earlier\n",
        "    fixed_positions = np.union1d(conserved_residues, active_site_np)\n",
        "    fixed_positions.sort()\n",
        "\n",
        "    print(\"\\n--- Final Results ---\")\n",
        "    print(f\"Active Site Residues ({len(active_site_np)}):\")\n",
        "    print(active_site_np)\n",
        "    print(f\"Conserved Residues ({len(conserved_residues)}):\")\n",
        "    print(conserved_residues)\n",
        "    print(f\"All Fixed Positions ({len(fixed_positions)}):\")\n",
        "    print(fixed_positions)\n",
        "\n",
        "    # --- 5. Save Final List to TXT File ---\n",
        "    print(f\"\\nSaving final fixed positions to {output_txt_file}...\")\n",
        "    fixed_positions_str = [str(int(res)) for res in fixed_positions]\n",
        "\n",
        "    with open(output_txt_file, 'w') as f:\n",
        "        f.write(' '.join(fixed_positions_str))\n",
        "\n",
        "    print(f\"âœ… Successfully saved fixed positions to {output_txt_file}.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ”¥ An error occurred: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VvF_lX9PZwBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. Run ProteinMPNN with evolutionary information"
      ],
      "metadata": {
        "id": "wekZ4-aIEVtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#@markdown ## 1) Setting up ProteinMPNN with fixed positions\n",
        "#@markdown ### 1. Input PDB\n",
        "#@markdown Path to the PDB file (must be in your Colab folder).\n",
        "pdb_path = \"protein.pdb\" #@param {type:\"string\"}\n",
        "\n",
        "homomer = False #@param {type:\"boolean\"}\n",
        "designed_chain = \"A\" #@param {type:\"string\"}\n",
        "fixed_chain = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if designed_chain == \"\":\n",
        "  designed_chain_list = []\n",
        "else:\n",
        "  designed_chain_list = re.sub(\"[^A-Za-z]+\",\",\", designed_chain).split(\",\")\n",
        "\n",
        "if fixed_chain == \"\":\n",
        "  fixed_chain_list = []\n",
        "else:\n",
        "  fixed_chain_list = re.sub(\"[^A-Za-z]+\",\",\", fixed_chain).split(\",\")\n",
        "\n",
        "chain_list = list(set(designed_chain_list + fixed_chain_list))\n",
        "\n",
        "#@markdown - `designed_chain`: Chain(s) to design (e.g., \"A\").\n",
        "#@markdown - `fixed_chain`: Chain(s) to keep fixed (e.g., \"C\").\n",
        "\n",
        "#@markdown ### 2. Design Options\n",
        "#@markdown Number of sequences to generate.\n",
        "num_seqs = 1 #@param {type:\"integer\"}\n",
        "num_seq_per_target = num_seqs\n",
        "\n",
        "#@markdown - Sampling temperature for amino acids, T=0.0 means taking argmax, T>>1.0 means sample randomly.\n",
        "sampling_temp = \"0.2\" #@param [\"0.0001\", \"0.1\", \"0.15\", \"0.2\", \"0.25\", \"0.3\", \"0.5\"]\n",
        "\n",
        "#@markdown - `omit_AAs`: Specify amino acids to omit (e.g., \"XC\" to omit Cys and Unknown).\n",
        "omit_AAs = \"XC\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### 3. Fixed Positions (Optional)\n",
        "#@markdown Provide the .txt file of conserved/active site residues to fix.\n",
        "fixed_positions_file = \"fixed_positions.txt\" #@param {type:\"string\"}\n",
        "#@markdown The chain these fixed positions apply to.\n",
        "fixed_positions_chain = \"A\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "# --- (Rest of your script's parameters) ---\n",
        "save_score=0\n",
        "save_probs=0\n",
        "score_only=0\n",
        "conditional_probs_only=0\n",
        "conditional_probs_only_backbone=0\n",
        "batch_size=1\n",
        "max_length=20000\n",
        "out_folder='.'\n",
        "jsonl_path=''\n",
        "# omit_AAs='X'  <--- THIS LINE IS NOW DELETED (replaced by the form variable)\n",
        "pssm_multi=0.0\n",
        "pssm_threshold=0.0\n",
        "pssm_log_odds_flag=0\n",
        "pssm_bias_flag=0\n",
        "\n",
        "##############################################################\n",
        "\n",
        "folder_for_outputs = out_folder\n",
        "\n",
        "NUM_BATCHES = num_seq_per_target//batch_size\n",
        "BATCH_COPIES = batch_size\n",
        "temperatures = [float(item) for item in sampling_temp.split()]\n",
        "omit_AAs_list = omit_AAs\n",
        "alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
        "\n",
        "omit_AAs_np = np.array([AA in omit_AAs_list for AA in alphabet]).astype(np.float32)\n",
        "\n",
        "chain_id_dict = None\n",
        "fixed_positions_dict = None\n",
        "pssm_dict = None\n",
        "omit_AA_dict = None\n",
        "bias_AA_dict = None\n",
        "tied_positions_dict = None\n",
        "bias_by_res_dict = None\n",
        "bias_AAs_np = np.zeros(len(alphabet))\n",
        "\n",
        "# --- New code to read fixed_positions.txt ---\n",
        "if fixed_positions_file and fixed_positions_chain:\n",
        "    try:\n",
        "        with open(fixed_positions_file, 'r') as f:\n",
        "            # Read the space-separated list of numbers\n",
        "            residues_to_fix = [int(res) for res in f.read().split()]\n",
        "\n",
        "        if residues_to_fix:\n",
        "            # Get the PDB name (basename without .pdb)\n",
        "            pdb_name = os.path.basename(pdb_path).replace('.pdb', '')\n",
        "\n",
        "            # Build the dictionary in the format MPNN expects\n",
        "            fixed_positions_dict = {\n",
        "                pdb_name: {\n",
        "                    fixed_positions_chain: residues_to_fix\n",
        "                }\n",
        "            }\n",
        "            print(f\"âœ… Successfully read {len(residues_to_fix)} fixed positions for chain {fixed_positions_chain} from {fixed_positions_file}.\")\n",
        "            print(f\"Fixed positions: {residues_to_fix}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ {fixed_positions_file} was found but is empty. No specific residues fixed.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âš ï¸ {fixed_positions_file} not found. No specific residues fixed (besides 'fixed_chain').\")\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸ”¥ Error reading {fixed_positions_file}: {e}\")\n",
        "else:\n",
        "    print(\"No fixed positions file provided. Only 'fixed_chain' (if any) will be fixed.\")\n",
        "# --- End new code ---\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# Check if PDB file exists before proceeding\n",
        "if not os.path.isfile(pdb_path):\n",
        "    print(f\"ðŸ”¥ Error: PDB file not found at {pdb_path}\")\n",
        "    print(\"Please make sure the file is in your Colab folder.\")\n",
        "else:\n",
        "    pdb_dict_list = parse_PDB(pdb_path, input_chain_list=chain_list)\n",
        "    dataset_valid = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=max_length)\n",
        "\n",
        "    chain_id_dict = {}\n",
        "    chain_id_dict[pdb_dict_list[0]['name']]= (designed_chain_list, fixed_chain_list)\n",
        "\n",
        "    print(f\"\\nChain assignments: {chain_id_dict}\")\n",
        "    for chain in chain_list:\n",
        "      l = len(pdb_dict_list[0][f\"seq_chain_{chain}\"])\n",
        "      print(f\"Length of chain {chain} is {l}\")\n",
        "\n",
        "    if homomer:\n",
        "      tied_positions_dict = make_tied_positions_for_homomers(pdb_dict_list)\n",
        "    else:\n",
        "      tied_positions_dict = None"
      ],
      "metadata": {
        "cellView": "form",
        "id": "agHE5Szc-yFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Run ProteinMPNN with fixed positions\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Output FASTA File\n",
        "output_fasta_file = \"generated_sequences.fasta\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 2. A3M Generation\n",
        "source_a3m_file = \"protein.filtered.a3m\"          #@param {type:\"string\"}\n",
        "output_msa_folder = \"msa\"                        #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os, copy, torch, numpy as np\n",
        "\n",
        "# --- 1. Read base alignment ---\n",
        "os.makedirs(output_msa_folder, exist_ok=True)\n",
        "alignment_body_lines = []\n",
        "try:\n",
        "    with open(source_a3m_file, 'r') as f:\n",
        "        found_first_seq = False\n",
        "        for line in f:\n",
        "            line = line.rstrip('\\n\\r') + '\\n'  # normalize newlines\n",
        "            if line.startswith('>'):\n",
        "                if not found_first_seq:\n",
        "                    found_first_seq = True\n",
        "                    continue  # skip query header\n",
        "                alignment_body_lines.append(line)\n",
        "            elif found_first_seq:\n",
        "                alignment_body_lines.append(line)\n",
        "    print(f\"Read {len(alignment_body_lines)//2} alignment hits from {source_a3m_file}.\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ WARNING reading {source_a3m_file}: {e}\")\n",
        "\n",
        "def _alignment_tail_to_write(alignment_lines):\n",
        "    \"\"\"Return alignment hits excluding the query sequence.\"\"\"\n",
        "    if not alignment_lines:\n",
        "        return []\n",
        "    if not alignment_lines[0].startswith('>'):\n",
        "        return alignment_lines[1:]\n",
        "    return alignment_lines\n",
        "\n",
        "# --- 2. Generate sequences ---\n",
        "with torch.no_grad():\n",
        "    print('Generating sequences...')\n",
        "    print(f\"Saving generated sequences to: {output_fasta_file}\")\n",
        "    with open(output_fasta_file, 'w') as fasta_f:\n",
        "        for ix, protein in enumerate(dataset_valid):\n",
        "            score_list, all_probs_list, all_log_probs_list, S_sample_list = [], [], [], []\n",
        "            batch_clones = [copy.deepcopy(protein) for _ in range(BATCH_COPIES)]\n",
        "            X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta = tied_featurize(\n",
        "                batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict,\n",
        "                pssm_dict, bias_by_res_dict)\n",
        "            pssm_log_odds_mask = (pssm_log_odds_all > pssm_threshold).float()\n",
        "            name_ = batch_clones[0]['name']\n",
        "\n",
        "            randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
        "            log_probs = model(X, S, mask, chain_M * chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
        "            mask_for_loss = mask * chain_M * chain_M_pos\n",
        "            native_score = _scores(S, log_probs, mask_for_loss).cpu().data.numpy()\n",
        "\n",
        "            for temp in temperatures:\n",
        "                for j in range(NUM_BATCHES):\n",
        "                    randn_2 = torch.randn(chain_M.shape, device=X.device)\n",
        "                    if tied_positions_dict is None:\n",
        "                        sample_dict = model.sample(\n",
        "                            X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp,\n",
        "                            omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos,\n",
        "                            omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias,\n",
        "                            pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag),\n",
        "                            pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag),\n",
        "                            bias_by_res=bias_by_res_all)\n",
        "                        S_sample = sample_dict[\"S\"]\n",
        "                    else:\n",
        "                        sample_dict = model.tied_sample(\n",
        "                            X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp,\n",
        "                            omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos,\n",
        "                            omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias,\n",
        "                            pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag),\n",
        "                            pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag),\n",
        "                            tied_pos=tied_pos_list_of_lists_list[0], tied_beta=tied_beta,\n",
        "                            bias_by_res=bias_by_res_all)\n",
        "                        S_sample = sample_dict[\"S\"]\n",
        "\n",
        "                    log_probs = model(X, S_sample, mask, chain_M * chain_M_pos,\n",
        "                                      residue_idx, chain_encoding_all, randn_2,\n",
        "                                      use_input_decoding_order=True,\n",
        "                                      decoding_order=sample_dict[\"decoding_order\"])\n",
        "                    mask_for_loss = mask * chain_M * chain_M_pos\n",
        "                    scores = _scores(S_sample, log_probs, mask_for_loss).cpu().data.numpy()\n",
        "\n",
        "                    for b_ix in range(BATCH_COPIES):\n",
        "                        masked_chain_length_list = masked_chain_length_list_list[b_ix]\n",
        "                        masked_list = masked_list_list[b_ix]\n",
        "                        seq_recovery_rate = torch.sum(\n",
        "                            torch.sum(torch.nn.functional.one_hot(S[b_ix],21)\n",
        "                                      * torch.nn.functional.one_hot(S_sample[b_ix],21), axis=-1)\n",
        "                            * mask_for_loss[b_ix]) / torch.sum(mask_for_loss[b_ix])\n",
        "                        seq = _S_to_seq(S_sample[b_ix], chain_M[b_ix])\n",
        "                        score = scores[b_ix]\n",
        "                        native_seq = _S_to_seq(S[b_ix], chain_M[b_ix])\n",
        "\n",
        "                        # --- Native written once ---\n",
        "                        if b_ix == 0 and j == 0 and temp == temperatures[0]:\n",
        "                            native_seq = \"\".join(native_seq.split(\"/\")).strip()\n",
        "                            native_score_print = np.format_float_positional(np.float32(native_score.mean()), unique=False, precision=4)\n",
        "                            line = f\">native, score={native_score_print}\\n{native_seq}\\n\"\n",
        "                            fasta_f.write(line)\n",
        "                            print(line.rstrip())\n",
        "\n",
        "                            a3m_filename = os.path.join(output_msa_folder, f\"{name_}_native.a3m\")\n",
        "                            try:\n",
        "                                with open(a3m_filename, 'w') as a3m_f:\n",
        "                                    native_len = len(native_seq)\n",
        "                                    a3m_f.write(f\"#{native_len}\\t1\\n>native\\n{native_seq}\\n\")\n",
        "                                    tail = _alignment_tail_to_write(alignment_body_lines)\n",
        "                                    a3m_f.writelines(tail)\n",
        "                                print(f\"Wrote native A3M: {a3m_filename}\")\n",
        "                            except Exception as e:\n",
        "                                print(f\"âš ï¸ Warning: Could not write native A3M {a3m_filename}. Error: {e}\")\n",
        "\n",
        "                        # --- Generated sequences ---\n",
        "                        seq = \"\".join(seq.split(\"/\")).strip()\n",
        "                        score_print = np.format_float_positional(np.float32(score), unique=False, precision=4)\n",
        "                        seq_rec_print = np.format_float_positional(np.float32(seq_recovery_rate.detach().cpu().numpy()), unique=False, precision=4)\n",
        "                        sample_index = j * BATCH_COPIES + b_ix + 1\n",
        "                        line = f\">sample{sample_index}, T={temp}, score={score_print}, seq_recovery={seq_rec_print}\\n{seq}\\n\"\n",
        "                        fasta_f.write(line)\n",
        "                        print(line.rstrip())\n",
        "\n",
        "                        sample_name = f\"sample{sample_index}\"\n",
        "                        a3m_filename = os.path.join(output_msa_folder, f\"{name_}_{sample_name}.a3m\")\n",
        "                        try:\n",
        "                            with open(a3m_filename, 'w') as a3m_f:\n",
        "                                seq_len = len(seq)\n",
        "                                a3m_f.write(f\"#{seq_len}\\t1\\n>{sample_name}\\n{seq}\\n\")\n",
        "                                tail = _alignment_tail_to_write(alignment_body_lines)\n",
        "                                a3m_f.writelines(tail)\n",
        "                            print(f\"Wrote sample A3M: {a3m_filename}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"âš ï¸ Warning: Could not write sample A3M {a3m_filename}. Error: {e}\")\n",
        "\n",
        "    print(f\"\\nâœ… All sequences saved to {output_fasta_file} and A3Ms in '{output_msa_folder}'.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w1qviwvLAyim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Visualize the generated sequences using an MSA Viewer in Google Colab\n",
        "#The following code is modified from the wonderful viewer developed by Damien Farrell\n",
        "#https://dmnfarrell.github.io/bioinformatics/bokeh-sequence-aligner\n",
        "\n",
        "#Importing all modules first\n",
        "import os, io, random\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from Bio.Seq import Seq\n",
        "from Bio.Align import MultipleSeqAlignment\n",
        "from Bio import AlignIO, SeqIO\n",
        "\n",
        "import panel as pn\n",
        "import panel.widgets as pnw\n",
        "pn.extension()\n",
        "\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.models import ColumnDataSource, Plot, Grid, Range1d\n",
        "from bokeh.models.glyphs import Text, Rect\n",
        "from bokeh.layouts import gridplot\n",
        "\n",
        "#Setting up the amino color code according to Zappo color scheme\n",
        "def get_colors(seqs):\n",
        "    #make colors for bases in sequence\n",
        "    text = [i for s in list(seqs) for i in s]\n",
        "    #Use Zappo color scheme\n",
        "    clrs =  {'K':'red',\n",
        "             'R':'red',\n",
        "             'H':'red',\n",
        "             'D':'green',\n",
        "             'E':'green',\n",
        "             'Q':'blue',\n",
        "             'N':'blue',\n",
        "             'S':'blue',\n",
        "             'T':'blue',\n",
        "             'A':'blue',\n",
        "             'I':'blue',\n",
        "             'L':'blue',\n",
        "             'M':'blue',\n",
        "             'V':'blue',\n",
        "             'F':'orange',\n",
        "             'Y':'orange',\n",
        "             'W':'orange',\n",
        "             'C':'blue',\n",
        "             'P':'yellow',\n",
        "             'G':'orange',\n",
        "             '-':'white'}\n",
        "    colors = [clrs[i] for i in text]\n",
        "    return colors\n",
        "\n",
        "#Setting up the MSA viewer\n",
        "def view_alignment(aln, fontsize=\"9pt\", plot_width=800):\n",
        "    \"\"\"Bokeh sequence alignment view\"\"\"\n",
        "\n",
        "    #make sequence and id lists from the aln object\n",
        "    seqs = [rec.seq for rec in (aln)]\n",
        "    ids = [rec.id for rec in aln]\n",
        "    text = [i for s in list(seqs) for i in s]\n",
        "    colors = get_colors(seqs)\n",
        "    N = len(seqs[0])\n",
        "    S = len(seqs)\n",
        "    width = .4\n",
        "\n",
        "    x = np.arange(1,N+1)\n",
        "    y = np.arange(0,S,1)\n",
        "    #creates a 2D grid of coords from the 1D arrays\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    #flattens the arrays\n",
        "    gx = xx.ravel()\n",
        "    gy = yy.flatten()\n",
        "    #use recty for rect coords with an offset\n",
        "    recty = gy+.5\n",
        "    h= 1/S\n",
        "    #now we can create the ColumnDataSource with all the arrays\n",
        "    source = ColumnDataSource(dict(x=gx, y=gy, recty=recty, text=text, colors=colors))\n",
        "    plot_height = len(seqs)*15+50\n",
        "    x_range = Range1d(0,N+1, bounds='auto')\n",
        "    if N>100:\n",
        "        viewlen=100\n",
        "    else:\n",
        "        viewlen=N\n",
        "    #view_range is for the close up view\n",
        "    view_range = (0,viewlen)\n",
        "    tools=\"xpan, xwheel_zoom, reset, save\"\n",
        "\n",
        "    #entire sequence view (no text, with zoom)\n",
        "    p = figure(title=None, width= plot_width, height=50,\n",
        "               x_range=x_range, y_range=(0,S), tools=tools,\n",
        "               min_border=0, toolbar_location='below')\n",
        "    rects = Rect(x=\"x\", y=\"recty\",  width=1, height=1, fill_color=\"colors\",\n",
        "                 line_color=None, fill_alpha=0.6)\n",
        "    p.add_glyph(source, rects)\n",
        "    p.yaxis.visible = False\n",
        "    p.grid.visible = False\n",
        "\n",
        "    #sequence text view with ability to scroll along x axis\n",
        "    p1 = figure(title=None, width=plot_width, height=plot_height,\n",
        "                x_range=view_range, y_range=ids, tools=\"xpan,reset\",\n",
        "                min_border=0, toolbar_location='below')#, lod_factor=1)\n",
        "    glyph = Text(x=\"x\", y=\"y\", text=\"text\", text_align='center',text_color=\"black\",\n",
        "                text_font=\"monospace\",text_font_size=fontsize)\n",
        "    rects = Rect(x=\"x\", y=\"recty\",  width=1, height=1, fill_color=\"colors\",\n",
        "                line_color=None, fill_alpha=0.4)\n",
        "    p1.add_glyph(source, glyph)\n",
        "    p1.add_glyph(source, rects)\n",
        "\n",
        "    p1.grid.visible = False\n",
        "    p1.xaxis.major_label_text_font_style = \"bold\"\n",
        "    p1.yaxis.minor_tick_line_width = 0\n",
        "    p1.yaxis.major_tick_line_width = 0\n",
        "\n",
        "    p = gridplot([[p],[p1]], toolbar_location='below')\n",
        "    return p\n",
        "\n",
        "#Loading the viewer by indicating the MSA file and format to read\n",
        "#@markdown Name of the MSA file (including the filetype)\n",
        "MSAfile = 'generated_sequences.fasta' #@param {type:\"string\"}\n",
        "MSAformat = 'fasta' #@param {type:\"string\"}\n",
        "aln = AlignIO.read(MSAfile,MSAformat)\n",
        "p = view_alignment(aln, plot_width=900)\n",
        "pn.pane.Bokeh(p)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cmDekMMXBjpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3. Predict  structures of the designed sequences with AF2"
      ],
      "metadata": {
        "id": "74-F6pY_Ey1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Run ColabFold using custom MSAs and a single model\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Input / Output Folders\n",
        "msa_dir = 'msa' #@param {type:\"string\"}\n",
        "predictions_dir = 'predictions' #@param {type:\"string\"}\n",
        "csv_output_file = 'confidence_metrics.csv' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Model Settings\n",
        "#@markdown Specify model number(s) to run (e.g., \"3\" or \"1,2,3,4,5\")\n",
        "model_order = \"4\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. Setup directories ---\n",
        "result_dir_path = Path(predictions_dir)\n",
        "msa_dir_path = Path(msa_dir)\n",
        "os.makedirs(result_dir_path, exist_ok=True)\n",
        "\n",
        "# --- 2. Run ColabFold using the simple batch command ---\n",
        "print(f\"ðŸš€ Starting ColabFold batch run...\")\n",
        "print(f\"   Input: {msa_dir}\")\n",
        "print(f\"   Output: {predictions_dir}\")\n",
        "print(f\"   Models: {model_order}\")\n",
        "\n",
        "!colabfold_batch \\\n",
        "  --model-order {model_order} \\\n",
        "  --model-type alphafold2_ptm \\\n",
        "  {msa_dir} \\\n",
        "  {predictions_dir}\n",
        "\n",
        "print(\"\\nâœ… Prediction run complete.\")\n",
        "\n",
        "# --- 3. Gather confidence results ---\n",
        "print(f\"\\nðŸ“Š Parsing results to create {csv_output_file}...\")\n",
        "results = []\n",
        "\n",
        "# --- FIX 1 ---\n",
        "# Search for \"_scores_rank_001_*.json\" instead of \"_unrelaxed_rank_001_*.json\"\n",
        "# Also removed the extra \"*/\" since there are no sub-folders.\n",
        "json_files = sorted(result_dir_path.glob(\"*_scores_rank_001_*.json\"))\n",
        "# --- End FIX 1 ---\n",
        "\n",
        "if not json_files:\n",
        "    print(f\"ðŸ”¥ Error: No JSON result files found in {predictions_dir}.\")\n",
        "    print(\"   Please check if the predictions ran correctly and produced output.\")\n",
        "    print(f\"   (Was looking for files like: *_scores_rank_001_*.json)\")\n",
        "else:\n",
        "    print(f\"   Found {len(json_files)} result files to parse.\")\n",
        "    for jf in json_files:\n",
        "        try:\n",
        "            # --- FIX 2 ---\n",
        "            # Split the filename by \"_scores\" to get the jobname\n",
        "            jobname = jf.name.split(\"_scores\")[0]\n",
        "            # --- End FIX 2 ---\n",
        "\n",
        "            data = json.load(open(jf))\n",
        "\n",
        "            # This part is correct: it calculates the average from the list\n",
        "            avg_plddt = np.mean(data.get(\"plddt\", [])) if data.get(\"plddt\") else None\n",
        "            ptm = data.get(\"ptm\", None)\n",
        "\n",
        "            model_name = \"unknown\"\n",
        "            if \"model_name\" in data:\n",
        "                model_name = data[\"model_name\"]\n",
        "            elif \"model\" in data:\n",
        "                model_name = data[\"model\"]\n",
        "            else:\n",
        "                for i in model_order.split(','):\n",
        "                    if f\"model_{i}\" in jf.name:\n",
        "                        model_name = f\"model_{i}\"\n",
        "                        break\n",
        "\n",
        "            results.append({\n",
        "                \"sequence_name\": jobname,\n",
        "                \"avg_plddt\": avg_plddt,\n",
        "                \"ptm\": ptm,\n",
        "                \"model_used\": model_name\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error parsing {jf.name}: {e}\")\n",
        "\n",
        "    if results:\n",
        "        df = pd.DataFrame(results).sort_values(\"avg_plddt\", ascending=False)\n",
        "        out_path = result_dir_path / csv_output_file\n",
        "        df.to_csv(out_path, index=False)\n",
        "        print(f\"\\nâœ… Successfully saved confidence metrics to: {out_path}\")\n",
        "        print(\"\\n--- Top Results ---\")\n",
        "        print(df.head())\n",
        "    else:\n",
        "        print(\"ðŸ”¥ No results were successfully parsed. Check predictions directory.\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ All done.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uWdgcQcDEuG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Calculate RMSD & Save Aligned Structures\n",
        "#@markdown ---\n",
        "#@markdown ### 1. File Locations\n",
        "#@markdown ---\n",
        "#@markdown Path to your single experimental/reference PDB file:\n",
        "experimental_pdb = \"protein.pdb\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown Folder where ColabFold saved the predictions and CSV:\n",
        "predictions_dir = \"predictions\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown Name of the CSV file to read and update:\n",
        "csv_output_file = \"confidence_metrics.csv\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Folder to save all aligned PDBs for visualization:**\n",
        "aligned_pdb_folder = \"aligned\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Alignment Options\n",
        "#@markdown ---\n",
        "#@markdown Select the method for structural superposition:\n",
        "alignment_mode = \"All Atoms\" #@param [\"All Atoms\", \"Iterative Exclusion\", \"Specific Residues\"]\n",
        "#@markdown ---\n",
        "#@markdown **For \"Iterative Exclusion\" mode:**\n",
        "#@markdown Cutoff in Ã…. Residues with CÎ± distance > cutoff after alignment will be excluded.\n",
        "iterative_rmsd_cutoff = 2.0 #@param {type:\"number\"}\n",
        "#@markdown Maximum number of iterations to run.\n",
        "iterative_max_cycles = 5 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "#@markdown **For \"Specific Residues\" mode:**\n",
        "#@markdown Provide a comma-separated list of residues or ranges (e.g., \"10-50, 80, 91-100\").\n",
        "residue_list_to_align = \"1-188\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# Try to import Biopython, which is needed for RMSD\n",
        "try:\n",
        "    from Bio.PDB import PDBParser, Superimposer, PDBIO\n",
        "except ImportError:\n",
        "    print(\" Biopython not found. Installing...\")\n",
        "    !pip install biopython\n",
        "    from Bio.PDB import PDBParser, Superimposer, PDBIO\n",
        "\n",
        "# --- Helper Function to Parse Residue List ---\n",
        "def parse_residue_list(res_string):\n",
        "    \"\"\"Parses a residue string like \"10-50, 80, 91-100\" into a set of integers.\"\"\"\n",
        "    residue_set = set()\n",
        "    if not res_string:\n",
        "        return residue_set\n",
        "\n",
        "    parts = res_string.split(',')\n",
        "    for part in parts:\n",
        "        part = part.strip()\n",
        "        if not part:\n",
        "            continue\n",
        "        if '-' in part:\n",
        "            try:\n",
        "                start, end = part.split('-')\n",
        "                start_res = int(start.strip())\n",
        "                end_res = int(end.strip())\n",
        "                residue_set.update(range(start_res, end_res + 1))\n",
        "            except ValueError:\n",
        "                print(f\"âš ï¸ Warning: Could not parse range '{part}'. Skipping.\")\n",
        "        else:\n",
        "            try:\n",
        "                residue_set.add(int(part.strip()))\n",
        "            except ValueError:\n",
        "                print(f\"âš ï¸ Warning: Could not parse residue number '{part}'. Skipping.\")\n",
        "    return residue_set\n",
        "\n",
        "# --- 1. Load Reference Structure & Setup Folders ---\n",
        "print(f\"Loading reference structure: {experimental_pdb}\")\n",
        "pdb_parser = PDBParser(QUIET=True)\n",
        "io = PDBIO() # Initialize PDB saver\n",
        "\n",
        "# Create the output folder for aligned PDBs\n",
        "os.makedirs(aligned_pdb_folder, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    ref_structure = pdb_parser.get_structure(\"reference\", experimental_pdb)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ðŸ”¥ Error: Experimental PDB not found at: {experimental_pdb}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Get all C-alpha atoms as a dictionary, keyed by residue number\n",
        "try:\n",
        "    ref_ca_dict = {\n",
        "        atom.get_parent().id[1]: atom\n",
        "        for atom in ref_structure[0].get_atoms()\n",
        "        if atom.name == \"CA\" and atom.get_parent().id[0] == ' ' # Ensure it's a standard residue\n",
        "    }\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ”¥ Error parsing reference PDB: {e}\")\n",
        "    print(\"   Make sure it's a valid PDB file.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if not ref_ca_dict:\n",
        "    print(f\"ðŸ”¥ Error: No standard C-alpha atoms (CA) found in {experimental_pdb}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"Loaded {len(ref_ca_dict)} C-alpha atoms from reference.\")\n",
        "\n",
        "# --- Save a copy of the reference PDB to the aligned folder ---\n",
        "print(f\"\\nSaving reference structure to {aligned_pdb_folder}...\")\n",
        "io.set_structure(ref_structure)\n",
        "ref_output_name = f\"{Path(experimental_pdb).stem}_ref.pdb\"\n",
        "ref_output_path = os.path.join(aligned_pdb_folder, ref_output_name)\n",
        "io.save(ref_output_path)\n",
        "print(f\"  Saved: {ref_output_path}\")\n",
        "\n",
        "# --- 2. Load CSV File ---\n",
        "csv_path = os.path.join(predictions_dir, csv_output_file)\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ðŸ”¥ Error: CSV file not found at: {csv_path}\")\n",
        "    print(\"   Please run the previous cell to generate the CSV.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 3. Parse alignment residues if needed ---\n",
        "alignment_residue_set = set()\n",
        "if alignment_mode == \"Specific Residues\":\n",
        "    alignment_residue_set = parse_residue_list(residue_list_to_align)\n",
        "    if not alignment_residue_set:\n",
        "        print(f\"ðŸ”¥ Error: 'Specific Residues' mode selected, but no valid residues found in '{residue_list_to_align}'.\")\n",
        "        sys.exit(1)\n",
        "    print(f\"Running in 'Specific Residues' mode. Aligning on {len(alignment_residue_set)} specified residues.\")\n",
        "elif alignment_mode == \"Iterative Exclusion\":\n",
        "    print(f\"Running in 'Iterative Exclusion' mode (Cutoff={iterative_rmsd_cutoff} Ã…, Max Cycles={iterative_max_cycles}).\")\n",
        "else:\n",
        "    print(\"Running in 'All Atoms' mode.\")\n",
        "\n",
        "# --- 4. Loop Through Predictions and Calculate RMSD ---\n",
        "rmsd_list = []\n",
        "aligned_residues_list = []\n",
        "super_imposer = Superimposer()\n",
        "\n",
        "print(\"\\nCalculating RMSD and saving aligned structures...\")\n",
        "for index, row in df.iterrows():\n",
        "    jobname = row['sequence_name']\n",
        "\n",
        "    # Find the corresponding PDB file\n",
        "    pdb_pattern = os.path.join(predictions_dir, f\"{jobname}_unrelaxed_rank_001_*.pdb\")\n",
        "    pdb_files = glob.glob(pdb_pattern)\n",
        "\n",
        "    if not pdb_files:\n",
        "        print(f\"âš ï¸ Warning: No PDB file found for {jobname}. Skipping.\")\n",
        "        rmsd_list.append(np.nan)\n",
        "        aligned_residues_list.append(np.nan)\n",
        "        continue\n",
        "\n",
        "    predicted_pdb_path = pdb_files[0]\n",
        "\n",
        "    try:\n",
        "        # Load the predicted structure and get its C-alpha dict\n",
        "        sample_structure = pdb_parser.get_structure(jobname, predicted_pdb_path)\n",
        "        sample_ca_dict = {\n",
        "            atom.get_parent().id[1]: atom\n",
        "            for atom in sample_structure[0].get_atoms()\n",
        "            if atom.name == \"CA\" and atom.get_parent().id[0] == ' '\n",
        "        }\n",
        "\n",
        "        if not sample_ca_dict:\n",
        "            print(f\"âš ï¸ Warning: No C-alpha atoms found in {jobname}. Skipping.\")\n",
        "            rmsd_list.append(np.nan)\n",
        "            aligned_residues_list.append(np.nan)\n",
        "            continue\n",
        "\n",
        "        # --- Start Alignment Logic ---\n",
        "        rmsd_val = np.nan\n",
        "        num_aligned = 0\n",
        "\n",
        "        if alignment_mode == \"Specific Residues\":\n",
        "            common_res_ids = alignment_residue_set & set(ref_ca_dict.keys()) & set(sample_ca_dict.keys())\n",
        "            if len(common_res_ids) < len(alignment_residue_set):\n",
        "                print(f\"  Info for {jobname}: Using {len(common_res_ids)} common residues out of {len(alignment_residue_set)} requested.\")\n",
        "\n",
        "            if not common_res_ids:\n",
        "                print(f\"  âš ï¸ Warning: No common residues for alignment in {jobname}. Skipping.\")\n",
        "                rmsd_list.append(np.nan)\n",
        "                aligned_residues_list.append(0)\n",
        "                continue\n",
        "\n",
        "            ref_atoms = [ref_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            sample_atoms = [sample_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            num_aligned = len(common_res_ids)\n",
        "\n",
        "            super_imposer.set_atoms(ref_atoms, sample_atoms)\n",
        "            # Apply transformation to the *entire* structure\n",
        "            super_imposer.apply(sample_structure[0].get_atoms())\n",
        "            rmsd_val = super_imposer.rms\n",
        "\n",
        "            print_msg = f\"  âœ… {jobname}: RMSD = {rmsd_val:.3f} Ã… (on {num_aligned} specified residues)\"\n",
        "\n",
        "\n",
        "        elif alignment_mode == \"Iterative Exclusion\":\n",
        "            current_res_ids = set(ref_ca_dict.keys()) & set(sample_ca_dict.keys())\n",
        "\n",
        "            for i in range(iterative_max_cycles):\n",
        "                if not current_res_ids:\n",
        "                    print(f\"  ðŸ”¥ Error for {jobname}: No atoms left to align during iteration.\")\n",
        "                    rmsd_val = np.nan\n",
        "                    break\n",
        "\n",
        "                ref_atoms_subset = [ref_ca_dict[res_id] for res_id in current_res_ids]\n",
        "                sample_atoms_subset = [sample_ca_dict[res_id] for res_id in current_res_ids]\n",
        "\n",
        "                super_imposer.set_atoms(ref_atoms_subset, sample_atoms_subset)\n",
        "                super_imposer.apply(sample_structure[0].get_atoms())\n",
        "                rmsd_val = super_imposer.rms\n",
        "\n",
        "                new_res_ids = set()\n",
        "                for res_id in current_res_ids:\n",
        "                    dist = ref_ca_dict[res_id] - sample_ca_dict[res_id]\n",
        "                    if dist < iterative_rmsd_cutoff:\n",
        "                        new_res_ids.add(res_id)\n",
        "\n",
        "                if len(new_res_ids) == len(current_res_ids):\n",
        "                    print_msg = f\"  âœ… {jobname}: Converged. RMSD = {rmsd_val:.3f} Ã… (on {len(new_res_ids)} core atoms)\"\n",
        "                    num_aligned = len(new_res_ids)\n",
        "                    break\n",
        "\n",
        "                print(f\"    Iter {i+1} for {jobname}: RMSD={rmsd_val:.3f} ({len(current_res_ids)} atoms) -> Removing {len(current_res_ids) - len(new_res_ids)} outliers.\")\n",
        "                current_res_ids = new_res_ids\n",
        "            else:\n",
        "                print_msg = f\"  âš ï¸ {jobname}: Max iterations reached. Final RMSD = {rmsd_val:.3f} Ã… (on {len(current_res_ids)} core atoms)\"\n",
        "                num_aligned = len(current_res_ids)\n",
        "\n",
        "            rmsd_list.append(rmsd_val)\n",
        "            aligned_residues_list.append(num_aligned)\n",
        "\n",
        "        else: # \"All Atoms\" mode (default)\n",
        "            common_res_ids = set(ref_ca_dict.keys()) & set(sample_ca_dict.keys())\n",
        "\n",
        "            ref_atoms = [ref_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            sample_atoms = [sample_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            num_aligned = len(common_res_ids)\n",
        "\n",
        "            if len(ref_atoms) != len(ref_ca_dict) or len(sample_atoms) != len(sample_ca_dict):\n",
        "                 print(f\"  Info for {jobname}: Found {num_aligned} common atoms for alignment.\")\n",
        "\n",
        "            super_imposer.set_atoms(ref_atoms, sample_atoms)\n",
        "            # Apply transformation to the *entire* structure\n",
        "            super_imposer.apply(sample_structure[0].get_atoms())\n",
        "            rmsd_val = super_imposer.rms\n",
        "\n",
        "            print_msg = f\"  âœ… {jobname}: RMSD = {rmsd_val:.3f} Ã… (on {num_aligned} atoms)\"\n",
        "\n",
        "        # --- Save the aligned structure ---\n",
        "        if not np.isnan(rmsd_val):\n",
        "            output_filename = os.path.join(aligned_pdb_folder, f\"{jobname}_aligned.pdb\")\n",
        "            io.set_structure(sample_structure)\n",
        "            io.save(output_filename)\n",
        "            print(print_msg + f\" -> Saved to {output_filename}\")\n",
        "        else:\n",
        "            print(print_msg) # Print error/warning message from loop\n",
        "\n",
        "        rmsd_list.append(rmsd_val)\n",
        "        aligned_residues_list.append(num_aligned)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸ”¥ Error processing {jobname}: {e}\")\n",
        "        rmsd_list.append(np.nan)\n",
        "        aligned_residues_list.append(np.nan)\n",
        "\n",
        "# --- 5. Update DataFrame and Save ---\n",
        "df['rmsd_to_exp (Ã…)'] = rmsd_list\n",
        "df['rmsd_aligned_residues'] = aligned_residues_list\n",
        "df = df.sort_values(\"rmsd_to_exp (Ã…)\", ascending=True)\n",
        "\n",
        "# Save the updated CSV\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Successfully calculated RMSD, saved aligned PDBs to '{aligned_pdb_folder}', and updated {csv_path}.\")\n",
        "print(f\"   Mode used: {alignment_mode}\")\n",
        "print(\"\\n--- Results (Sorted by RMSD) ---\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iSRF6nd0jw0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Plot pLDDT, pTM, and RMSD (Interactive, Wide)\n",
        "#@markdown ---\n",
        "#@markdown ### 1. File Locations\n",
        "#@markdown ---\n",
        "#@markdown Folder where your CSV file is located:\n",
        "predictions_dir = \"predictions\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown Name of the CSV file to read:\n",
        "csv_output_file = \"confidence_metrics.csv\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Name of your native/reference sequence:**\n",
        "#@markdown (This must match the 'sequence_name' in the CSV exactly for red coloring)\n",
        "native_sequence_name = \"protein_native\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Output\n",
        "#@markdown ---\n",
        "#@markdown Name of the interactive plot file to save (must be .json):\n",
        "plot_output_file_json = \"all_metrics_plot.json\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import altair as alt\n",
        "import sys\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "csv_path = os.path.join(predictions_dir, csv_output_file)\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ðŸ”¥ Error: CSV file not found at: {csv_path}\")\n",
        "    print(\"   Please run the previous cells to generate and update the CSV.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Check if required columns exist\n",
        "required_cols = ['sequence_name', 'avg_plddt', 'ptm', 'rmsd_to_exp (Ã…)']\n",
        "if not all(col in df.columns for col in required_cols):\n",
        "    print(f\"ðŸ”¥ Error: The CSV file is missing one or more required columns.\")\n",
        "    print(f\"   Required: {required_cols}\")\n",
        "    print(f\"   Found: {list(df.columns)}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 2. Prepare Data for Plotting ---\n",
        "\n",
        "# --- NEW: Divide pLDDT by 100 ---\n",
        "print(\"Applying pLDDT / 100 transformation...\")\n",
        "df['avg_plddt'] = df['avg_plddt'] / 100.0\n",
        "# ---\n",
        "\n",
        "# Create the 'model_type' column for coloring\n",
        "if native_sequence_name not in df['sequence_name'].values:\n",
        "    print(f\"âš ï¸ Warning: Native sequence name '{native_sequence_name}' not found in CSV.\")\n",
        "    print(\"   All points will be colored orange.\")\n",
        "    df['model_type'] = \"Designed\"\n",
        "else:\n",
        "    df['model_type'] = np.where(\n",
        "        df['sequence_name'] == native_sequence_name,\n",
        "        'Native',\n",
        "        'Designed'\n",
        "    )\n",
        "\n",
        "# \"Melt\" the DataFrame from wide to long format\n",
        "df_melted = df.melt(\n",
        "    id_vars=['sequence_name', 'model_type'],\n",
        "    value_vars=['avg_plddt', 'ptm', 'rmsd_to_exp (Ã…)'],\n",
        "    var_name='Metric',\n",
        "    value_name='Value'\n",
        ")\n",
        "\n",
        "# --- UPDATED: Clean up metric names for better plot titles ---\n",
        "df_melted['Metric'] = df_melted['Metric'].replace({\n",
        "    'avg_plddt': 'pLDDT / 100',  # <-- Title changed here\n",
        "    'ptm': 'pTM Score',\n",
        "    'rmsd_to_exp (Ã…)': 'RMSD (Ã…)'\n",
        "})\n",
        "\n",
        "# --- 3. Create the Interactive Altair Plot ---\n",
        "print(f\"Generating interactive plot...\")\n",
        "\n",
        "# Define the custom color scale\n",
        "color_scale = alt.Scale(domain=['Native', 'Designed'],\n",
        "                        range=['red', 'orange'])\n",
        "\n",
        "# Create the base chart\n",
        "base = alt.Chart(df_melted).mark_circle(size=80, opacity=0.7).encode(\n",
        "    # X-axis: Native vs. Designed (no title, labels at bottom)\n",
        "    x=alt.X('model_type:N', title=None, axis=alt.Axis(labels=True, ticks=False, title=\"\")),\n",
        "\n",
        "    # Y-axis: The metric's value\n",
        "    y=alt.Y('Value:Q', title='Value'),\n",
        "\n",
        "    # Color based on type\n",
        "    color=alt.Color('model_type:N', scale=color_scale, legend=alt.Legend(title=\"Model Type\")),\n",
        "\n",
        "    # Show this information on hover\n",
        "    tooltip=[\n",
        "        alt.Tooltip('sequence_name:N', title='Model'),\n",
        "        alt.Tooltip('Metric:N', title='Metric'),\n",
        "        alt.Tooltip('Value:Q', title='Value', format='.3f') # Use .3f for 0-1 scale\n",
        "    ]\n",
        ").properties(\n",
        "    # --- NEW: Make each plot wider ---\n",
        "    width=100\n",
        ").interactive() # Make the chart interactive (zoom/pan)\n",
        "\n",
        "# Create the final faceted chart\n",
        "chart = base.facet(\n",
        "    # Create one column for each \"Metric\".\n",
        "    # The header for each facet will be the metric's name\n",
        "    column=alt.Column('Metric:N', header=alt.Header(\n",
        "        titleOrient=\"top\",\n",
        "        labelOrient=\"top\"\n",
        "    ))\n",
        ").resolve_scale(\n",
        "    # Make the Y-axis independent for each plot\n",
        "    y='independent'\n",
        ")\n",
        "\n",
        "# --- 4. Save and Display the Plot ---\n",
        "json_path = os.path.join(predictions_dir, plot_output_file_json)\n",
        "chart.save(json_path)\n",
        "\n",
        "print(f\"âœ… Successfully saved interactive plot to: {json_path}\")\n",
        "\n",
        "# Display the chart in the Colab output\n",
        "chart"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8ifkiA22okZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK7X9T44pWb7",
        "cellView": "form"
      },
      "source": [
        "#@title 4) Display the Aligned 3D Structure {run: \"auto\"}\n",
        "import py3Dmol\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from colabfold.colabfold import plot_plddt_legend\n",
        "from colabfold.colabfold import pymol_color_list, alphabet_list\n",
        "import sys # Added for error checking\n",
        "from pathlib import Path\n",
        "\n",
        "#@markdown ### 1. PDB Location\n",
        "#@markdown ---\n",
        "#@markdown Folder where the aligned structures were saved:\n",
        "aligned_structures_folder = \"aligned\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Jobname of the sequence to display:**\n",
        "#@markdown (e.g., \"1LVM_A_native\", \"1LVM_A_sample1\")\n",
        "jobname = \"protein_sample1\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Display Options\n",
        "#@markdown ---\n",
        "color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
        "show_sidechains = False #@param {type:\"boolean\"}\n",
        "show_mainchains = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown **Overlay the reference structure?**\n",
        "show_reference = True #@param {type:\"boolean\"}\n",
        "#@markdown Path to the saved reference PDB:\n",
        "reference_pdb_path = \"aligned/protein_ref.pdb\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "# --- Find the aligned PDB file ---\n",
        "pdb_pattern = f\"{aligned_structures_folder}/{jobname}_aligned.pdb\"\n",
        "pdb_file_list = glob.glob(pdb_pattern)\n",
        "\n",
        "def show_pdb(\n",
        "    predicted_pdb_path,\n",
        "    show_reference=False,\n",
        "    reference_pdb_path=None,\n",
        "    show_sidechains=False,\n",
        "    show_mainchains=False,\n",
        "    color=\"lDDT\"\n",
        "):\n",
        "\n",
        "    view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "\n",
        "    # --- 1. Add Reference Structure (if requested) ---\n",
        "    if show_reference:\n",
        "        try:\n",
        "            view.addModel(open(reference_pdb_path,'r').read(),'pdb')\n",
        "            # Style the *first* model (index 0) as gray\n",
        "            view.setStyle({'model': 0}, {'cartoon': {'color': 'gray'}})\n",
        "        except FileNotFoundError:\n",
        "            print(f\"âš ï¸ Warning: Reference PDB not found at {reference_pdb_path}. Skipping.\")\n",
        "\n",
        "    # --- 2. Add Predicted Structure ---\n",
        "    view.addModel(open(predicted_pdb_path,'r').read(),'pdb')\n",
        "    # Style the *last added* model (index -1)\n",
        "    model_style = {'model': -1}\n",
        "\n",
        "    if color == \"lDDT\":\n",
        "        view.setStyle(model_style, {'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "    elif color == \"rainbow\":\n",
        "        view.setStyle(model_style, {'cartoon': {'color':'spectrum'}})\n",
        "    elif color == \"chain\":\n",
        "        # Simple chain coloring\n",
        "        view.setStyle(model_style, {'cartoon': {'color':'chain'}})\n",
        "\n",
        "    if show_sidechains:\n",
        "        BB = ['C','O','N']\n",
        "        view.addStyle({'and':[model_style, {'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                            {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "        view.addStyle({'and':[model_style, {'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                            {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "        view.addStyle({'and':[model_style, {'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                            {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    if show_mainchains:\n",
        "        BB = ['C','O','N','CA']\n",
        "        view.addStyle({'and':[model_style, {'atom':BB}]},\n",
        "                            {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "    view.zoomTo()\n",
        "    return view\n",
        "\n",
        "# --- Display the structure ---\n",
        "if not pdb_file_list:\n",
        "    print(f\"ðŸ”¥ Error: Could not find aligned PDB file.\")\n",
        "    print(f\"   Searched for pattern: {pdb_pattern}\")\n",
        "    print(f\"   Please check 'aligned_structures_folder' and 'jobname'.\")\n",
        "    print(f\"   (Did you run the RMSD script to generate the aligned files?)\")\n",
        "else:\n",
        "    pdb_to_show = pdb_file_list[0]\n",
        "    print(f\"Displaying: {pdb_to_show}\")\n",
        "    if show_reference:\n",
        "        print(f\"Overlaying: {reference_pdb_path}\")\n",
        "\n",
        "    view = show_pdb(\n",
        "        pdb_to_show,\n",
        "        show_reference=show_reference,\n",
        "        reference_pdb_path=reference_pdb_path,\n",
        "        show_sidechains=show_sidechains,\n",
        "        show_mainchains=show_mainchains,\n",
        "        color=color\n",
        "    )\n",
        "    view.show()\n",
        "\n",
        "    if color == \"lDDT\":\n",
        "        plot_plddt_legend().show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}