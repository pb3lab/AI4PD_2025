{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4yBrceuFbf3"
      },
      "source": [
        "# **Native sequence redesign using ProteinMPNN and evolutionary information (v2)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pb3lab/AI4PD_2025/blob/main/tutorials/native_enzdes/native_enzdes.ipynb)"
      ],
      "metadata": {
        "id": "_49KeNzjry_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we will exemplify how to combine ProteinMPNN with experimental information about active site residues and evolutionary information – in the form conservation of residues at difference sequence identity percentages from a multiple sequence alignment (MSA) – to perform **native sequence redesign of natural enzymes**.\n",
        "\n",
        "This tutorial is largely based on the breakthough article by **Kiera Sumida**, published in [JACS in 2024](https://pubs.acs.org/doi/10.1021/jacs.3c10941), with some key differences and similarities:\n",
        "1) As in the original article, active site residues are defined as residues containing backbone atoms within 7 Å or sidechains atoms within 6 Å of the ligand.\n",
        "2) We are using an MSA generated using [MMseqs2](https://github.com/soedinglab/MMseqs2) instead of [HHblits](https://github.com/soedinglab/hh-suite) for retrieven homologous enzymes to generate an MSA. The primary reason for using MMseqs2 over HHblits is its massive speed advantage.\n",
        "3) In the original article, four iterative HHblits searches were performed against the UniRef30 database at E-value cutoffs of 1e-50, 1e-30, 1e-10 and 1e-4. Here, the database being used by MMseqs2 are built from extensive sequence sets like UniRef30 and other environmental sequences\n",
        "4) We maintained the filtering of the sequences in the MSA at 90% sequence identity, 50% coverage and 30% minimum query identity.\n",
        "5) In the original article, each position in the MSA was ranked based on how highly conserved the most frequent amino acid identity was, selecting the top 30%, 50%, and 70% most conserved positions to fix. Here, we are only fixing the top 70% most conserved positions.\n",
        "6) We are using the same ProteinMPNN model, trained with 0.2 Å applied to the training set of protein bacbones.\n",
        "7) Three sampling temperatures were tried for ProteinMPNN during the protein sequence generation stage (0.1, 0.2, 0.3), whereas only one 0.2 is used in this tutorial.\n",
        "8) Only 4 sequences are being generated, whereas 144 sequences were generated in the original article.\n",
        "9) Only model 4 is being used for the AlphaFold predictions, as in the original article, but we are only filtering candidates based on pLDDT >85, and not on Cα RMSD < 2 Å. This will be added in the future."
      ],
      "metadata": {
        "id": "7NPqtfGm5jgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0. Install the different packages required to run this tutorial\n",
        "\n",
        "### **Please install all the different dependencies at the beginning of the tutorial in the order they are indicated in this notebook.**"
      ],
      "metadata": {
        "id": "bB9h6Pn8BAzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Install ProteinMPNN\n",
        "!pip install jupyter_bokeh --quiet\n",
        "import json, time, os, sys, glob\n",
        "\n",
        "if not os.path.isdir(\"ProteinMPNN\"):\n",
        "  os.system(\"git clone -q https://github.com/dauparas/ProteinMPNN.git\")\n",
        "sys.path.append('/content/ProteinMPNN')"
      ],
      "metadata": {
        "id": "sEi7JwX4dWaV",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8f7673-984b-4b7e-e727-67618a3c0e1e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/148.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.6/148.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/139.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m153.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Setup ProteinMPNN model\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split, Subset\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import os.path\n",
        "from protein_mpnn_utils import loss_nll, loss_smoothed, gather_edges, gather_nodes, gather_nodes_t, cat_neighbors_nodes, _scores, _S_to_seq, tied_featurize, parse_PDB\n",
        "from protein_mpnn_utils import StructureDataset, StructureDatasetPDB, ProteinMPNN\n",
        "\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "#v_48_010=version with 48 edges 0.10A noise\n",
        "model_name = \"v_48_020\" #@param [\"v_48_002\", \"v_48_010\", \"v_48_020\", \"v_48_030\"]\n",
        "\n",
        "\n",
        "backbone_noise=0.00               # Standard deviation of Gaussian noise to add to backbone atoms\n",
        "\n",
        "path_to_model_weights='/content/ProteinMPNN/vanilla_model_weights'\n",
        "hidden_dim = 128\n",
        "num_layers = 3\n",
        "model_folder_path = path_to_model_weights\n",
        "if model_folder_path[-1] != '/':\n",
        "    model_folder_path = model_folder_path + '/'\n",
        "checkpoint_path = model_folder_path + f'{model_name}.pt'\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "print('Number of edges:', checkpoint['num_edges'])\n",
        "noise_level_print = checkpoint['noise_level']\n",
        "print(f'Training noise level: {noise_level_print}A')\n",
        "model = ProteinMPNN(num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
        "model.to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(\"Model loaded\")"
      ],
      "metadata": {
        "id": "HNvZ8PSydflj",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a88bd682-0e1d-47a5-90c1-deec3d0b933c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of edges: 48\n",
            "Training noise level: 0.2A\n",
            "Model loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Helper functions for ProteinMPNN\n",
        "def make_tied_positions_for_homomers(pdb_dict_list):\n",
        "    my_dict = {}\n",
        "    for result in pdb_dict_list:\n",
        "        all_chain_list = sorted([item[-1:] for item in list(result) if item[:9]=='seq_chain']) #A, B, C, ...\n",
        "        tied_positions_list = []\n",
        "        chain_length = len(result[f\"seq_chain_{all_chain_list[0]}\"])\n",
        "        for i in range(1,chain_length+1):\n",
        "            temp_dict = {}\n",
        "            for j, chain in enumerate(all_chain_list):\n",
        "                temp_dict[chain] = [i] #needs to be a list\n",
        "            tied_positions_list.append(temp_dict)\n",
        "        my_dict[result['name']] = tied_positions_list\n",
        "    return my_dict"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p8UCmlCnvIah"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4) Setup ColabFold without AMBER (i.e. without relax)\n",
        "\n",
        "from sys import version_info\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "use_amber = False\n",
        "use_templates = True\n",
        "python_version = python_version"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uubxVRG9JjHd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5) Install dependencies for running MMseqs2 and ColabFold\n",
        "%%time\n",
        "%%bash -s $use_amber $use_templates $python_version\n",
        "\n",
        "set -e\n",
        "\n",
        "USE_AMBER=$1\n",
        "USE_TEMPLATES=$2\n",
        "PYTHON_VERSION=$3\n",
        "\n",
        "if [ ! -f COLABFOLD_READY ]; then\n",
        "  # install dependencies\n",
        "  # We have to use \"--no-warn-conflicts\" because colab already has a lot preinstalled with requirements different to ours\n",
        "  pip install -q --no-warn-conflicts \"colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold\"\n",
        "  if [ -n \"${TPU_NAME}\" ]; then\n",
        "    pip install -q --no-warn-conflicts -U dm-haiku==0.0.10 jax==0.3.25\n",
        "  fi\n",
        "  ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\n",
        "  ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\n",
        "  # hack to fix TF crash\n",
        "  rm -f /usr/local/lib/python3.*/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so\n",
        "  touch COLABFOLD_READY\n",
        "fi\n",
        "\n",
        "# Download params (~1min)\n",
        "python -m colabfold.download\n",
        "\n",
        "# setup conda\n",
        "if [ ${USE_AMBER} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
        "  if [ ! -f CONDA_READY ]; then\n",
        "    wget -qnc https://github.com/conda-forge/miniforge/releases/download/25.3.1-0/Miniforge3-25.3.1-0-Linux-x86_64.sh\n",
        "    bash Miniforge3-25.3.1-0-Linux-x86_64.sh -bfp /usr/local 2>&1 1>/dev/null\n",
        "    rm Miniforge3-25.3.1-0-Linux-x86_64.sh\n",
        "    conda config --set auto_update_conda false\n",
        "    touch CONDA_READY\n",
        "  fi\n",
        "fi\n",
        "# setup template search\n",
        "if [ ${USE_TEMPLATES} == \"True\" ] && [ ! -f HH_READY ]; then\n",
        "  conda install -y -q -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python=\"${PYTHON_VERSION}\" 2>&1 1>/dev/null\n",
        "  touch HH_READY\n",
        "fi\n",
        "# setup openmm for amber refinement\n",
        "if [ ${USE_AMBER} == \"True\" ] && [ ! -f AMBER_READY ]; then\n",
        "  conda install -y -q -c conda-forge openmm=8.2.0 python=\"${PYTHON_VERSION}\" pdbfixer 2>&1 1>/dev/null\n",
        "  touch AMBER_READY\n",
        "fi"
      ],
      "metadata": {
        "id": "AzIKiDiCaHAn",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77760abe-efc4-49b3-b09c-353de94a251e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 248.4/248.4 kB 10.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 86.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 373.8/373.8 kB 32.7 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 259.0/259.0 MB 2.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 130.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.7/76.7 kB 7.4 MB/s eta 0:00:00\n",
            "warning  libmamba [python-3.12.11-h9e4cc4f_0_cpython] The following files were already present in the environment:\n",
            "    - bin/python\n",
            "warning  libmamba [charset-normalizer-3.4.2-pyhd8ed1ab_0] The following files were already present in the environment:\n",
            "    - bin/normalizer\n",
            "warning  libmamba [distro-1.9.0-pyhd8ed1ab_1] The following files were already present in the environment:\n",
            "    - bin/distro\n",
            "warning  libmamba [jsonpointer-3.0.0-py312h7900ff3_1] The following files were already present in the environment:\n",
            "    - bin/jsonpointer\n",
            "warning  libmamba [wheel-0.45.1-pyhd8ed1ab_1] The following files were already present in the environment:\n",
            "    - bin/wheel\n",
            "warning  libmamba [jsonpatch-1.33-pyhd8ed1ab_1] The following files were already present in the environment:\n",
            "    - bin/jsondiff\n",
            "    - bin/jsonpatch\n",
            "warning  libmamba [pip-25.1.1-pyh8b19718_0] The following files were already present in the environment:\n",
            "    - bin/pip\n",
            "    - bin/pip3\n",
            "warning  libmamba [tqdm-4.67.1-pyhd8ed1ab_1] The following files were already present in the environment:\n",
            "    - bin/tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   0%|          | 0/4099624960 [00:00<?, ?it/s]/content/colabfold/download.py:85: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  file.extractall(path=params_dir)\n",
            "\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   0%|          | 12.7M/3.82G [00:00<00:30, 133MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   1%|          | 45.1M/3.82G [00:00<00:15, 255MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   2%|▏         | 78.4M/3.82G [00:00<00:13, 298MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   3%|▎         | 107M/3.82G [00:00<00:26, 150MB/s] \rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   3%|▎         | 127M/3.82G [00:00<00:24, 159MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   4%|▍         | 151M/3.82G [00:00<00:21, 180MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   5%|▍         | 177M/3.82G [00:00<00:19, 205MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   5%|▌         | 204M/3.82G [00:01<00:17, 223MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   6%|▌         | 229M/3.82G [00:01<00:16, 234MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   7%|▋         | 257M/3.82G [00:01<00:15, 250MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   7%|▋         | 282M/3.82G [00:01<00:15, 250MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   8%|▊         | 307M/3.82G [00:01<00:15, 252MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   8%|▊         | 332M/3.82G [00:01<00:14, 256MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:   9%|▉         | 358M/3.82G [00:01<00:14, 259MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  10%|▉         | 383M/3.82G [00:01<00:14, 263MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  10%|█         | 409M/3.82G [00:01<00:14, 245MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  11%|█         | 434M/3.82G [00:01<00:14, 251MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  12%|█▏        | 459M/3.82G [00:02<00:19, 182MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  12%|█▏        | 485M/3.82G [00:02<00:17, 205MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  13%|█▎        | 509M/3.82G [00:02<00:16, 214MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  14%|█▎        | 534M/3.82G [00:02<00:15, 226MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  14%|█▍        | 564M/3.82G [00:02<00:13, 251MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  15%|█▌        | 589M/3.82G [00:02<00:13, 249MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  16%|█▌        | 614M/3.82G [00:02<00:13, 252MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  16%|█▋        | 639M/3.82G [00:02<00:13, 250MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  17%|█▋        | 666M/3.82G [00:03<00:13, 260MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  18%|█▊        | 691M/3.82G [00:03<00:13, 255MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  18%|█▊        | 718M/3.82G [00:03<00:12, 262MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  19%|█▉        | 745M/3.82G [00:03<00:12, 268MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  20%|█▉        | 771M/3.82G [00:03<00:12, 268MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  20%|██        | 797M/3.82G [00:03<00:12, 270MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  21%|██        | 824M/3.82G [00:03<00:11, 275MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  22%|██▏       | 851M/3.82G [00:03<00:11, 277MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  22%|██▏       | 878M/3.82G [00:03<00:11, 276MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  23%|██▎       | 904M/3.82G [00:04<00:13, 242MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  24%|██▍       | 931M/3.82G [00:04<00:12, 252MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  25%|██▍       | 958M/3.82G [00:04<00:11, 262MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  25%|██▌       | 985M/3.82G [00:04<00:11, 267MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  26%|██▌       | 0.99G/3.82G [00:04<00:11, 270MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  27%|██▋       | 1.01G/3.82G [00:04<00:12, 238MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  27%|██▋       | 1.04G/3.82G [00:04<00:13, 222MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  28%|██▊       | 1.06G/3.82G [00:04<00:12, 230MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  28%|██▊       | 1.09G/3.82G [00:04<00:12, 243MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  29%|██▉       | 1.11G/3.82G [00:05<00:12, 236MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  30%|██▉       | 1.13G/3.82G [00:05<00:11, 244MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  30%|███       | 1.16G/3.82G [00:05<00:11, 241MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  31%|███       | 1.18G/3.82G [00:05<00:11, 247MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  32%|███▏      | 1.21G/3.82G [00:05<00:10, 258MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  32%|███▏      | 1.23G/3.82G [00:05<00:10, 257MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  33%|███▎      | 1.26G/3.82G [00:05<00:10, 262MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  34%|███▎      | 1.28G/3.82G [00:05<00:10, 268MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  34%|███▍      | 1.31G/3.82G [00:05<00:09, 273MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  35%|███▍      | 1.34G/3.82G [00:05<00:10, 260MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  36%|███▌      | 1.36G/3.82G [00:06<00:11, 239MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  36%|███▌      | 1.38G/3.82G [00:06<00:10, 242MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  37%|███▋      | 1.41G/3.82G [00:06<00:10, 245MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  37%|███▋      | 1.43G/3.82G [00:06<00:10, 250MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  38%|███▊      | 1.46G/3.82G [00:06<00:10, 252MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  39%|███▊      | 1.48G/3.82G [00:06<00:10, 248MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  39%|███▉      | 1.50G/3.82G [00:06<00:09, 252MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  40%|███▉      | 1.53G/3.82G [00:06<00:09, 250MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  41%|████      | 1.55G/3.82G [00:06<00:09, 250MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  41%|████      | 1.57G/3.82G [00:06<00:09, 254MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  42%|████▏     | 1.60G/3.82G [00:07<00:09, 243MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  42%|████▏     | 1.62G/3.82G [00:07<00:09, 241MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  43%|████▎     | 1.65G/3.82G [00:07<00:09, 246MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  44%|████▎     | 1.67G/3.82G [00:07<00:09, 251MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  44%|████▍     | 1.70G/3.82G [00:07<00:08, 259MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  45%|████▌     | 1.72G/3.82G [00:07<00:08, 256MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  46%|████▌     | 1.74G/3.82G [00:07<00:10, 218MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  46%|████▌     | 1.76G/3.82G [00:08<00:18, 116MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  47%|████▋     | 1.78G/3.82G [00:08<00:23, 93.2MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  47%|████▋     | 1.79G/3.82G [00:08<00:24, 89.0MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  47%|████▋     | 1.81G/3.82G [00:08<00:19, 108MB/s] \rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  48%|████▊     | 1.83G/3.82G [00:08<00:16, 130MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  49%|████▊     | 1.85G/3.82G [00:08<00:14, 146MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  49%|████▉     | 1.87G/3.82G [00:09<00:15, 133MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  49%|████▉     | 1.89G/3.82G [00:09<00:14, 142MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  50%|████▉     | 1.91G/3.82G [00:09<00:12, 160MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  50%|█████     | 1.93G/3.82G [00:09<00:11, 176MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  51%|█████     | 1.95G/3.82G [00:09<00:10, 184MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  51%|█████▏    | 1.97G/3.82G [00:09<00:10, 184MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  52%|█████▏    | 1.98G/3.82G [00:09<00:11, 177MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  53%|█████▎    | 2.00G/3.82G [00:09<00:10, 190MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  53%|█████▎    | 2.02G/3.82G [00:10<00:12, 152MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  53%|█████▎    | 2.04G/3.82G [00:10<00:13, 138MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  54%|█████▍    | 2.06G/3.82G [00:10<00:12, 154MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  54%|█████▍    | 2.07G/3.82G [00:10<00:12, 154MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  55%|█████▍    | 2.10G/3.82G [00:10<00:10, 182MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  56%|█████▌    | 2.12G/3.82G [00:10<00:08, 209MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  56%|█████▋    | 2.15G/3.82G [00:10<00:07, 225MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  57%|█████▋    | 2.17G/3.82G [00:12<00:39, 44.6MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  57%|█████▋    | 2.19G/3.82G [00:14<01:21, 21.5MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  58%|█████▊    | 2.21G/3.82G [00:14<00:57, 29.9MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  58%|█████▊    | 2.22G/3.82G [00:14<00:48, 35.4MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  59%|█████▊    | 2.24G/3.82G [00:14<00:39, 42.9MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  59%|█████▉    | 2.25G/3.82G [00:14<00:30, 55.3MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  59%|█████▉    | 2.27G/3.82G [00:15<00:24, 67.7MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  60%|█████▉    | 2.28G/3.82G [00:15<00:20, 80.9MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  60%|██████    | 2.30G/3.82G [00:15<00:16, 98.8MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  61%|██████    | 2.33G/3.82G [00:15<00:12, 129MB/s] \rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  62%|██████▏   | 2.35G/3.82G [00:15<00:10, 155MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  62%|██████▏   | 2.37G/3.82G [00:15<00:08, 173MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  63%|██████▎   | 2.39G/3.82G [00:15<00:07, 197MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  63%|██████▎   | 2.42G/3.82G [00:15<00:06, 218MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  64%|██████▍   | 2.45G/3.82G [00:15<00:06, 229MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  65%|██████▍   | 2.47G/3.82G [00:15<00:06, 220MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  65%|██████▌   | 2.49G/3.82G [00:16<00:06, 232MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  66%|██████▌   | 2.52G/3.82G [00:16<00:06, 224MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  67%|██████▋   | 2.54G/3.82G [00:16<00:05, 234MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  67%|██████▋   | 2.56G/3.82G [00:16<00:05, 241MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  68%|██████▊   | 2.59G/3.82G [00:16<00:05, 245MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  68%|██████▊   | 2.61G/3.82G [00:16<00:05, 238MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  69%|██████▉   | 2.64G/3.82G [00:16<00:05, 249MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  70%|██████▉   | 2.66G/3.82G [00:16<00:04, 252MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  70%|███████   | 2.69G/3.82G [00:16<00:04, 262MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  71%|███████   | 2.71G/3.82G [00:16<00:04, 264MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  72%|███████▏  | 2.74G/3.82G [00:17<00:04, 264MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  72%|███████▏  | 2.76G/3.82G [00:17<00:04, 264MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  73%|███████▎  | 2.79G/3.82G [00:17<00:04, 269MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  74%|███████▎  | 2.81G/3.82G [00:17<00:04, 257MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  74%|███████▍  | 2.84G/3.82G [00:17<00:04, 249MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  75%|███████▍  | 2.86G/3.82G [00:17<00:04, 239MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  76%|███████▌  | 2.88G/3.82G [00:17<00:04, 245MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  76%|███████▌  | 2.91G/3.82G [00:17<00:03, 250MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  77%|███████▋  | 2.93G/3.82G [00:17<00:03, 254MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  77%|███████▋  | 2.96G/3.82G [00:17<00:03, 260MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  78%|███████▊  | 2.99G/3.82G [00:18<00:03, 269MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  79%|███████▉  | 3.01G/3.82G [00:18<00:03, 261MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  80%|███████▉  | 3.04G/3.82G [00:18<00:03, 262MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  80%|████████  | 3.06G/3.82G [00:18<00:03, 248MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  81%|████████  | 3.09G/3.82G [00:18<00:03, 223MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  81%|████████▏ | 3.11G/3.82G [00:18<00:03, 219MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  82%|████████▏ | 3.13G/3.82G [00:18<00:03, 226MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  83%|████████▎ | 3.16G/3.82G [00:18<00:02, 244MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  83%|████████▎ | 3.18G/3.82G [00:18<00:02, 247MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  84%|████████▍ | 3.21G/3.82G [00:19<00:02, 253MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  85%|████████▍ | 3.23G/3.82G [00:19<00:02, 259MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  85%|████████▌ | 3.26G/3.82G [00:19<00:02, 258MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  86%|████████▌ | 3.28G/3.82G [00:19<00:02, 256MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  87%|████████▋ | 3.31G/3.82G [00:19<00:02, 256MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  87%|████████▋ | 3.33G/3.82G [00:19<00:02, 261MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  88%|████████▊ | 3.36G/3.82G [00:19<00:01, 249MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  89%|████████▊ | 3.38G/3.82G [00:19<00:01, 251MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  89%|████████▉ | 3.40G/3.82G [00:19<00:01, 250MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  90%|████████▉ | 3.43G/3.82G [00:20<00:01, 254MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  90%|█████████ | 3.45G/3.82G [00:20<00:01, 256MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  91%|█████████ | 3.48G/3.82G [00:20<00:01, 258MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  92%|█████████▏| 3.50G/3.82G [00:20<00:01, 260MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  92%|█████████▏| 3.52G/3.82G [00:20<00:01, 250MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  93%|█████████▎| 3.55G/3.82G [00:20<00:01, 224MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  93%|█████████▎| 3.57G/3.82G [00:20<00:01, 212MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  94%|█████████▍| 3.59G/3.82G [00:20<00:01, 198MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  95%|█████████▍| 3.61G/3.82G [00:20<00:01, 192MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  95%|█████████▌| 3.63G/3.82G [00:21<00:00, 204MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  96%|█████████▌| 3.65G/3.82G [00:21<00:00, 211MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  96%|█████████▌| 3.67G/3.82G [00:21<00:00, 220MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  97%|█████████▋| 3.70G/3.82G [00:21<00:00, 221MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  97%|█████████▋| 3.72G/3.82G [00:21<00:00, 192MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  98%|█████████▊| 3.73G/3.82G [00:21<00:00, 176MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  98%|█████████▊| 3.76G/3.82G [00:21<00:00, 188MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  99%|█████████▉| 3.77G/3.82G [00:21<00:00, 190MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold:  99%|█████████▉| 3.79G/3.82G [00:21<00:00, 177MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold: 100%|█████████▉| 3.81G/3.82G [00:22<00:00, 184MB/s]\rDownloading alphafold2_multimer_v3 weights to /root/.cache/colabfold: 100%|██████████| 3.82G/3.82G [00:22<00:00, 186MB/s]\n",
            "\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   0%|          | 0/3722752000 [00:00<?, ?it/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   0%|          | 4.46M/3.47G [00:00<01:19, 46.8MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   1%|          | 25.9M/3.47G [00:00<00:24, 151MB/s] \rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   1%|          | 40.3M/3.47G [00:00<00:24, 148MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   2%|▏         | 55.5M/3.47G [00:00<00:24, 152MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   2%|▏         | 70.0M/3.47G [00:00<00:24, 147MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   2%|▏         | 88.2M/3.47G [00:00<00:22, 161MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   3%|▎         | 105M/3.47G [00:00<00:21, 166MB/s] \rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   3%|▎         | 122M/3.47G [00:00<00:21, 169MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   4%|▍         | 138M/3.47G [00:00<00:21, 166MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   4%|▍         | 156M/3.47G [00:01<00:20, 175MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   5%|▍         | 173M/3.47G [00:01<00:25, 140MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   5%|▌         | 188M/3.47G [00:01<00:25, 137MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   6%|▌         | 202M/3.47G [00:01<00:34, 101MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   6%|▌         | 213M/3.47G [00:01<00:34, 101MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   6%|▋         | 224M/3.47G [00:01<00:37, 92.0MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   7%|▋         | 242M/3.47G [00:01<00:30, 112MB/s] \rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   7%|▋         | 256M/3.47G [00:02<00:30, 115MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   8%|▊         | 280M/3.47G [00:02<00:23, 148MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   9%|▊         | 305M/3.47G [00:02<00:19, 179MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:   9%|▉         | 326M/3.47G [00:02<00:17, 189MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  10%|▉         | 345M/3.47G [00:02<00:17, 187MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  10%|█         | 364M/3.47G [00:02<00:24, 135MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  11%|█         | 390M/3.47G [00:02<00:19, 166MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  12%|█▏        | 418M/3.47G [00:02<00:16, 197MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  12%|█▏        | 443M/3.47G [00:03<00:15, 212MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  13%|█▎        | 465M/3.47G [00:03<00:16, 195MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  14%|█▎        | 486M/3.47G [00:03<00:17, 181MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  14%|█▍        | 512M/3.47G [00:03<00:18, 174MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  15%|█▌        | 537M/3.47G [00:03<00:16, 194MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  16%|█▌        | 562M/3.47G [00:03<00:14, 212MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  17%|█▋        | 588M/3.47G [00:03<00:13, 227MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  17%|█▋        | 613M/3.47G [00:03<00:12, 237MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  18%|█▊        | 638M/3.47G [00:03<00:12, 245MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  19%|█▊        | 662M/3.47G [00:04<00:12, 235MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  19%|█▉        | 689M/3.47G [00:04<00:12, 248MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  20%|██        | 713M/3.47G [00:04<00:13, 219MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  21%|██        | 741M/3.47G [00:04<00:12, 237MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  22%|██▏       | 764M/3.47G [00:04<00:14, 207MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  22%|██▏       | 788M/3.47G [00:04<00:13, 218MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  23%|██▎       | 811M/3.47G [00:04<00:12, 223MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  24%|██▎       | 836M/3.47G [00:04<00:12, 235MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  24%|██▍       | 862M/3.47G [00:05<00:11, 245MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  25%|██▍       | 886M/3.47G [00:05<00:11, 241MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  26%|██▌       | 912M/3.47G [00:05<00:11, 251MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  26%|██▋       | 936M/3.47G [00:05<00:11, 228MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  27%|██▋       | 958M/3.47G [00:05<00:11, 229MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  28%|██▊       | 983M/3.47G [00:05<00:11, 236MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  28%|██▊       | 0.98G/3.47G [00:05<00:11, 237MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  29%|██▉       | 1.00G/3.47G [00:05<00:11, 234MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  30%|██▉       | 1.03G/3.47G [00:05<00:10, 247MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  30%|███       | 1.06G/3.47G [00:05<00:10, 252MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  31%|███       | 1.08G/3.47G [00:06<00:09, 258MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  32%|███▏      | 1.10G/3.47G [00:06<00:10, 239MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  33%|███▎      | 1.13G/3.47G [00:06<00:09, 258MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  33%|███▎      | 1.16G/3.47G [00:06<00:09, 262MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  34%|███▍      | 1.18G/3.47G [00:06<00:09, 259MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  35%|███▍      | 1.21G/3.47G [00:06<00:09, 250MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  36%|███▌      | 1.23G/3.47G [00:06<00:11, 215MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  36%|███▋      | 1.26G/3.47G [00:06<00:09, 239MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  37%|███▋      | 1.29G/3.47G [00:06<00:09, 255MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  38%|███▊      | 1.31G/3.47G [00:07<00:08, 264MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  39%|███▊      | 1.34G/3.47G [00:07<00:09, 249MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  39%|███▉      | 1.36G/3.47G [00:07<00:10, 222MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  40%|████      | 1.39G/3.47G [00:07<00:09, 230MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  41%|████      | 1.41G/3.47G [00:07<00:09, 238MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  41%|████▏     | 1.44G/3.47G [00:07<00:08, 243MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  42%|████▏     | 1.46G/3.47G [00:07<00:08, 249MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  43%|████▎     | 1.48G/3.47G [00:07<00:08, 253MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  43%|████▎     | 1.51G/3.47G [00:07<00:09, 229MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  44%|████▍     | 1.53G/3.47G [00:08<00:08, 236MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  45%|████▍     | 1.56G/3.47G [00:08<00:08, 245MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  46%|████▌     | 1.58G/3.47G [00:08<00:08, 231MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  46%|████▋     | 1.60G/3.47G [00:08<00:08, 242MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  47%|████▋     | 1.63G/3.47G [00:08<00:07, 252MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  48%|████▊     | 1.66G/3.47G [00:08<00:07, 256MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  48%|████▊     | 1.68G/3.47G [00:08<00:07, 259MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  49%|████▉     | 1.71G/3.47G [00:08<00:07, 262MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  50%|████▉     | 1.73G/3.47G [00:08<00:07, 247MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  51%|█████     | 1.75G/3.47G [00:09<00:07, 247MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  51%|█████     | 1.78G/3.47G [00:09<00:08, 224MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  52%|█████▏    | 1.80G/3.47G [00:09<00:07, 232MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  53%|█████▎    | 1.82G/3.47G [00:09<00:08, 220MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  53%|█████▎    | 1.85G/3.47G [00:09<00:07, 234MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  54%|█████▍    | 1.87G/3.47G [00:09<00:07, 242MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  55%|█████▍    | 1.89G/3.47G [00:09<00:07, 227MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  55%|█████▌    | 1.92G/3.47G [00:09<00:06, 238MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  56%|█████▌    | 1.94G/3.47G [00:09<00:06, 244MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  57%|█████▋    | 1.97G/3.47G [00:10<00:06, 240MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  57%|█████▋    | 1.99G/3.47G [00:10<00:06, 248MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  58%|█████▊    | 2.02G/3.47G [00:10<00:06, 233MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  59%|█████▉    | 2.04G/3.47G [00:10<00:06, 242MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  59%|█████▉    | 2.06G/3.47G [00:10<00:06, 226MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  60%|██████    | 2.09G/3.47G [00:10<00:06, 233MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  61%|██████    | 2.11G/3.47G [00:10<00:06, 236MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  62%|██████▏   | 2.13G/3.47G [00:10<00:05, 243MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  62%|██████▏   | 2.16G/3.47G [00:10<00:05, 256MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  63%|██████▎   | 2.19G/3.47G [00:10<00:05, 263MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  64%|██████▍   | 2.21G/3.47G [00:11<00:06, 196MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  64%|██████▍   | 2.23G/3.47G [00:11<00:06, 194MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  65%|██████▍   | 2.25G/3.47G [00:11<00:07, 177MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  65%|██████▌   | 2.27G/3.47G [00:11<00:07, 180MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  66%|██████▌   | 2.29G/3.47G [00:11<00:06, 183MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  66%|██████▋   | 2.30G/3.47G [00:12<00:16, 75.5MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  67%|██████▋   | 2.33G/3.47G [00:12<00:12, 96.3MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  68%|██████▊   | 2.35G/3.47G [00:12<00:10, 118MB/s] \rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  68%|██████▊   | 2.37G/3.47G [00:12<00:09, 129MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  69%|██████▉   | 2.39G/3.47G [00:12<00:07, 149MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  69%|██████▉   | 2.40G/3.47G [00:12<00:08, 127MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  70%|██████▉   | 2.42G/3.47G [00:13<00:08, 137MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  70%|███████   | 2.44G/3.47G [00:13<00:07, 147MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  71%|███████   | 2.46G/3.47G [00:13<00:06, 159MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  71%|███████▏  | 2.47G/3.47G [00:13<00:06, 158MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  72%|███████▏  | 2.49G/3.47G [00:13<00:06, 164MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  72%|███████▏  | 2.51G/3.47G [00:13<00:05, 172MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  73%|███████▎  | 2.53G/3.47G [00:13<00:05, 183MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  74%|███████▎  | 2.55G/3.47G [00:13<00:04, 208MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  74%|███████▍  | 2.58G/3.47G [00:13<00:04, 224MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  75%|███████▍  | 2.60G/3.47G [00:13<00:04, 206MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  76%|███████▌  | 2.62G/3.47G [00:14<00:04, 224MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  76%|███████▋  | 2.65G/3.47G [00:14<00:03, 248MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  77%|███████▋  | 2.68G/3.47G [00:14<00:03, 255MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  78%|███████▊  | 2.70G/3.47G [00:14<00:03, 255MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  79%|███████▊  | 2.73G/3.47G [00:14<00:03, 242MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  79%|███████▉  | 2.76G/3.47G [00:14<00:02, 264MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  80%|████████  | 2.78G/3.47G [00:14<00:03, 244MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  81%|████████  | 2.81G/3.47G [00:14<00:02, 254MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  82%|████████▏ | 2.83G/3.47G [00:14<00:02, 254MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  82%|████████▏ | 2.85G/3.47G [00:15<00:02, 246MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  83%|████████▎ | 2.88G/3.47G [00:15<00:02, 258MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  84%|████████▍ | 2.91G/3.47G [00:15<00:02, 264MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  85%|████████▍ | 2.93G/3.47G [00:15<00:02, 238MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  85%|████████▌ | 2.96G/3.47G [00:15<00:02, 254MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  86%|████████▌ | 2.98G/3.47G [00:15<00:02, 258MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  87%|████████▋ | 3.01G/3.47G [00:15<00:01, 260MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  87%|████████▋ | 3.03G/3.47G [00:15<00:01, 243MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  88%|████████▊ | 3.06G/3.47G [00:15<00:01, 249MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  89%|████████▉ | 3.08G/3.47G [00:15<00:01, 245MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  90%|████████▉ | 3.11G/3.47G [00:16<00:01, 255MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  90%|█████████ | 3.13G/3.47G [00:16<00:01, 253MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  91%|█████████ | 3.16G/3.47G [00:16<00:01, 259MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  92%|█████████▏| 3.18G/3.47G [00:16<00:01, 260MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  93%|█████████▎| 3.21G/3.47G [00:16<00:01, 269MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  93%|█████████▎| 3.23G/3.47G [00:16<00:00, 261MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  94%|█████████▍| 3.26G/3.47G [00:16<00:00, 261MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  95%|█████████▍| 3.28G/3.47G [00:16<00:00, 255MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  95%|█████████▌| 3.31G/3.47G [00:16<00:00, 251MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  96%|█████████▌| 3.33G/3.47G [00:17<00:00, 254MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  97%|█████████▋| 3.36G/3.47G [00:17<00:00, 264MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  98%|█████████▊| 3.38G/3.47G [00:17<00:00, 258MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  98%|█████████▊| 3.41G/3.47G [00:17<00:00, 256MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold:  99%|█████████▉| 3.43G/3.47G [00:17<00:00, 262MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold: 100%|█████████▉| 3.46G/3.47G [00:17<00:00, 264MB/s]\rDownloading AlphaFold2-ptm weights to /root/.cache/colabfold: 100%|██████████| 3.47G/3.47G [00:17<00:00, 212MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 71 ms, sys: 30.8 ms, total: 102 ms\n",
            "Wall time: 2min 22s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Once you have finished installing these dependencies, we are ready to perform the tutorial**"
      ],
      "metadata": {
        "id": "zps1RWW0BlKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. Determine the active site residues and highly conserved residues of the enzyme to fix during sequence redesign\n",
        "\n",
        "### Please follow the steps in the order they are presented in this tutorial"
      ],
      "metadata": {
        "id": "7WZL0b3nCZ4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Load Structure, Extract Sequence, and Split Protein and Ligand Chains\n",
        "#@markdown ---\n",
        "#@markdown ### 1. PDB Input\n",
        "#@markdown Choose to download from RCSB or upload your own file.\n",
        "pdb_id = 'AZO1' #@param {type:\"string\"}\n",
        "use_upload = True #@param {type:\"boolean\"}\n",
        "#@markdown > If uploading, the script will assume the first chain is the protein.\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Protein (Target) Chain\n",
        "#@markdown Provide the chain letter for your main protein target.\n",
        "target_chain_letter = 'A' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 3. Ligand\n",
        "#@markdown Choose your method for extracting the ligand.\n",
        "ligand_extraction_method = \"By HETATM Residue Name\" #@param [\"By Chain\", \"By HETATM Residue Name\"]\n",
        "#@markdown **If 'By Chain':** Provide chain letter.\n",
        "#@markdown **If 'By HETATM Residue Name':** Provide the chain letter where the ligand(s) are located, and their 3-letter name(s).\n",
        "ligand_chain_letter = 'C' #@param {type:\"string\"}\n",
        "ligand_residue_name = 'NAP' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import warnings\n",
        "\n",
        "try:\n",
        "    from Bio.PDB import (\n",
        "        PDBParser, PDBIO, Select,\n",
        "        Polypeptide, CaPPBuilder, PDBExceptions\n",
        "    )\n",
        "    from Bio.Seq import Seq\n",
        "    from Bio.SeqRecord import SeqRecord\n",
        "    from Bio import SeqIO\n",
        "except ImportError:\n",
        "    print(\"Biopython not found. Installing...\")\n",
        "    !pip install biopython\n",
        "    from Bio.PDB import (\n",
        "        PDBParser, PDBIO, Select,\n",
        "        Polypeptide, CaPPBuilder, PDBExceptions\n",
        "    )\n",
        "    from Bio.Seq import Seq\n",
        "    from Bio.SeqRecord import SeqRecord\n",
        "    from Bio import SeqIO\n",
        "\n",
        "# Suppress Biopython warnings\n",
        "warnings.filterwarnings(\"ignore\", category=PDBExceptions.PDBConstructionWarning)\n",
        "\n",
        "# --- 0A. Python-based renumbering function for the protein ---\n",
        "def renumber_protein_pdb(filename):\n",
        "    \"\"\"\n",
        "    Reads a PDB file, keeps only ATOM and TER lines, and renumbers\n",
        "    residues sequentially from 1, incrementing on 'N' atoms.\n",
        "    \"\"\"\n",
        "    renumbered_lines = []\n",
        "    current_res_num = 0\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            if line.startswith(\"ATOM\"):\n",
        "                atom_name = line[12:16].strip()\n",
        "\n",
        "                # Increment residue number if this is a Nitrogen atom\n",
        "                # This mimics the `if( $3==\"N\" ) ++num;` logic\n",
        "                if atom_name == \"N\":\n",
        "                    current_res_num += 1\n",
        "\n",
        "                # Safety check if PDB doesn't start with N\n",
        "                if current_res_num == 0:\n",
        "                    current_res_num = 1\n",
        "\n",
        "                new_res_num_str = str(current_res_num).rjust(4)\n",
        "\n",
        "                # Splice in the new res num\n",
        "                new_line = line[:22] + new_res_num_str + line[26:]\n",
        "                renumbered_lines.append(new_line)\n",
        "\n",
        "            elif line.startswith(\"TER\"):\n",
        "                renumbered_lines.append(line)\n",
        "\n",
        "        # Overwrite the original file\n",
        "        with open(filename, 'w') as f:\n",
        "            f.writelines(renumbered_lines)\n",
        "\n",
        "        print(f\"✅ Successfully filtered and renumbered {filename}\")\n",
        "        return current_res_num # Return the last residue number used\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error renumbering protein {filename}: {e}\")\n",
        "        return 0\n",
        "\n",
        "# --- 0B. Python-based renumbering function for ligands ---\n",
        "def renumber_and_filter_ligand(filename, start_res_num, new_chain_letter, is_last_file=False):\n",
        "    \"\"\"\n",
        "    Reads a PDB file, filters for ATOM/HETATM/TER, renumbers residues,\n",
        "    and sets a new chain ID. Adds END instead of TER if it's the last file.\n",
        "    \"\"\"\n",
        "    renumbered_lines = []\n",
        "    current_offset = -1 # Will be 0 on the first new residue\n",
        "    last_original_res_num_str = None\n",
        "\n",
        "    # Ensure chain letter is a single character\n",
        "    if not isinstance(new_chain_letter, str) or len(new_chain_letter) != 1:\n",
        "        print(f\"⚠️ Invalid chain letter '{new_chain_letter}'. Defaulting to 'X'.\")\n",
        "        new_chain_letter = 'X'\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            if line.startswith(\"ATOM\") or line.startswith(\"HETATM\"):\n",
        "                original_res_num_str = line[22:26] # Keep as string for comparison\n",
        "\n",
        "                if original_res_num_str != last_original_res_num_str:\n",
        "                    current_offset += 1 # Increment for each new residue\n",
        "                    last_original_res_num_str = original_res_num_str\n",
        "\n",
        "                new_res_num = start_res_num + current_offset\n",
        "                new_res_num_str = str(new_res_num).rjust(4)\n",
        "\n",
        "                # Splice in the new chain ID and new res num\n",
        "                new_line = line[:21] + new_chain_letter + new_res_num_str + line[26:]\n",
        "                renumbered_lines.append(new_line)\n",
        "\n",
        "            elif line.startswith(\"TER\"):\n",
        "                # Only append TER if it's NOT the last file\n",
        "                if not is_last_file:\n",
        "                    renumbered_lines.append(line)\n",
        "\n",
        "        # After processing all lines, add END if it IS the last file\n",
        "        if is_last_file:\n",
        "            renumbered_lines.append(\"END\\n\")\n",
        "\n",
        "        # Overwrite the original file\n",
        "        with open(filename, 'w') as f:\n",
        "            f.writelines(renumbered_lines)\n",
        "\n",
        "        final_res_num = start_res_num + current_offset\n",
        "        print(f\"✅ Successfully filtered and renumbered {filename} (Chain {new_chain_letter}, Res {start_res_num}-{final_res_num})\")\n",
        "        return final_res_num # Return the last residue number used\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error renumbering {filename}: {e}\")\n",
        "        return start_res_num - 1\n",
        "\n",
        "# --- Biopython Selectors ---\n",
        "class ProteinSelect(Select):\n",
        "    \"\"\" Selects only the ATOM records for a specific protein chain. \"\"\"\n",
        "    def __init__(self, chain_id):\n",
        "        self.chain_id = chain_id\n",
        "\n",
        "    def accept_chain(self, chain):\n",
        "        return chain.id == self.chain_id\n",
        "\n",
        "    def accept_residue(self, residue):\n",
        "        # Only accept standard residues (no HETATMs)\n",
        "        return residue.id[0] == ' '\n",
        "\n",
        "    def accept_atom(self, atom):\n",
        "        return True\n",
        "\n",
        "class LigandChainSelect(Select):\n",
        "    \"\"\" Selects all ATOM/HETATM records for a specific ligand chain. \"\"\"\n",
        "    def __init__(self, chain_id):\n",
        "        self.chain_id = chain_id\n",
        "\n",
        "    def accept_chain(self, chain):\n",
        "        return chain.id == self.chain_id\n",
        "\n",
        "    def accept_residue(self, residue):\n",
        "        return True # Accept both ATOM and HETATM\n",
        "\n",
        "    def accept_atom(self, atom):\n",
        "        return True\n",
        "\n",
        "class HetatmResidueSelect(Select):\n",
        "    \"\"\" Selects only a specific HETATM residue. \"\"\"\n",
        "    def __init__(self, target_residue):\n",
        "        self.target_residue = target_residue\n",
        "\n",
        "    def accept_chain(self, chain):\n",
        "        return chain.id == self.target_residue.get_parent().id\n",
        "\n",
        "    def accept_residue(self, residue):\n",
        "        return residue == self.target_residue\n",
        "\n",
        "    def accept_atom(self, atom):\n",
        "        return True\n",
        "\n",
        "# --- 1. Handle PDB Input (Upload vs. Download) ---\n",
        "full_pdb_filename = \"\"\n",
        "pdb_id_base = \"\"\n",
        "\n",
        "if use_upload:\n",
        "    print(\"Please upload your PDB file...\")\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            raise Exception(\"File upload cancelled.\")\n",
        "\n",
        "        uploaded_filename = next(iter(uploaded)) # Get the first filename\n",
        "        pdb_content = uploaded[uploaded_filename].decode('utf-8') # Get the file content\n",
        "\n",
        "        full_pdb_filename = \"uploaded_file.pdb\"\n",
        "        with open(full_pdb_filename, 'w') as f:\n",
        "            f.write(pdb_content) # Write the content to a local file\n",
        "\n",
        "        pdb_id_base = os.path.splitext(uploaded_filename)[0] # Use file name as base\n",
        "        print(f\"Using uploaded file: {uploaded_filename} (saved as {full_pdb_filename})\")\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error during file upload: {e}\")\n",
        "        raise SystemExit(\"File upload failed.\")\n",
        "else:\n",
        "    pdb_id_base = pdb_id\n",
        "    full_pdb_filename = f\"{pdb_id_base}.pdb\"\n",
        "    if not os.path.isfile(full_pdb_filename):\n",
        "        print(f\"Downloading {full_pdb_filename} from RCSB PDB...\")\n",
        "        !wget -q -O {full_pdb_filename} https://files.rcsb.org/download/{full_pdb_filename}\n",
        "        print(f\"Successfully downloaded {full_pdb_filename}.\")\n",
        "    else:\n",
        "        print(f\"{full_pdb_filename} already exists. Using local file.\")\n",
        "\n",
        "# --- 2. Define Output Filenames ---\n",
        "target_fasta_filename = \"protein.fasta\"\n",
        "target_pdb_filename = \"protein.pdb\"\n",
        "\n",
        "# --- 3. Load Structure and Process Chains ---\n",
        "if full_pdb_filename and os.path.exists(full_pdb_filename):\n",
        "    try:\n",
        "        print(f\"\\nLoading full structure from {full_pdb_filename}...\")\n",
        "        parser = PDBParser(QUIET=True)\n",
        "        structure = parser.get_structure(pdb_id_base, full_pdb_filename)\n",
        "        model = structure[0] # Assume first model\n",
        "        last_protein_res_num = 0\n",
        "\n",
        "        # --- 3A. Process Target Chain (Protein) ---\n",
        "        print(f\"\\nProcessing target chain: {target_chain_letter}\")\n",
        "\n",
        "        if target_chain_letter not in model:\n",
        "            if use_upload:\n",
        "                original_letter = target_chain_letter\n",
        "                target_chain_letter = next(iter(model.child_dict.keys()))\n",
        "                print(f\"Warning: Chain '{original_letter}' not found. Using first chain '{target_chain_letter}' as protein.\")\n",
        "            else:\n",
        "                print(f\"⚠️ Error: Target protein chain '{target_chain_letter}' not found in PDB. Stopping.\")\n",
        "                raise SystemExit()\n",
        "\n",
        "        protein_chain = model[target_chain_letter]\n",
        "\n",
        "        # Get sequence\n",
        "        sequence = \"\"\n",
        "        for res in protein_chain.get_residues():\n",
        "            # Check if it's a standard amino acid\n",
        "            if res.id[0] == ' ' and Polypeptide.is_aa(res.get_resname(), standard=True):\n",
        "                try:\n",
        "                    # FIX: Use the protein_letters_3to1 dictionary instead of the old function\n",
        "                    sequence += Polypeptide.protein_letters_3to1[res.get_resname()]\n",
        "                except KeyError:\n",
        "                    sequence += 'X' # Unknown residue\n",
        "\n",
        "        # MODIFIED: Set description to \"\" to only get \">protein\"\n",
        "        seq_record = SeqRecord(Seq(sequence), id=\"protein\", description=\"\")\n",
        "        with open(target_fasta_filename, 'w') as f:\n",
        "            SeqIO.write(seq_record, f, \"fasta\")\n",
        "        print(f\"✅ Successfully saved sequence to {target_fasta_filename}\")\n",
        "\n",
        "        # Save protein structure (ATOM lines only)\n",
        "        io = PDBIO()\n",
        "        io.set_structure(structure)\n",
        "        io.save(target_pdb_filename, ProteinSelect(target_chain_letter))\n",
        "        print(f\"✅ Successfully saved structure to {target_pdb_filename}\")\n",
        "\n",
        "        # Renumber protein PDB\n",
        "        last_protein_res_num = renumber_protein_pdb(target_pdb_filename)\n",
        "        if last_protein_res_num > 0:\n",
        "            print(f\"Protein renumbered. Last residue number is: {last_protein_res_num}\")\n",
        "        else:\n",
        "            print(\"⚠️ Warning: Protein renumbering failed or protein was empty.\")\n",
        "\n",
        "        # --- 3B. Process Ligand(s) ---\n",
        "        current_ligand_res_start = last_protein_res_num + 1\n",
        "        start_chain_ord = ord(target_chain_letter.upper())\n",
        "        ligand_chain_counter = 1 # To assign B, C, D...\n",
        "\n",
        "        if ligand_extraction_method == \"By Chain\":\n",
        "            print(f\"\\nProcessing ligand by chain: {ligand_chain_letter}\")\n",
        "\n",
        "            if ligand_chain_letter not in model:\n",
        "                print(f\"⚠️ Error: Ligand chain '{ligand_chain_letter}' not found.\")\n",
        "            else:\n",
        "                ligand_pdb_filename = f\"{ligand_chain_letter}_ligand.pdb\"\n",
        "                io.save(ligand_pdb_filename, LigandChainSelect(ligand_chain_letter))\n",
        "                print(f\"✅ Successfully saved structure to {ligand_pdb_filename}\")\n",
        "\n",
        "                # Renumber this ligand chain\n",
        "                current_new_chain_letter = chr(start_chain_ord + ligand_chain_counter)\n",
        "                renumber_and_filter_ligand(\n",
        "                    ligand_pdb_filename,\n",
        "                    current_ligand_res_start,\n",
        "                    current_new_chain_letter,\n",
        "                    is_last_file=True # Assume this is the only ligand\n",
        "                )\n",
        "\n",
        "        elif ligand_extraction_method == \"By HETATM Residue Name\":\n",
        "            if not ligand_residue_name:\n",
        "                print(\"⚠️ Error: Please provide a HETATM residue name to search for.\")\n",
        "            else:\n",
        "                ligand_names_to_find = [name.strip().upper() for name in ligand_residue_name.split(',')]\n",
        "                print(f\"\\nProcessing ligands by HETATM name(s): {ligand_names_to_find}\")\n",
        "\n",
        "                if ligand_chain_letter not in model:\n",
        "                     print(f\"⚠️ Error: Specified ligand chain '{ligand_chain_letter}' not found. Cannot search for HETATMs.\")\n",
        "                else:\n",
        "                    print(f\"Searching for HETATMs on chain '{ligand_chain_letter}'...\")\n",
        "\n",
        "                    search_chain = model[ligand_chain_letter]\n",
        "                    matching_residues = []\n",
        "\n",
        "                    for res in search_chain.get_residues():\n",
        "                        res_name_upper = res.get_resname().strip().upper()\n",
        "                        # Check if it's a HETATM and matches one of the names\n",
        "                        if res.id[0] != ' ' and res_name_upper in ligand_names_to_find:\n",
        "                            matching_residues.append(res)\n",
        "\n",
        "                    found_ligands = len(matching_residues)\n",
        "                    ligand_file_counts = {}\n",
        "\n",
        "                    if found_ligands == 0:\n",
        "                        print(f\"⚠️ No HETATM residues matching {ligand_names_to_find} found on chain {ligand_chain_letter}.\")\n",
        "                    else:\n",
        "                        print(f\"Found {found_ligands} matching HETATM residues. Processing...\")\n",
        "\n",
        "                        for i, res in enumerate(matching_residues):\n",
        "                            is_last = (i == found_ligands - 1) # Check if this is the last item\n",
        "                            res_name_upper = res.get_resname().strip().upper()\n",
        "\n",
        "                            # Determine chain letter\n",
        "                            current_new_chain_letter = chr(start_chain_ord + ligand_chain_counter)\n",
        "                            ligand_chain_counter += 1 # Increment for the *next* ligand\n",
        "\n",
        "                            # Handle file naming to prevent overwrites\n",
        "                            count = ligand_file_counts.get(res_name_upper, 0) + 1\n",
        "                            ligand_file_counts[res_name_upper] = count\n",
        "\n",
        "                            ligand_pdb_filename = f\"{res_name_upper}.pdb\"\n",
        "                            if count > 1:\n",
        "                                ligand_pdb_filename = f\"{res_name_upper}_{count}.pdb\"\n",
        "\n",
        "                            # Save this single residue to a file\n",
        "                            io.save(ligand_pdb_filename, HetatmResidueSelect(res))\n",
        "                            print(f\"✅ Extracted HETATM {i+1}/{found_ligands} ({res_name_upper}) to {ligand_pdb_filename}\")\n",
        "\n",
        "                            # Call renumbering with the new chain letter and is_last flag\n",
        "                            last_num_used = renumber_and_filter_ligand(\n",
        "                                ligand_pdb_filename,\n",
        "                                current_ligand_res_start,\n",
        "                                current_new_chain_letter,\n",
        "                                is_last_file=is_last\n",
        "                            )\n",
        "\n",
        "                            # Update the *next* starting number\n",
        "                            current_ligand_res_start = last_num_used + 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error during PDB processing: {e}\")\n",
        "else:\n",
        "    print(\"🔥 Error: PDB file not found or specified. Halting.\")\n",
        "\n",
        "print(\"\\nProcess complete.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Kt2nJuU0HbOp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "78ceb350-440e-4a52-e2eb-26018a09b25d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your PDB file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fc3a8176-01eb-4530-8e51-93e8f23fc39b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fc3a8176-01eb-4530-8e51-93e8f23fc39b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving AZO1.pdb to AZO1.pdb\n",
            "Using uploaded file: AZO1.pdb (saved as uploaded_file.pdb)\n",
            "\n",
            "Loading full structure from uploaded_file.pdb...\n",
            "\n",
            "Processing target chain: A\n",
            "✅ Successfully saved sequence to protein.fasta\n",
            "✅ Successfully saved structure to protein.pdb\n",
            "✅ Successfully filtered and renumbered protein.pdb\n",
            "Protein renumbered. Last residue number is: 188\n",
            "\n",
            "Processing ligands by HETATM name(s): ['NAP']\n",
            "Searching for HETATMs on chain 'C'...\n",
            "Found 1 matching HETATM residues. Processing...\n",
            "✅ Extracted HETATM 1/1 (NAP) to NAP.pdb\n",
            "✅ Successfully filtered and renumbered NAP.pdb (Chain B, Res 189-189)\n",
            "\n",
            "Process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import requests\n",
        "import subprocess\n",
        "from glob import glob\n",
        "import warnings\n",
        "\n",
        "# --- Configuration ---\n",
        "#@title 2) Define Active Site Residues Around Ligand\n",
        "#@markdown ### 1. Input Files\n",
        "protein_pdb_file = \"protein.pdb\"  #@param {type:\"string\"}\n",
        "ligand_pdb_files_str = \"NAP.pdb\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### 2. Ligand Parameters\n",
        "#@markdown **Important:** The chains here MUST match the chains you assigned\n",
        "#@markdown in the previous PDB splitting step (e.g., 'B', 'C', etc.)\n",
        "ligand_chains_str = \"B\"   #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### 3. Output File\n",
        "merged_pdb_file = \"protein_ligand_complex.pdb\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### 4. Active Site Definition\n",
        "#@markdown ---\n",
        "#@markdown Define the active site by two criteria:\n",
        "#@markdown 1.  Protein **backbone** atoms ('N', 'CA', 'C', 'O') within X angstroms of the ligand.\n",
        "#@markdown 2.  Protein **sidechain** atoms (all others) within Y angstroms of the ligand.\n",
        "cutoff_backbone = 7.0  #@param {type:\"number\"}\n",
        "cutoff_sidechain = 6.0  #@param {type:\"number\"}\n",
        "\n",
        "# --- 1. Install/Import Biopython ---\n",
        "try:\n",
        "    from Bio.PDB import PDBParser, NeighborSearch, PDBExceptions\n",
        "    from Bio.PDB.Atom import Atom\n",
        "except ImportError:\n",
        "    print(\"Biopython not found. Installing...\")\n",
        "    !pip install biopython\n",
        "    from Bio.PDB import PDBParser, NeighborSearch, PDBExceptions\n",
        "    from Bio.PDB.Atom import Atom\n",
        "\n",
        "# Suppress Biopython warnings\n",
        "warnings.filterwarnings(\"ignore\", category=PDBExceptions.PDBConstructionWarning)\n",
        "\n",
        "# --- 2. Merge Protein and Ligand PDBs ---\n",
        "# This step is necessary to create a single complex for Biopython to analyze\n",
        "print(\"\\n--- Merging PDB Files ---\")\n",
        "\n",
        "processed_ligand_pdbs = [f.strip() for f in ligand_pdb_files_str.split(',')]\n",
        "\n",
        "with open(merged_pdb_file, 'w') as outfile:\n",
        "    if os.path.exists(protein_pdb_file):\n",
        "        with open(protein_pdb_file, 'r') as infile:\n",
        "            for line in infile:\n",
        "                # Filter out CRYST1, REMARK, and any existing END\n",
        "                if not line.startswith((\"CRYST1\", \"REMARK\", \"END\")):\n",
        "                    outfile.write(line)\n",
        "        print(f\"Added {protein_pdb_file} to {merged_pdb_file}\")\n",
        "    else:\n",
        "        print(f\"⚠️ Warning: Protein file {protein_pdb_file} not found. Merged file will only contain ligands.\")\n",
        "\n",
        "    # Add a TER record if protein was added and it didn't end with one\n",
        "    if os.path.exists(protein_pdb_file):\n",
        "         with open(protein_pdb_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            if lines and not lines[-1].startswith(\"TER\"):\n",
        "                outfile.write(\"TER\\n\")\n",
        "\n",
        "    for ligand_file in processed_ligand_pdbs:\n",
        "        if not os.path.exists(ligand_file):\n",
        "            print(f\"⚠️ Warning: Ligand file {ligand_file} not found. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        with open(ligand_file, 'r') as infile:\n",
        "            for line in infile:\n",
        "                 # Don't add the \"END\" line from the ligand, we'll add one at the very end\n",
        "                 if not line.startswith(\"END\"):\n",
        "                    outfile.write(line)\n",
        "        print(f\"Added {ligand_file} to {merged_pdb_file}\")\n",
        "\n",
        "    outfile.write(\"END\\n\")\n",
        "\n",
        "print(f\"✅ Merged file created: {merged_pdb_file}\")\n",
        "\n",
        "\n",
        "# --- 3. Load Structure and Prepare Atoms ---\n",
        "print(\"\\n--- Loading Structure with Biopython ---\")\n",
        "try:\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"complex\", merged_pdb_file)\n",
        "    model = structure[0] # Assume first model\n",
        "except Exception as e:\n",
        "    print(f\"🔥 Error parsing merged PDB file: {e}\")\n",
        "    print(\"   Please check the merged file for errors.\")\n",
        "    raise SystemExit\n",
        "\n",
        "ligand_chains = [c.strip() for c in ligand_chains_str.split(',')]\n",
        "protein_chains = []\n",
        "for chain in model:\n",
        "    if chain.id not in ligand_chains:\n",
        "        protein_chains.append(chain.id)\n",
        "\n",
        "print(f\"Protein chains identified: {protein_chains}\")\n",
        "print(f\"Ligand chains identified: {ligand_chains}\")\n",
        "\n",
        "# Define backbone atom names\n",
        "backbone_atoms = {'N', 'CA', 'C', 'O'}\n",
        "\n",
        "ligand_atoms = []\n",
        "protein_bb_atoms = []\n",
        "protein_sc_atoms = []\n",
        "\n",
        "print(\"Separating protein (backbone/sidechain) and ligand atoms...\")\n",
        "for chain in model:\n",
        "    if chain.id in ligand_chains:\n",
        "        for atom in chain.get_atoms():\n",
        "            ligand_atoms.append(atom)\n",
        "\n",
        "    elif chain.id in protein_chains:\n",
        "        for residue in chain.get_residues():\n",
        "            # Only process standard protein residues\n",
        "            if residue.id[0] == ' ': # ' ' indicates a standard residue\n",
        "                for atom in residue.get_atoms():\n",
        "                    if atom.name in backbone_atoms:\n",
        "                        protein_bb_atoms.append(atom)\n",
        "                    else:\n",
        "                        protein_sc_atoms.append(atom)\n",
        "\n",
        "print(f\"Found {len(ligand_atoms)} ligand atoms.\")\n",
        "print(f\"Found {len(protein_bb_atoms)} protein backbone atoms.\")\n",
        "print(f\"Found {len(protein_sc_atoms)} protein sidechain atoms.\")\n",
        "\n",
        "if not ligand_atoms:\n",
        "    print(\"🔥 Error: No ligand atoms found. Cannot proceed.\")\n",
        "    raise SystemExit\n",
        "\n",
        "# --- 4. Perform Neighbor Search ---\n",
        "print(\"Finding nearby residues...\")\n",
        "\n",
        "# Create the NeighborSearch object from all ligand atoms\n",
        "ns = NeighborSearch(ligand_atoms)\n",
        "\n",
        "# Find all protein backbone atoms within the backbone cutoff\n",
        "nearby_bb_atoms = set()\n",
        "for atom in protein_bb_atoms:\n",
        "    nearby = ns.search(atom.coord, cutoff_backbone, 'A') # 'A' = atom level\n",
        "    if nearby:\n",
        "        nearby_bb_atoms.add(atom)\n",
        "\n",
        "# Find all protein sidechain atoms within the sidechain cutoff\n",
        "nearby_sc_atoms = set()\n",
        "for atom in protein_sc_atoms:\n",
        "    nearby = ns.search(atom.coord, cutoff_sidechain, 'A') # 'A' = atom level\n",
        "    if nearby:\n",
        "        nearby_sc_atoms.add(atom)\n",
        "\n",
        "print(f\"Found {len(nearby_bb_atoms)} backbone atoms within {cutoff_backbone} Å.\")\n",
        "print(f\"Found {len(nearby_sc_atoms)} sidechain atoms within {cutoff_sidechain} Å.\")\n",
        "\n",
        "# --- 5. Map Atoms to Residues and Finalize List ---\n",
        "active_site_residues = set()\n",
        "\n",
        "# Get parent residues for backbone atoms\n",
        "for atom in nearby_bb_atoms:\n",
        "    active_site_residues.add(atom.get_parent())\n",
        "\n",
        "# Get parent residues for sidechain atoms\n",
        "for atom in nearby_sc_atoms:\n",
        "    active_site_residues.add(atom.get_parent())\n",
        "\n",
        "print(\"\\n--- Active Site Analysis Complete ---\")\n",
        "if not active_site_residues:\n",
        "    print(\"⚠️ No active site residues found with the given cutoffs.\")\n",
        "else:\n",
        "    print(f\"✅ Found {len(active_site_residues)} unique residues in the active site.\")\n",
        "\n",
        "    # Sort residues by chain and residue number\n",
        "    sorted_residues = sorted(list(active_site_residues),\n",
        "                             key=lambda res: (res.get_parent().id, res.id[1]))\n",
        "\n",
        "    active_site_pdb_names = []\n",
        "\n",
        "    # CRITICAL: Create the global variable for the next script\n",
        "    # This must be a list of 1-indexed residue numbers\n",
        "    global active_site_residue_numbers\n",
        "    active_site_residue_numbers = []\n",
        "\n",
        "    print(\"Active Site Residues (Chain + PDB Number):\")\n",
        "    for res in sorted_residues:\n",
        "        chain_id = res.get_parent().id\n",
        "        res_num = res.id[1]\n",
        "        res_name = res.get_resname()\n",
        "\n",
        "        active_site_pdb_names.append(f\"{chain_id}{res_num} ({res_name})\")\n",
        "        active_site_residue_numbers.append(res_num)\n",
        "\n",
        "    print(\", \".join(active_site_pdb_names))\n",
        "\n",
        "    print(\"\\nCorresponding Residue Numbers (for next script):\")\n",
        "    print(active_site_residue_numbers)\n",
        "\n",
        "print(\"\\n🎯 Script finished successfully.\")"
      ],
      "metadata": {
        "id": "7Q3yqLnD0o_M",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d6db8f-723a-444f-fb1b-8d7ce2884251"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Merging PDB Files ---\n",
            "Added protein.pdb to protein_ligand_complex.pdb\n",
            "Added NAP.pdb to protein_ligand_complex.pdb\n",
            "✅ Merged file created: protein_ligand_complex.pdb\n",
            "\n",
            "--- Loading Structure with Biopython ---\n",
            "Protein chains identified: ['A']\n",
            "Ligand chains identified: ['B']\n",
            "Separating protein (backbone/sidechain) and ligand atoms...\n",
            "Found 48 ligand atoms.\n",
            "Found 752 protein backbone atoms.\n",
            "Found 722 protein sidechain atoms.\n",
            "Finding nearby residues...\n",
            "Found 80 backbone atoms within 7.0 Å.\n",
            "Found 49 sidechain atoms within 6.0 Å.\n",
            "\n",
            "--- Active Site Analysis Complete ---\n",
            "✅ Found 27 unique residues in the active site.\n",
            "Active Site Residues (Chain + PDB Number):\n",
            "A8 (GLY), A9 (SER), A10 (ALA), A11 (GLN), A12 (VAL), A13 (ASN), A14 (SER), A15 (HIS), A16 (THR), A17 (SER), A18 (ALA), A80 (THR), A81 (PRO), A82 (ASN), A83 (TYR), A84 (HIS), A85 (GLY), A86 (SER), A112 (GLY), A113 (ASN), A114 (SER), A115 (GLY), A116 (GLY), A117 (ILE), A145 (HIS), A146 (ASP), A157 (TYR)\n",
            "\n",
            "Corresponding Residue Numbers (for next script):\n",
            "[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 80, 81, 82, 83, 84, 85, 86, 112, 113, 114, 115, 116, 117, 145, 146, 157]\n",
            "\n",
            "🎯 Script finished successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Generate MSA (.a3m) and optionally filter based on different parameters\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Specify Input\n",
        "#@markdown Enter the name of the FASTA file you generated.\n",
        "input_fasta_file = 'protein.fasta' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 2. HHfilter Options\n",
        "#@markdown Check the box to run hhfilter on the generated .a3m file.\n",
        "run_hhfilter = True #@param {type:\"boolean\"}\n",
        "id_redundancy = 90 #@param {type:\"integer\"}\n",
        "coverage = 50 #@param {type:\"integer\"}\n",
        "query_identity = 30 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. Check if the input FASTA file exists\n",
        "if not os.path.isfile(input_fasta_file):\n",
        "    print(f\"🔥 Error: Input file not found: {input_fasta_file}\")\n",
        "    print(\"Please make sure the filename matches the one from the previous step.\")\n",
        "    sys.exit(f\"File not found: {input_fasta_file}\")\n",
        "else:\n",
        "    print(f\"Found input file: {input_fasta_file}\")\n",
        "\n",
        "# 2. Define the output directory as the current folder\n",
        "output_dir = \".\" # This will save files to /content/\n",
        "\n",
        "# 3. Run the colabfold_batch command\n",
        "print(f\"Running colabfold_batch on {input_fasta_file}...\")\n",
        "print(f\"This will generate the .a3m file and then stop.\")\n",
        "\n",
        "# Run the alignment generation\n",
        "!colabfold_batch {input_fasta_file} {output_dir} --msa-mode \"mmseqs2_uniref_env\" --msa-only\n",
        "\n",
        "# 4. Check the output files from colabfold_batch\n",
        "print(\"\\nAlignment generation complete.\")\n",
        "\n",
        "# --- 5. Run HHfilter (New Step) ---\n",
        "base_name = os.path.splitext(input_fasta_file)[0]\n",
        "original_a3m = f\"{base_name}.a3m\"\n",
        "filtered_a3m = f\"{base_name}.filtered.a3m\"\n",
        "\n",
        "if run_hhfilter:\n",
        "    print(f\"\\nRunning hhfilter on {original_a3m}...\")\n",
        "    if not os.path.isfile(original_a3m):\n",
        "        print(f\"🔥 Error: The original alignment file {original_a3m} was not found. Skipping filter.\")\n",
        "    else:\n",
        "        # Build and run the hhfilter command\n",
        "        !hhfilter -i {original_a3m} -o {filtered_a3m} -id {id_redundancy} -cov {coverage} -qid {query_identity}\n",
        "        print(f\"Filtering complete. Filtered file saved as: {filtered_a3m}\")\n",
        "else:\n",
        "    print(\"\\nSkipping hhfilter step.\")\n",
        "\n",
        "# --- 6. Find and report the final .a3m file ---\n",
        "print(\"\\n--- Final Output ---\")\n",
        "try:\n",
        "    if run_hhfilter and os.path.isfile(filtered_a3m):\n",
        "        print(f\"✅ Your FINAL filtered alignment file is ready: {filtered_a3m}\")\n",
        "    elif os.path.isfile(original_a3m):\n",
        "        print(f\"✅ Your original (unfiltered) alignment file is ready: {original_a3m}\")\n",
        "    else:\n",
        "        print(f\"⚠️ Error: No .a3m file ({original_a3m} or {filtered_a3m}) was found.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n⚠️ Error finding .a3m file: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SFSdQBD4Aw3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517a9da7-37dc-421a-f6d2-1585a5977a9f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found input file: protein.fasta\n",
            "Running colabfold_batch on protein.fasta...\n",
            "This will generate the .a3m file and then stop.\n",
            "2025-11-07 01:20:51,514 Running colabfold 1.5.5 (31518c387dcb56c38cfa4f8e3c338a79ab3ff684)\n",
            "\n",
            "WARNING: You are welcome to use the default MSA server, however keep in mind that it's a\n",
            "limited shared resource only capable of processing a few thousand MSAs per day. Please\n",
            "submit jobs only from a single IP address. We reserve the right to limit access to the\n",
            "server case-by-case when usage exceeds fair use. If you require more MSAs: You can \n",
            "precompute all MSAs with `colabfold_search` or host your own API and pass it to `--host-url`\n",
            "\n",
            "2025-11-07 01:21:02,479 Running on GPU\n",
            "2025-11-07 01:21:02,840 Found 4 citations for tools or databases\n",
            "2025-11-07 01:21:02,840 Query 1/1: protein (length 188)\n",
            "COMPLETE: 100% 150/150 [00:01<00:00, 132.82it/s]\n",
            "2025-11-07 01:21:03,990 Saved protein.pickle\n",
            "2025-11-07 01:21:04,831 Done\n",
            "\n",
            "Alignment generation complete.\n",
            "\n",
            "Running hhfilter on protein.a3m...\n",
            "- 01:21:06.916 INFO: Input file = protein.a3m\n",
            "\n",
            "- 01:21:06.916 INFO: Output file = protein.filtered.a3m\n",
            "\n",
            "Filtering complete. Filtered file saved as: protein.filtered.a3m\n",
            "\n",
            "--- Final Output ---\n",
            "✅ Your FINAL filtered alignment file is ready: protein.filtered.a3m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4) Find Conserved Residues based on the MSA\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Specify Input Files\n",
        "#@markdown Enter the name of the filtered .a3m file (from previous step).\n",
        "filtered_a3m_file = 'protein.filtered.a3m' #@param {type:\"string\"}\n",
        "#@markdown\n",
        "#@markdown ### 2. Conservation Settings\n",
        "#@markdown Fraction of most-conserved residues to select (e.g., 0.5 = 50%).\n",
        "frac_conserved = 0.5 #@param {type:\"number\"}\n",
        "#@markdown\n",
        "#@markdown ### 3. Output File\n",
        "#@markdown Name for the final .txt file containing the fixed positions.\n",
        "output_txt_file = \"fixed_positions.txt\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Self-contained A3M Parser (Replaces tools.py) ---\n",
        "def robust_parse_a3m(a3m_file_path):\n",
        "    \"\"\"\n",
        "    Parses an .a3m file, removes insertions (lowercase), and converts to numbers.\n",
        "    This version correctly skips header lines before the first sequence.\n",
        "    \"\"\"\n",
        "    # Mapping from AA to number (0-19 are AA, 20 is gap)\n",
        "    aa_to_num = {\n",
        "        'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9,\n",
        "        'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19,\n",
        "        '-': 20, 'X': 20, 'B': 20, 'Z': 20 # Treat unknowns as gaps\n",
        "    }\n",
        "\n",
        "    msa_sequences = []\n",
        "    seq = \"\"\n",
        "    found_first_header = False # Flag to skip junk lines at the start\n",
        "\n",
        "    try:\n",
        "        with open(a3m_file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line.startswith('>'):\n",
        "                    found_first_header = True # We've found the first sequence, start parsing\n",
        "                    if seq: # save previous sequence\n",
        "                        msa_sequences.append(seq)\n",
        "                    seq = \"\" # start new sequence\n",
        "                elif found_first_header and line: # Only append if we're parsing and line is not empty\n",
        "                    seq += line\n",
        "    except FileNotFoundError:\n",
        "        print(f\"🔥 Error: The alignment file '{a3m_file_path}' was not found.\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error reading {a3m_file_path}: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "    if seq: # save last sequence\n",
        "        msa_sequences.append(seq)\n",
        "\n",
        "    if not msa_sequences:\n",
        "        raise ValueError(f\"No sequences found in {a3m_file_path}. File might be malformed.\")\n",
        "\n",
        "    # Get query length from the first sequence\n",
        "    query_seq = msa_sequences[0]\n",
        "    query_L = len([c for c in query_seq if c.isupper() or c == '-'])\n",
        "\n",
        "    if query_L == 0:\n",
        "        raise ValueError(f\"Query sequence found, but it has no match/delete characters (Length is 0).\")\n",
        "\n",
        "    msa_aligned = []\n",
        "    for seq in msa_sequences:\n",
        "        aligned_seq = \"\"\n",
        "        for char in seq:\n",
        "            # Keep only uppercase (match) and gaps (delete)\n",
        "            if char.isupper() or char == '-':\n",
        "                aligned_seq += char\n",
        "\n",
        "        # Ensure all sequences have the same length as the query\n",
        "        if len(aligned_seq) == query_L:\n",
        "            msa_aligned.append(aligned_seq)\n",
        "\n",
        "    if not msa_aligned:\n",
        "         raise ValueError(f\"No valid, aligned sequences found in {a3m_file_path}.\")\n",
        "\n",
        "    # Convert to numbers\n",
        "    msa_numeric = []\n",
        "    for seq in msa_aligned:\n",
        "        num_seq = [aa_to_num.get(char.upper(), 20) for char in seq] # .upper() for safety\n",
        "        msa_numeric.append(num_seq)\n",
        "\n",
        "    return {\n",
        "        'msa': np.array(msa_aligned),\n",
        "        'msa_num': np.array(msa_numeric, dtype=int)\n",
        "    }\n",
        "# --- End of Parser ---\n",
        "\n",
        "\n",
        "# --- 2. Check for 'active_site_residue_numbers' variable ---\n",
        "# This variable should be created by the define_active_site_biopython.py script\n",
        "if 'active_site_residue_numbers' not in locals() and 'active_site_residue_numbers' not in globals():\n",
        "    print(\"🔥 Error: The 'active_site_residue_numbers' list was not found.\")\n",
        "    print(\"   Please re-run the 'Define Active Site (Biopython)' script first.\")\n",
        "    sys.exit(1)\n",
        "else:\n",
        "    # Ensure it's a numpy array for union1d\n",
        "    active_site_np = np.array(active_site_residue_numbers)\n",
        "    print(f\"Found active_site_residue_numbers list with {len(active_site_np)} residues.\")\n",
        "\n",
        "# --- 3. Run Conservation Analysis ---\n",
        "try:\n",
        "    if not os.path.isfile(filtered_a3m_file):\n",
        "        print(f\"🔥 Error: The alignment file '{filtered_a3m_file}' was not found.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Parsing alignment file: {filtered_a3m_file}...\")\n",
        "    # Use our new robust parser\n",
        "    aln = robust_parse_a3m(filtered_a3m_file)\n",
        "\n",
        "    msa_num = aln['msa_num']\n",
        "    L = msa_num.shape[1] # Get length (L) from the numeric array\n",
        "    num_seqs = msa_num.shape[0]\n",
        "\n",
        "    if num_seqs == 0 or L == 0:\n",
        "        print(f\"🔥 Error: The alignment file '{filtered_a3m_file}' contains no valid sequences or has length 0.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Alignment loaded. Length: {L}, Number of sequences: {num_seqs}\")\n",
        "\n",
        "    # Calculate conservation\n",
        "    counts = np.stack([np.bincount(column, minlength=21) for column in msa_num.T]).T\n",
        "    max_count = np.max(counts, axis=0)\n",
        "\n",
        "    freq = counts / num_seqs\n",
        "\n",
        "    # Handle columns that are 100% gaps (to avoid divide-by-zero)\n",
        "    freq_sum = freq[:20].sum(axis=0)\n",
        "    freq_sum[freq_sum == 0] = 1.0 # Set sum to 1.0 to prevent error\n",
        "\n",
        "    freq_norm = freq[:20] / freq_sum\n",
        "    max_freq_norm = np.max(freq_norm, axis=0)\n",
        "\n",
        "    # Only apply low-count penalty if we have more than one sequence\n",
        "    if num_seqs > 1:\n",
        "        max_freq_norm[max_count < 10] = 0 # Don't choose positions with low counts\n",
        "    else:\n",
        "        print(\"Only 1 sequence found, skipping low-count filter.\")\n",
        "\n",
        "    # Save the conserved residues as a list\n",
        "    num_conserved = int(L * frac_conserved)\n",
        "    conserved_residues = np.argsort(max_freq_norm)[::-1][:num_conserved] + 1 # make 1-indexed\n",
        "    conserved_residues.sort()\n",
        "\n",
        "    print(f\"Found {len(conserved_residues)} conserved residues (top {frac_conserved*100}%).\")\n",
        "\n",
        "    # --- 4. Intersect with Active Site ---\n",
        "    # Use the active_site_np array we defined earlier\n",
        "    fixed_positions = np.union1d(conserved_residues, active_site_np)\n",
        "    fixed_positions.sort()\n",
        "\n",
        "    print(\"\\n--- Final Results ---\")\n",
        "    print(f\"Active Site Residues ({len(active_site_np)}):\")\n",
        "    print(active_site_np)\n",
        "    print(f\"Conserved Residues ({len(conserved_residues)}):\")\n",
        "    print(conserved_residues)\n",
        "    print(f\"All Fixed Positions ({len(fixed_positions)}):\")\n",
        "    print(fixed_positions)\n",
        "\n",
        "    # --- 5. Save Final List to TXT File ---\n",
        "    print(f\"\\nSaving final fixed positions to {output_txt_file}...\")\n",
        "    fixed_positions_str = [str(int(res)) for res in fixed_positions]\n",
        "\n",
        "    with open(output_txt_file, 'w') as f:\n",
        "        f.write(' '.join(fixed_positions_str))\n",
        "\n",
        "    print(f\"✅ Successfully saved fixed positions to {output_txt_file}.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"🔥 An error occurred: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VvF_lX9PZwBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4230fb75-8657-4c46-e626-30a27614d5ab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found active_site_residue_numbers list with 27 residues.\n",
            "Parsing alignment file: protein.filtered.a3m...\n",
            "Alignment loaded. Length: 188, Number of sequences: 1856\n",
            "Found 94 conserved residues (top 50.0%).\n",
            "\n",
            "--- Final Results ---\n",
            "Active Site Residues (27):\n",
            "[  8   9  10  11  12  13  14  15  16  17  18  80  81  82  83  84  85  86\n",
            " 112 113 114 115 116 117 145 146 157]\n",
            "Conserved Residues (94):\n",
            "[  1   2   3   6   8   9  10  11  14  16  19  20  23  31  32  33  35  36\n",
            "  38  39  41  43  44  45  48  63  66  70  73  74  75  77  78  80  81  82\n",
            "  83  84  85  86  88  89  90  91  92  93  94  95  96  98 103 105 106 107\n",
            " 108 109 110 115 116 122 123 125 126 127 129 131 133 134 135 136 137 139\n",
            " 141 143 144 147 150 151 154 155 159 161 162 164 167 175 176 179 182 183\n",
            " 185 186 187 188]\n",
            "All Fixed Positions (106):\n",
            "[  1   2   3   6   8   9  10  11  12  13  14  15  16  17  18  19  20  23\n",
            "  31  32  33  35  36  38  39  41  43  44  45  48  63  66  70  73  74  75\n",
            "  77  78  80  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96\n",
            "  98 103 105 106 107 108 109 110 112 113 114 115 116 117 122 123 125 126\n",
            " 127 129 131 133 134 135 136 137 139 141 143 144 145 146 147 150 151 154\n",
            " 155 157 159 161 162 164 167 175 176 179 182 183 185 186 187 188]\n",
            "\n",
            "Saving final fixed positions to fixed_positions.txt...\n",
            "✅ Successfully saved fixed positions to fixed_positions.txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. Run ProteinMPNN with evolutionary information"
      ],
      "metadata": {
        "id": "wekZ4-aIEVtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#@markdown ## 1) Setting up ProteinMPNN with fixed positions\n",
        "#@markdown ### 1. Input PDB\n",
        "#@markdown Path to the PDB file (must be in your Colab folder).\n",
        "pdb_path = \"protein.pdb\" #@param {type:\"string\"}\n",
        "\n",
        "homomer = False #@param {type:\"boolean\"}\n",
        "designed_chain = \"A\" #@param {type:\"string\"}\n",
        "fixed_chain = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if designed_chain == \"\":\n",
        "  designed_chain_list = []\n",
        "else:\n",
        "  designed_chain_list = re.sub(\"[^A-Za-z]+\",\",\", designed_chain).split(\",\")\n",
        "\n",
        "if fixed_chain == \"\":\n",
        "  fixed_chain_list = []\n",
        "else:\n",
        "  fixed_chain_list = re.sub(\"[^A-Za-z]+\",\",\", fixed_chain).split(\",\")\n",
        "\n",
        "chain_list = list(set(designed_chain_list + fixed_chain_list))\n",
        "\n",
        "#@markdown - `designed_chain`: Chain(s) to design (e.g., \"A\").\n",
        "#@markdown - `fixed_chain`: Chain(s) to keep fixed (e.g., \"C\").\n",
        "\n",
        "#@markdown ### 2. Design Options\n",
        "#@markdown Number of sequences to generate.\n",
        "num_seqs = 1 #@param {type:\"integer\"}\n",
        "num_seq_per_target = num_seqs\n",
        "\n",
        "#@markdown - Sampling temperature for amino acids, T=0.0 means taking argmax, T>>1.0 means sample randomly.\n",
        "sampling_temp = \"0.2\" #@param [\"0.0001\", \"0.1\", \"0.15\", \"0.2\", \"0.25\", \"0.3\", \"0.5\"]\n",
        "\n",
        "#@markdown - `omit_AAs`: Specify amino acids to omit (e.g., \"XC\" to omit Cys and Unknown).\n",
        "omit_AAs = \"XC\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### 3. Fixed Positions (Optional)\n",
        "#@markdown Provide the .txt file of conserved/active site residues to fix.\n",
        "fixed_positions_file = \"fixed_positions.txt\" #@param {type:\"string\"}\n",
        "#@markdown The chain these fixed positions apply to.\n",
        "fixed_positions_chain = \"A\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "# --- (Rest of your script's parameters) ---\n",
        "save_score=0\n",
        "save_probs=0\n",
        "score_only=0\n",
        "conditional_probs_only=0\n",
        "conditional_probs_only_backbone=0\n",
        "batch_size=1\n",
        "max_length=20000\n",
        "out_folder='.'\n",
        "jsonl_path=''\n",
        "# omit_AAs='X'  <--- THIS LINE IS NOW DELETED (replaced by the form variable)\n",
        "pssm_multi=0.0\n",
        "pssm_threshold=0.0\n",
        "pssm_log_odds_flag=0\n",
        "pssm_bias_flag=0\n",
        "\n",
        "##############################################################\n",
        "\n",
        "folder_for_outputs = out_folder\n",
        "\n",
        "NUM_BATCHES = num_seq_per_target//batch_size\n",
        "BATCH_COPIES = batch_size\n",
        "temperatures = [float(item) for item in sampling_temp.split()]\n",
        "omit_AAs_list = omit_AAs\n",
        "alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
        "\n",
        "omit_AAs_np = np.array([AA in omit_AAs_list for AA in alphabet]).astype(np.float32)\n",
        "\n",
        "chain_id_dict = None\n",
        "fixed_positions_dict = None\n",
        "pssm_dict = None\n",
        "omit_AA_dict = None\n",
        "bias_AA_dict = None\n",
        "tied_positions_dict = None\n",
        "bias_by_res_dict = None\n",
        "bias_AAs_np = np.zeros(len(alphabet))\n",
        "\n",
        "# --- New code to read fixed_positions.txt ---\n",
        "if fixed_positions_file and fixed_positions_chain:\n",
        "    try:\n",
        "        with open(fixed_positions_file, 'r') as f:\n",
        "            # Read the space-separated list of numbers\n",
        "            residues_to_fix = [int(res) for res in f.read().split()]\n",
        "\n",
        "        if residues_to_fix:\n",
        "            # Get the PDB name (basename without .pdb)\n",
        "            pdb_name = os.path.basename(pdb_path).replace('.pdb', '')\n",
        "\n",
        "            # Build the dictionary in the format MPNN expects\n",
        "            fixed_positions_dict = {\n",
        "                pdb_name: {\n",
        "                    fixed_positions_chain: residues_to_fix\n",
        "                }\n",
        "            }\n",
        "            print(f\"✅ Successfully read {len(residues_to_fix)} fixed positions for chain {fixed_positions_chain} from {fixed_positions_file}.\")\n",
        "            print(f\"Fixed positions: {residues_to_fix}\")\n",
        "        else:\n",
        "            print(f\"⚠️ {fixed_positions_file} was found but is empty. No specific residues fixed.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"⚠️ {fixed_positions_file} not found. No specific residues fixed (besides 'fixed_chain').\")\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error reading {fixed_positions_file}: {e}\")\n",
        "else:\n",
        "    print(\"No fixed positions file provided. Only 'fixed_chain' (if any) will be fixed.\")\n",
        "# --- End new code ---\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# Check if PDB file exists before proceeding\n",
        "if not os.path.isfile(pdb_path):\n",
        "    print(f\"🔥 Error: PDB file not found at {pdb_path}\")\n",
        "    print(\"Please make sure the file is in your Colab folder.\")\n",
        "else:\n",
        "    pdb_dict_list = parse_PDB(pdb_path, input_chain_list=chain_list)\n",
        "    dataset_valid = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=max_length)\n",
        "\n",
        "    chain_id_dict = {}\n",
        "    chain_id_dict[pdb_dict_list[0]['name']]= (designed_chain_list, fixed_chain_list)\n",
        "\n",
        "    print(f\"\\nChain assignments: {chain_id_dict}\")\n",
        "    for chain in chain_list:\n",
        "      l = len(pdb_dict_list[0][f\"seq_chain_{chain}\"])\n",
        "      print(f\"Length of chain {chain} is {l}\")\n",
        "\n",
        "    if homomer:\n",
        "      tied_positions_dict = make_tied_positions_for_homomers(pdb_dict_list)\n",
        "    else:\n",
        "      tied_positions_dict = None"
      ],
      "metadata": {
        "cellView": "form",
        "id": "agHE5Szc-yFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05808112-6c30-40da-8405-de64c656b8db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully read 106 fixed positions for chain A from fixed_positions.txt.\n",
            "Fixed positions: [1, 2, 3, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 23, 31, 32, 33, 35, 36, 38, 39, 41, 43, 44, 45, 48, 63, 66, 70, 73, 74, 75, 77, 78, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 103, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 122, 123, 125, 126, 127, 129, 131, 133, 134, 135, 136, 137, 139, 141, 143, 144, 145, 146, 147, 150, 151, 154, 155, 157, 159, 161, 162, 164, 167, 175, 176, 179, 182, 183, 185, 186, 187, 188]\n",
            "\n",
            "Chain assignments: {'protein': (['A'], [])}\n",
            "Length of chain A is 188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Run ProteinMPNN with fixed positions\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Output FASTA File\n",
        "output_fasta_file = \"generated_sequences.fasta\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 2. A3M Generation\n",
        "source_a3m_file = \"protein.filtered.a3m\"          #@param {type:\"string\"}\n",
        "output_msa_folder = \"msa\"                        #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os, copy, torch, numpy as np\n",
        "\n",
        "# --- 1. Read base alignment ---\n",
        "os.makedirs(output_msa_folder, exist_ok=True)\n",
        "alignment_body_lines = []\n",
        "try:\n",
        "    with open(source_a3m_file, 'r') as f:\n",
        "        found_first_seq = False\n",
        "        for line in f:\n",
        "            line = line.rstrip('\\n\\r') + '\\n'  # normalize newlines\n",
        "            if line.startswith('>'):\n",
        "                if not found_first_seq:\n",
        "                    found_first_seq = True\n",
        "                    continue  # skip query header\n",
        "                alignment_body_lines.append(line)\n",
        "            elif found_first_seq:\n",
        "                alignment_body_lines.append(line)\n",
        "    print(f\"Read {len(alignment_body_lines)//2} alignment hits from {source_a3m_file}.\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ WARNING reading {source_a3m_file}: {e}\")\n",
        "\n",
        "def _alignment_tail_to_write(alignment_lines):\n",
        "    \"\"\"Return alignment hits excluding the query sequence.\"\"\"\n",
        "    if not alignment_lines:\n",
        "        return []\n",
        "    if not alignment_lines[0].startswith('>'):\n",
        "        return alignment_lines[1:]\n",
        "    return alignment_lines\n",
        "\n",
        "# --- 2. Generate sequences ---\n",
        "with torch.no_grad():\n",
        "    print('Generating sequences...')\n",
        "    print(f\"Saving generated sequences to: {output_fasta_file}\")\n",
        "    with open(output_fasta_file, 'w') as fasta_f:\n",
        "        for ix, protein in enumerate(dataset_valid):\n",
        "            score_list, all_probs_list, all_log_probs_list, S_sample_list = [], [], [], []\n",
        "            batch_clones = [copy.deepcopy(protein) for _ in range(BATCH_COPIES)]\n",
        "            X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta = tied_featurize(\n",
        "                batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict,\n",
        "                pssm_dict, bias_by_res_dict)\n",
        "            pssm_log_odds_mask = (pssm_log_odds_all > pssm_threshold).float()\n",
        "            name_ = batch_clones[0]['name']\n",
        "\n",
        "            randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
        "            log_probs = model(X, S, mask, chain_M * chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
        "            mask_for_loss = mask * chain_M * chain_M_pos\n",
        "            native_score = _scores(S, log_probs, mask_for_loss).cpu().data.numpy()\n",
        "\n",
        "            for temp in temperatures:\n",
        "                for j in range(NUM_BATCHES):\n",
        "                    randn_2 = torch.randn(chain_M.shape, device=X.device)\n",
        "                    if tied_positions_dict is None:\n",
        "                        sample_dict = model.sample(\n",
        "                            X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp,\n",
        "                            omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos,\n",
        "                            omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias,\n",
        "                            pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag),\n",
        "                            pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag),\n",
        "                            bias_by_res=bias_by_res_all)\n",
        "                        S_sample = sample_dict[\"S\"]\n",
        "                    else:\n",
        "                        sample_dict = model.tied_sample(\n",
        "                            X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp,\n",
        "                            omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos,\n",
        "                            omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias,\n",
        "                            pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag),\n",
        "                            pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag),\n",
        "                            tied_pos=tied_pos_list_of_lists_list[0], tied_beta=tied_beta,\n",
        "                            bias_by_res=bias_by_res_all)\n",
        "                        S_sample = sample_dict[\"S\"]\n",
        "\n",
        "                    log_probs = model(X, S_sample, mask, chain_M * chain_M_pos,\n",
        "                                      residue_idx, chain_encoding_all, randn_2,\n",
        "                                      use_input_decoding_order=True,\n",
        "                                      decoding_order=sample_dict[\"decoding_order\"])\n",
        "                    mask_for_loss = mask * chain_M * chain_M_pos\n",
        "                    scores = _scores(S_sample, log_probs, mask_for_loss).cpu().data.numpy()\n",
        "\n",
        "                    for b_ix in range(BATCH_COPIES):\n",
        "                        masked_chain_length_list = masked_chain_length_list_list[b_ix]\n",
        "                        masked_list = masked_list_list[b_ix]\n",
        "                        seq_recovery_rate = torch.sum(\n",
        "                            torch.sum(torch.nn.functional.one_hot(S[b_ix],21)\n",
        "                                      * torch.nn.functional.one_hot(S_sample[b_ix],21), axis=-1)\n",
        "                            * mask_for_loss[b_ix]) / torch.sum(mask_for_loss[b_ix])\n",
        "                        seq = _S_to_seq(S_sample[b_ix], chain_M[b_ix])\n",
        "                        score = scores[b_ix]\n",
        "                        native_seq = _S_to_seq(S[b_ix], chain_M[b_ix])\n",
        "\n",
        "                        # --- Native written once ---\n",
        "                        if b_ix == 0 and j == 0 and temp == temperatures[0]:\n",
        "                            native_seq = \"\".join(native_seq.split(\"/\")).strip()\n",
        "                            native_score_print = np.format_float_positional(np.float32(native_score.mean()), unique=False, precision=4)\n",
        "                            line = f\">native, score={native_score_print}\\n{native_seq}\\n\"\n",
        "                            fasta_f.write(line)\n",
        "                            print(line.rstrip())\n",
        "\n",
        "                            a3m_filename = os.path.join(output_msa_folder, f\"{name_}_native.a3m\")\n",
        "                            try:\n",
        "                                with open(a3m_filename, 'w') as a3m_f:\n",
        "                                    native_len = len(native_seq)\n",
        "                                    a3m_f.write(f\"#{native_len}\\t1\\n>native\\n{native_seq}\\n\")\n",
        "                                    tail = _alignment_tail_to_write(alignment_body_lines)\n",
        "                                    a3m_f.writelines(tail)\n",
        "                                print(f\"Wrote native A3M: {a3m_filename}\")\n",
        "                            except Exception as e:\n",
        "                                print(f\"⚠️ Warning: Could not write native A3M {a3m_filename}. Error: {e}\")\n",
        "\n",
        "                        # --- Generated sequences ---\n",
        "                        seq = \"\".join(seq.split(\"/\")).strip()\n",
        "                        score_print = np.format_float_positional(np.float32(score), unique=False, precision=4)\n",
        "                        seq_rec_print = np.format_float_positional(np.float32(seq_recovery_rate.detach().cpu().numpy()), unique=False, precision=4)\n",
        "                        sample_index = j * BATCH_COPIES + b_ix + 1\n",
        "                        line = f\">sample{sample_index}, T={temp}, score={score_print}, seq_recovery={seq_rec_print}\\n{seq}\\n\"\n",
        "                        fasta_f.write(line)\n",
        "                        print(line.rstrip())\n",
        "\n",
        "                        sample_name = f\"sample{sample_index}\"\n",
        "                        a3m_filename = os.path.join(output_msa_folder, f\"{name_}_{sample_name}.a3m\")\n",
        "                        try:\n",
        "                            with open(a3m_filename, 'w') as a3m_f:\n",
        "                                seq_len = len(seq)\n",
        "                                a3m_f.write(f\"#{seq_len}\\t1\\n>{sample_name}\\n{seq}\\n\")\n",
        "                                tail = _alignment_tail_to_write(alignment_body_lines)\n",
        "                                a3m_f.writelines(tail)\n",
        "                            print(f\"Wrote sample A3M: {a3m_filename}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"⚠️ Warning: Could not write sample A3M {a3m_filename}. Error: {e}\")\n",
        "\n",
        "    print(f\"\\n✅ All sequences saved to {output_fasta_file} and A3Ms in '{output_msa_folder}'.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w1qviwvLAyim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84bb7d84-e638-47a2-ccec-9d6a174a89cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 1855 alignment hits from protein.filtered.a3m.\n",
            "Generating sequences...\n",
            "Saving generated sequences to: generated_sequences.fasta\n",
            ">native, score=1.8388\n",
            "MKGLIIIGSAQVNSHTSALARYLTEHFKTHDIEAEIFDLAEKPLNQLDFSGTTPSIDEIKQNMKDLKEKAMAADFLILGTPNYHGSYSGILKNALDHLNMDYFKMKPVGLIGNSGGIVSSEPLSHLRVIVRSLLGIAVPTQIATHDSDFAKNEDGSYYLNDSEFQLRARLFVDQIVSFVNNSPYEHLK\n",
            "Wrote native A3M: msa/protein_native.a3m\n",
            ">sample1, T=0.2, score=0.9757, seq_recovery=0.4878\n",
            "MKGLIIVGSAQVNSHTSALARYLSEIFKEHDIEAEIFDLAEHPLNPPDSKGTQPKIPEKEKNMKLLKEKAKEADFIILATPNYHGSYSGILKNALDHLDKSHFKMKPVGLIGNSGGIRSSKPLEHLRKIVRELLGIAIPTQIATHDSDFAKNSDGTYYLSDSSFKKRAREYVDEIVDAVKNSPYEHLK\n",
            "Wrote sample A3M: msa/protein_sample1.a3m\n",
            "\n",
            "✅ All sequences saved to generated_sequences.fasta and A3Ms in 'msa'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Visualize the generated sequences using an MSA Viewer in Google Colab\n",
        "#The following code is modified from the wonderful viewer developed by Damien Farrell\n",
        "#https://dmnfarrell.github.io/bioinformatics/bokeh-sequence-aligner\n",
        "\n",
        "#Importing all modules first\n",
        "import os, io, random\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from Bio.Seq import Seq\n",
        "from Bio.Align import MultipleSeqAlignment\n",
        "from Bio import AlignIO, SeqIO\n",
        "\n",
        "import panel as pn\n",
        "import panel.widgets as pnw\n",
        "pn.extension()\n",
        "\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.models import ColumnDataSource, Plot, Grid, Range1d\n",
        "from bokeh.models.glyphs import Text, Rect\n",
        "from bokeh.layouts import gridplot\n",
        "\n",
        "#Setting up the amino color code according to Zappo color scheme\n",
        "def get_colors(seqs):\n",
        "    #make colors for bases in sequence\n",
        "    text = [i for s in list(seqs) for i in s]\n",
        "    #Use Zappo color scheme\n",
        "    clrs =  {'K':'red',\n",
        "             'R':'red',\n",
        "             'H':'red',\n",
        "             'D':'green',\n",
        "             'E':'green',\n",
        "             'Q':'blue',\n",
        "             'N':'blue',\n",
        "             'S':'blue',\n",
        "             'T':'blue',\n",
        "             'A':'blue',\n",
        "             'I':'blue',\n",
        "             'L':'blue',\n",
        "             'M':'blue',\n",
        "             'V':'blue',\n",
        "             'F':'orange',\n",
        "             'Y':'orange',\n",
        "             'W':'orange',\n",
        "             'C':'blue',\n",
        "             'P':'yellow',\n",
        "             'G':'orange',\n",
        "             '-':'white'}\n",
        "    colors = [clrs[i] for i in text]\n",
        "    return colors\n",
        "\n",
        "#Setting up the MSA viewer\n",
        "def view_alignment(aln, fontsize=\"9pt\", plot_width=800):\n",
        "    \"\"\"Bokeh sequence alignment view\"\"\"\n",
        "\n",
        "    #make sequence and id lists from the aln object\n",
        "    seqs = [rec.seq for rec in (aln)]\n",
        "    ids = [rec.id for rec in aln]\n",
        "    text = [i for s in list(seqs) for i in s]\n",
        "    colors = get_colors(seqs)\n",
        "    N = len(seqs[0])\n",
        "    S = len(seqs)\n",
        "    width = .4\n",
        "\n",
        "    x = np.arange(1,N+1)\n",
        "    y = np.arange(0,S,1)\n",
        "    #creates a 2D grid of coords from the 1D arrays\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    #flattens the arrays\n",
        "    gx = xx.ravel()\n",
        "    gy = yy.flatten()\n",
        "    #use recty for rect coords with an offset\n",
        "    recty = gy+.5\n",
        "    h= 1/S\n",
        "    #now we can create the ColumnDataSource with all the arrays\n",
        "    source = ColumnDataSource(dict(x=gx, y=gy, recty=recty, text=text, colors=colors))\n",
        "    plot_height = len(seqs)*15+50\n",
        "    x_range = Range1d(0,N+1, bounds='auto')\n",
        "    if N>100:\n",
        "        viewlen=100\n",
        "    else:\n",
        "        viewlen=N\n",
        "    #view_range is for the close up view\n",
        "    view_range = (0,viewlen)\n",
        "    tools=\"xpan, xwheel_zoom, reset, save\"\n",
        "\n",
        "    #entire sequence view (no text, with zoom)\n",
        "    p = figure(title=None, width= plot_width, height=50,\n",
        "               x_range=x_range, y_range=(0,S), tools=tools,\n",
        "               min_border=0, toolbar_location='below')\n",
        "    rects = Rect(x=\"x\", y=\"recty\",  width=1, height=1, fill_color=\"colors\",\n",
        "                 line_color=None, fill_alpha=0.6)\n",
        "    p.add_glyph(source, rects)\n",
        "    p.yaxis.visible = False\n",
        "    p.grid.visible = False\n",
        "\n",
        "    #sequence text view with ability to scroll along x axis\n",
        "    p1 = figure(title=None, width=plot_width, height=plot_height,\n",
        "                x_range=view_range, y_range=ids, tools=\"xpan,reset\",\n",
        "                min_border=0, toolbar_location='below')#, lod_factor=1)\n",
        "    glyph = Text(x=\"x\", y=\"y\", text=\"text\", text_align='center',text_color=\"black\",\n",
        "                text_font=\"monospace\",text_font_size=fontsize)\n",
        "    rects = Rect(x=\"x\", y=\"recty\",  width=1, height=1, fill_color=\"colors\",\n",
        "                line_color=None, fill_alpha=0.4)\n",
        "    p1.add_glyph(source, glyph)\n",
        "    p1.add_glyph(source, rects)\n",
        "\n",
        "    p1.grid.visible = False\n",
        "    p1.xaxis.major_label_text_font_style = \"bold\"\n",
        "    p1.yaxis.minor_tick_line_width = 0\n",
        "    p1.yaxis.major_tick_line_width = 0\n",
        "\n",
        "    p = gridplot([[p],[p1]], toolbar_location='below')\n",
        "    return p\n",
        "\n",
        "#Loading the viewer by indicating the MSA file and format to read\n",
        "#@markdown Name of the MSA file (including the filetype)\n",
        "MSAfile = 'generated_sequences.fasta' #@param {type:\"string\"}\n",
        "MSAformat = 'fasta' #@param {type:\"string\"}\n",
        "aln = AlignIO.read(MSAfile,MSAformat)\n",
        "p = view_alignment(aln, plot_width=900)\n",
        "pn.pane.Bokeh(p)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cmDekMMXBjpT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "a83be7cd-c08a-4c0b-af09-fdaf7552af74"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
              "*[data-root-id] > * {\n",
              "  box-sizing: border-box;\n",
              "  font-family: var(--jp-ui-font-family);\n",
              "  font-size: var(--jp-ui-font-size1);\n",
              "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
              "}\n",
              "\n",
              "/* Override VSCode background color */\n",
              ".cell-output-ipywidget-background:has(\n",
              "  > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
              "),\n",
              ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
              "  background-color: transparent !important;\n",
              "}\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  const force = true;\n",
              "  const py_version = '3.7.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n",
              "  const reloading = false;\n",
              "  const Bokeh = root.Bokeh;\n",
              "\n",
              "  // Set a timeout for this load but only if we are not already initializing\n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
              "        if (callback != null)\n",
              "          callback();\n",
              "      });\n",
              "    } finally {\n",
              "      delete root._bokeh_onload_callbacks;\n",
              "    }\n",
              "    console.debug(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n",
              "    if (css_urls == null) css_urls = [];\n",
              "    if (js_urls == null) js_urls = [];\n",
              "    if (js_modules == null) js_modules = [];\n",
              "    if (js_exports == null) js_exports = {};\n",
              "\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      // Don't load bokeh if it is still initializing\n",
              "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n",
              "      // There is nothing to load\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "\n",
              "    function on_load() {\n",
              "      root._bokeh_is_loading--;\n",
              "      if (root._bokeh_is_loading === 0) {\n",
              "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
              "        run_callbacks()\n",
              "      }\n",
              "    }\n",
              "    window._bokeh_on_load = on_load\n",
              "\n",
              "    function on_error(e) {\n",
              "      const src_el = e.srcElement\n",
              "      console.error(\"failed to load \" + (src_el.href || src_el.src));\n",
              "    }\n",
              "\n",
              "    const skip = [];\n",
              "    if (window.requirejs) {\n",
              "      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n",
              "      root._bokeh_is_loading = css_urls.length + 0;\n",
              "    } else {\n",
              "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n",
              "    }\n",
              "\n",
              "    const existing_stylesheets = []\n",
              "    const links = document.getElementsByTagName('link')\n",
              "    for (let i = 0; i < links.length; i++) {\n",
              "      const link = links[i]\n",
              "      if (link.href != null) {\n",
              "        existing_stylesheets.push(link.href)\n",
              "      }\n",
              "    }\n",
              "    for (let i = 0; i < css_urls.length; i++) {\n",
              "      const url = css_urls[i];\n",
              "      const escaped = encodeURI(url)\n",
              "      if (existing_stylesheets.indexOf(escaped) !== -1) {\n",
              "        on_load()\n",
              "        continue;\n",
              "      }\n",
              "      const element = document.createElement(\"link\");\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.rel = \"stylesheet\";\n",
              "      element.type = \"text/css\";\n",
              "      element.href = url;\n",
              "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
              "      document.body.appendChild(element);\n",
              "    }    var existing_scripts = []\n",
              "    const scripts = document.getElementsByTagName('script')\n",
              "    for (let i = 0; i < scripts.length; i++) {\n",
              "      var script = scripts[i]\n",
              "      if (script.src != null) {\n",
              "        existing_scripts.push(script.src)\n",
              "      }\n",
              "    }\n",
              "    for (let i = 0; i < js_urls.length; i++) {\n",
              "      const url = js_urls[i];\n",
              "      const escaped = encodeURI(url)\n",
              "      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n",
              "        if (!window.requirejs) {\n",
              "          on_load();\n",
              "        }\n",
              "        continue;\n",
              "      }\n",
              "      const element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    for (let i = 0; i < js_modules.length; i++) {\n",
              "      const url = js_modules[i];\n",
              "      const escaped = encodeURI(url)\n",
              "      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n",
              "        if (!window.requirejs) {\n",
              "          on_load();\n",
              "        }\n",
              "        continue;\n",
              "      }\n",
              "      var element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      element.type = \"module\";\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    for (const name in js_exports) {\n",
              "      const url = js_exports[name];\n",
              "      const escaped = encodeURI(url)\n",
              "      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n",
              "        if (!window.requirejs) {\n",
              "          on_load();\n",
              "        }\n",
              "        continue;\n",
              "      }\n",
              "      var element = document.createElement('script');\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.type = \"module\";\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      element.textContent = `\n",
              "      import ${name} from \"${url}\"\n",
              "      window.${name} = ${name}\n",
              "      window._bokeh_on_load()\n",
              "      `\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    if (!js_urls.length && !js_modules.length) {\n",
              "      on_load()\n",
              "    }\n",
              "  };\n",
              "\n",
              "  function inject_raw_css(css) {\n",
              "    const element = document.createElement(\"style\");\n",
              "    element.appendChild(document.createTextNode(css));\n",
              "    document.body.appendChild(element);\n",
              "  }\n",
              "\n",
              "  const js_urls = [\"https://cdn.holoviz.org/panel/1.8.2/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.holoviz.org/panel/1.8.2/dist/panel.min.js\"];\n",
              "  const js_modules = [];\n",
              "  const js_exports = {};\n",
              "  const css_urls = [];\n",
              "  const inline_js = [    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "function(Bokeh) {} // ensure no trailing comma for IE\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
              "      for (let i = 0; i < inline_js.length; i++) {\n",
              "        try {\n",
              "          inline_js[i].call(root, root.Bokeh);\n",
              "        } catch(e) {\n",
              "          if (!reloading) {\n",
              "            throw e;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      // Cache old bokeh versions\n",
              "      if (Bokeh != undefined && !reloading) {\n",
              "        var NewBokeh = root.Bokeh;\n",
              "        if (Bokeh.versions === undefined) {\n",
              "          Bokeh.versions = new Map();\n",
              "        }\n",
              "        if (NewBokeh.version !== Bokeh.version) {\n",
              "          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n",
              "        }\n",
              "        root.Bokeh = Bokeh;\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    }\n",
              "    root._bokeh_is_initializing = false\n",
              "  }\n",
              "\n",
              "  function load_or_wait() {\n",
              "    // Implement a backoff loop that tries to ensure we do not load multiple\n",
              "    // versions of Bokeh and its dependencies at the same time.\n",
              "    // In recent versions we use the root._bokeh_is_initializing flag\n",
              "    // to determine whether there is an ongoing attempt to initialize\n",
              "    // bokeh, however for backward compatibility we also try to ensure\n",
              "    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n",
              "    // before older versions are fully initialized.\n",
              "    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n",
              "      // If the timeout and bokeh was not successfully loaded we reset\n",
              "      // everything and try loading again\n",
              "      root._bokeh_timeout = Date.now() + 5000;\n",
              "      root._bokeh_is_initializing = false;\n",
              "      root._bokeh_onload_callbacks = undefined;\n",
              "      root._bokeh_is_loading = 0\n",
              "      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n",
              "      load_or_wait();\n",
              "    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n",
              "      setTimeout(load_or_wait, 100);\n",
              "    } else {\n",
              "      root._bokeh_is_initializing = true\n",
              "      root._bokeh_onload_callbacks = []\n",
              "      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n",
              "      if (!reloading && !bokeh_loaded) {\n",
              "        if (root.Bokeh) {\n",
              "          root.Bokeh = undefined;\n",
              "        }\n",
              "        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "      }\n",
              "      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n",
              "        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "        run_inline_js();\n",
              "      });\n",
              "    }\n",
              "  }\n",
              "  // Give older versions of the autoload script a head-start to ensure\n",
              "  // they initialize before we start loading newer version.\n",
              "  setTimeout(load_or_wait, 100)\n",
              "}(window));"
            ],
            "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.7.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.8.2/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.holoviz.org/panel/1.8.2/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        let retries = 0;\n        const open = () => {\n          if (comm.active) {\n            comm.open();\n          } else if (retries > 3) {\n            console.warn('Comm target never activated')\n          } else {\n            retries += 1\n            setTimeout(open, 500)\n          }\n        }\n        if (comm.active) {\n          comm.open();\n        } else {\n          setTimeout(open, 500)\n        }\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
            "application/javascript": [
              "\n",
              "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
              "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
              "}\n",
              "\n",
              "\n",
              "    function JupyterCommManager() {\n",
              "    }\n",
              "\n",
              "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
              "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
              "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
              "        comm_manager.register_target(comm_id, function(comm) {\n",
              "          comm.on_msg(msg_handler);\n",
              "        });\n",
              "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
              "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
              "          comm.onMsg = msg_handler;\n",
              "        });\n",
              "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
              "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
              "          var messages = comm.messages[Symbol.asyncIterator]();\n",
              "          function processIteratorResult(result) {\n",
              "            var message = result.value;\n",
              "            var content = {data: message.data, comm_id};\n",
              "            var buffers = []\n",
              "            for (var buffer of message.buffers || []) {\n",
              "              buffers.push(new DataView(buffer))\n",
              "            }\n",
              "            var metadata = message.metadata || {};\n",
              "            var msg = {content, buffers, metadata}\n",
              "            msg_handler(msg);\n",
              "            return messages.next().then(processIteratorResult);\n",
              "          }\n",
              "          return messages.next().then(processIteratorResult);\n",
              "        })\n",
              "      }\n",
              "    }\n",
              "\n",
              "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
              "      if (comm_id in window.PyViz.comms) {\n",
              "        return window.PyViz.comms[comm_id];\n",
              "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
              "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
              "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
              "        if (msg_handler) {\n",
              "          comm.on_msg(msg_handler);\n",
              "        }\n",
              "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
              "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
              "        let retries = 0;\n",
              "        const open = () => {\n",
              "          if (comm.active) {\n",
              "            comm.open();\n",
              "          } else if (retries > 3) {\n",
              "            console.warn('Comm target never activated')\n",
              "          } else {\n",
              "            retries += 1\n",
              "            setTimeout(open, 500)\n",
              "          }\n",
              "        }\n",
              "        if (comm.active) {\n",
              "          comm.open();\n",
              "        } else {\n",
              "          setTimeout(open, 500)\n",
              "        }\n",
              "        if (msg_handler) {\n",
              "          comm.onMsg = msg_handler;\n",
              "        }\n",
              "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
              "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
              "        comm_promise.then((comm) => {\n",
              "          window.PyViz.comms[comm_id] = comm;\n",
              "          if (msg_handler) {\n",
              "            var messages = comm.messages[Symbol.asyncIterator]();\n",
              "            function processIteratorResult(result) {\n",
              "              var message = result.value;\n",
              "              var content = {data: message.data};\n",
              "              var metadata = message.metadata || {comm_id};\n",
              "              var msg = {content, metadata}\n",
              "              msg_handler(msg);\n",
              "              return messages.next().then(processIteratorResult);\n",
              "            }\n",
              "            return messages.next().then(processIteratorResult);\n",
              "          }\n",
              "        })\n",
              "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
              "          return comm_promise.then((comm) => {\n",
              "            comm.send(data, metadata, buffers, disposeOnDone);\n",
              "          });\n",
              "        };\n",
              "        var comm = {\n",
              "          send: sendClosure\n",
              "        };\n",
              "      }\n",
              "      window.PyViz.comms[comm_id] = comm;\n",
              "      return comm;\n",
              "    }\n",
              "    window.PyViz.comm_manager = new JupyterCommManager();\n",
              "    \n",
              "\n",
              "\n",
              "var JS_MIME_TYPE = 'application/javascript';\n",
              "var HTML_MIME_TYPE = 'text/html';\n",
              "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
              "var CLASS_NAME = 'output';\n",
              "\n",
              "/**\n",
              " * Render data to the DOM node\n",
              " */\n",
              "function render(props, node) {\n",
              "  var div = document.createElement(\"div\");\n",
              "  var script = document.createElement(\"script\");\n",
              "  node.appendChild(div);\n",
              "  node.appendChild(script);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle when a new output is added\n",
              " */\n",
              "function handle_add_output(event, handle) {\n",
              "  var output_area = handle.output_area;\n",
              "  var output = handle.output;\n",
              "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
              "    return\n",
              "  }\n",
              "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "  if (id !== undefined) {\n",
              "    var nchildren = toinsert.length;\n",
              "    var html_node = toinsert[nchildren-1].children[0];\n",
              "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "    var scripts = [];\n",
              "    var nodelist = html_node.querySelectorAll(\"script\");\n",
              "    for (var i in nodelist) {\n",
              "      if (nodelist.hasOwnProperty(i)) {\n",
              "        scripts.push(nodelist[i])\n",
              "      }\n",
              "    }\n",
              "\n",
              "    scripts.forEach( function (oldScript) {\n",
              "      var newScript = document.createElement(\"script\");\n",
              "      var attrs = [];\n",
              "      var nodemap = oldScript.attributes;\n",
              "      for (var j in nodemap) {\n",
              "        if (nodemap.hasOwnProperty(j)) {\n",
              "          attrs.push(nodemap[j])\n",
              "        }\n",
              "      }\n",
              "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
              "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
              "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
              "    });\n",
              "    if (JS_MIME_TYPE in output.data) {\n",
              "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
              "    }\n",
              "    output_area._hv_plot_id = id;\n",
              "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
              "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
              "    } else {\n",
              "      window.PyViz.plot_index[id] = null;\n",
              "    }\n",
              "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "    var bk_div = document.createElement(\"div\");\n",
              "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "    var script_attrs = bk_div.children[0].attributes;\n",
              "    for (var i = 0; i < script_attrs.length; i++) {\n",
              "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "    }\n",
              "    // store reference to server id on output_area\n",
              "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "  }\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle when an output is cleared or removed\n",
              " */\n",
              "function handle_clear_output(event, handle) {\n",
              "  var id = handle.cell.output_area._hv_plot_id;\n",
              "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
              "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
              "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
              "  if (server_id !== null) {\n",
              "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
              "    return;\n",
              "  } else if (comm !== null) {\n",
              "    comm.send({event_type: 'delete', 'id': id});\n",
              "  }\n",
              "  delete PyViz.plot_index[id];\n",
              "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
              "    var doc = window.Bokeh.index[id].model.document\n",
              "    doc.clear();\n",
              "    const i = window.Bokeh.documents.indexOf(doc);\n",
              "    if (i > -1) {\n",
              "      window.Bokeh.documents.splice(i, 1);\n",
              "    }\n",
              "  }\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle kernel restart event\n",
              " */\n",
              "function handle_kernel_cleanup(event, handle) {\n",
              "  delete PyViz.comms[\"hv-extension-comm\"];\n",
              "  window.PyViz.plot_index = {}\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle update_display_data messages\n",
              " */\n",
              "function handle_update_output(event, handle) {\n",
              "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
              "  handle_add_output(event, handle)\n",
              "}\n",
              "\n",
              "function register_renderer(events, OutputArea) {\n",
              "  function append_mime(data, metadata, element) {\n",
              "    // create a DOM node to render to\n",
              "    var toinsert = this.create_output_subarea(\n",
              "    metadata,\n",
              "    CLASS_NAME,\n",
              "    EXEC_MIME_TYPE\n",
              "    );\n",
              "    this.keyboard_manager.register_events(toinsert);\n",
              "    // Render to node\n",
              "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "    render(props, toinsert[0]);\n",
              "    element.append(toinsert);\n",
              "    return toinsert\n",
              "  }\n",
              "\n",
              "  events.on('output_added.OutputArea', handle_add_output);\n",
              "  events.on('output_updated.OutputArea', handle_update_output);\n",
              "  events.on('clear_output.CodeCell', handle_clear_output);\n",
              "  events.on('delete.Cell', handle_clear_output);\n",
              "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
              "\n",
              "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "    safe: true,\n",
              "    index: 0\n",
              "  });\n",
              "}\n",
              "\n",
              "if (window.Jupyter !== undefined) {\n",
              "  try {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  } catch(err) {\n",
              "  }\n",
              "}\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div id='93ed4c2d-b582-4e81-9670-9e99bcc84b1b'>\n",
              "  <div id=\"f3746d3c-5f6b-4e7b-b32a-a725374335eb\" data-root-id=\"93ed4c2d-b582-4e81-9670-9e99bcc84b1b\" style=\"display: contents;\"></div>\n",
              "</div>\n",
              "<script type=\"application/javascript\">(function(root) {\n",
              "  var docs_json = {\"9cc5832e-e507-4519-b4cc-2e516fad02f2\":{\"version\":\"3.7.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"93ed4c2d-b582-4e81-9670-9e99bcc84b1b\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"01683edc-bf09-4d4a-996a-9208441dc2d1\",\"attributes\":{\"plot_id\":\"93ed4c2d-b582-4e81-9670-9e99bcc84b1b\",\"comm_id\":\"659d56d642b6401f849b889db05b5810\",\"client_comm_id\":\"61c5d02aa03c4542b7041383ec747049\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"start\",\"kind\":\"Any\",\"default\":0},{\"name\":\"end\",\"kind\":\"Any\",\"default\":100},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"max_notifications\",\"kind\":\"Any\",\"default\":5},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
              "  var render_items = [{\"docid\":\"9cc5832e-e507-4519-b4cc-2e516fad02f2\",\"roots\":{\"93ed4c2d-b582-4e81-9670-9e99bcc84b1b\":\"f3746d3c-5f6b-4e7b-b32a-a725374335eb\"},\"root_ids\":[\"93ed4c2d-b582-4e81-9670-9e99bcc84b1b\"]}];\n",
              "  var docs = Object.values(docs_json)\n",
              "  if (!docs) {\n",
              "    return\n",
              "  }\n",
              "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
              "  async function embed_document(root) {\n",
              "    var Bokeh = get_bokeh(root)\n",
              "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "    for (const render_item of render_items) {\n",
              "      for (const root_id of render_item.root_ids) {\n",
              "\tconst id_el = document.getElementById(root_id)\n",
              "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
              "\t  const root_el = id_el.children[0]\n",
              "\t  root_el.id = root_el.id + '-rendered'\n",
              "\t  for (const child of root_el.children) {\n",
              "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
              "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
              "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
              "\t  }\n",
              "\t}\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  function get_bokeh(root) {\n",
              "    if (root.Bokeh === undefined) {\n",
              "      return null\n",
              "    } else if (root.Bokeh.version !== py_version) {\n",
              "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
              "\treturn null\n",
              "      }\n",
              "      return root.Bokeh.versions.get(py_version);\n",
              "    } else if (root.Bokeh.version === py_version) {\n",
              "      return root.Bokeh\n",
              "    }\n",
              "    return null\n",
              "  }\n",
              "  function is_loaded(root) {\n",
              "    var Bokeh = get_bokeh(root)\n",
              "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
              "  }\n",
              "  if (is_loaded(root)) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (is_loaded(root)) {\n",
              "        clearInterval(timer);\n",
              "        embed_document(root);\n",
              "      } else if (document.readyState == \"complete\") {\n",
              "        attempts++;\n",
              "        if (attempts > 200) {\n",
              "          clearInterval(timer);\n",
              "\t  var Bokeh = get_bokeh(root)\n",
              "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
              "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
              "\t  } else {\n",
              "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
              "\t    embed_document(root)\n",
              "\t  }\n",
              "        }\n",
              "      }\n",
              "    }, 25, root)\n",
              "  }\n",
              "})(window);</script>"
            ],
            "application/vnd.holoviews_exec.v0+json": ""
          },
          "metadata": {
            "application/vnd.holoviews_exec.v0+json": {
              "id": "93ed4c2d-b582-4e81-9670-9e99bcc84b1b"
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
              "*[data-root-id] > * {\n",
              "  box-sizing: border-box;\n",
              "  font-family: var(--jp-ui-font-family);\n",
              "  font-size: var(--jp-ui-font-size1);\n",
              "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
              "}\n",
              "\n",
              "/* Override VSCode background color */\n",
              ".cell-output-ipywidget-background:has(\n",
              "  > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
              "),\n",
              ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
              "  background-color: transparent !important;\n",
              "}\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  const force = true;\n",
              "  const py_version = '3.7.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n",
              "  const reloading = false;\n",
              "  const Bokeh = root.Bokeh;\n",
              "\n",
              "  // Set a timeout for this load but only if we are not already initializing\n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
              "        if (callback != null)\n",
              "          callback();\n",
              "      });\n",
              "    } finally {\n",
              "      delete root._bokeh_onload_callbacks;\n",
              "    }\n",
              "    console.debug(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n",
              "    if (css_urls == null) css_urls = [];\n",
              "    if (js_urls == null) js_urls = [];\n",
              "    if (js_modules == null) js_modules = [];\n",
              "    if (js_exports == null) js_exports = {};\n",
              "\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      // Don't load bokeh if it is still initializing\n",
              "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n",
              "      // There is nothing to load\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "\n",
              "    function on_load() {\n",
              "      root._bokeh_is_loading--;\n",
              "      if (root._bokeh_is_loading === 0) {\n",
              "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
              "        run_callbacks()\n",
              "      }\n",
              "    }\n",
              "    window._bokeh_on_load = on_load\n",
              "\n",
              "    function on_error(e) {\n",
              "      const src_el = e.srcElement\n",
              "      console.error(\"failed to load \" + (src_el.href || src_el.src));\n",
              "    }\n",
              "\n",
              "    const skip = [];\n",
              "    if (window.requirejs) {\n",
              "      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n",
              "      root._bokeh_is_loading = css_urls.length + 0;\n",
              "    } else {\n",
              "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n",
              "    }\n",
              "\n",
              "    const existing_stylesheets = []\n",
              "    const links = document.getElementsByTagName('link')\n",
              "    for (let i = 0; i < links.length; i++) {\n",
              "      const link = links[i]\n",
              "      if (link.href != null) {\n",
              "        existing_stylesheets.push(link.href)\n",
              "      }\n",
              "    }\n",
              "    for (let i = 0; i < css_urls.length; i++) {\n",
              "      const url = css_urls[i];\n",
              "      const escaped = encodeURI(url)\n",
              "      if (existing_stylesheets.indexOf(escaped) !== -1) {\n",
              "        on_load()\n",
              "        continue;\n",
              "      }\n",
              "      const element = document.createElement(\"link\");\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.rel = \"stylesheet\";\n",
              "      element.type = \"text/css\";\n",
              "      element.href = url;\n",
              "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
              "      document.body.appendChild(element);\n",
              "    }    var existing_scripts = []\n",
              "    const scripts = document.getElementsByTagName('script')\n",
              "    for (let i = 0; i < scripts.length; i++) {\n",
              "      var script = scripts[i]\n",
              "      if (script.src != null) {\n",
              "        existing_scripts.push(script.src)\n",
              "      }\n",
              "    }\n",
              "    for (let i = 0; i < js_urls.length; i++) {\n",
              "      const url = js_urls[i];\n",
              "      const escaped = encodeURI(url)\n",
              "      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n",
              "        if (!window.requirejs) {\n",
              "          on_load();\n",
              "        }\n",
              "        continue;\n",
              "      }\n",
              "      const element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    for (let i = 0; i < js_modules.length; i++) {\n",
              "      const url = js_modules[i];\n",
              "      const escaped = encodeURI(url)\n",
              "      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n",
              "        if (!window.requirejs) {\n",
              "          on_load();\n",
              "        }\n",
              "        continue;\n",
              "      }\n",
              "      var element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      element.type = \"module\";\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    for (const name in js_exports) {\n",
              "      const url = js_exports[name];\n",
              "      const escaped = encodeURI(url)\n",
              "      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n",
              "        if (!window.requirejs) {\n",
              "          on_load();\n",
              "        }\n",
              "        continue;\n",
              "      }\n",
              "      var element = document.createElement('script');\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.type = \"module\";\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      element.textContent = `\n",
              "      import ${name} from \"${url}\"\n",
              "      window.${name} = ${name}\n",
              "      window._bokeh_on_load()\n",
              "      `\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    if (!js_urls.length && !js_modules.length) {\n",
              "      on_load()\n",
              "    }\n",
              "  };\n",
              "\n",
              "  function inject_raw_css(css) {\n",
              "    const element = document.createElement(\"style\");\n",
              "    element.appendChild(document.createTextNode(css));\n",
              "    document.body.appendChild(element);\n",
              "  }\n",
              "\n",
              "  const js_urls = [\"https://cdn.holoviz.org/panel/1.8.2/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.holoviz.org/panel/1.8.2/dist/panel.min.js\"];\n",
              "  const js_modules = [];\n",
              "  const js_exports = {};\n",
              "  const css_urls = [];\n",
              "  const inline_js = [    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "function(Bokeh) {} // ensure no trailing comma for IE\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
              "      for (let i = 0; i < inline_js.length; i++) {\n",
              "        try {\n",
              "          inline_js[i].call(root, root.Bokeh);\n",
              "        } catch(e) {\n",
              "          if (!reloading) {\n",
              "            throw e;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      // Cache old bokeh versions\n",
              "      if (Bokeh != undefined && !reloading) {\n",
              "        var NewBokeh = root.Bokeh;\n",
              "        if (Bokeh.versions === undefined) {\n",
              "          Bokeh.versions = new Map();\n",
              "        }\n",
              "        if (NewBokeh.version !== Bokeh.version) {\n",
              "          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n",
              "        }\n",
              "        root.Bokeh = Bokeh;\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    }\n",
              "    root._bokeh_is_initializing = false\n",
              "  }\n",
              "\n",
              "  function load_or_wait() {\n",
              "    // Implement a backoff loop that tries to ensure we do not load multiple\n",
              "    // versions of Bokeh and its dependencies at the same time.\n",
              "    // In recent versions we use the root._bokeh_is_initializing flag\n",
              "    // to determine whether there is an ongoing attempt to initialize\n",
              "    // bokeh, however for backward compatibility we also try to ensure\n",
              "    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n",
              "    // before older versions are fully initialized.\n",
              "    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n",
              "      // If the timeout and bokeh was not successfully loaded we reset\n",
              "      // everything and try loading again\n",
              "      root._bokeh_timeout = Date.now() + 5000;\n",
              "      root._bokeh_is_initializing = false;\n",
              "      root._bokeh_onload_callbacks = undefined;\n",
              "      root._bokeh_is_loading = 0\n",
              "      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n",
              "      load_or_wait();\n",
              "    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n",
              "      setTimeout(load_or_wait, 100);\n",
              "    } else {\n",
              "      root._bokeh_is_initializing = true\n",
              "      root._bokeh_onload_callbacks = []\n",
              "      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n",
              "      if (!reloading && !bokeh_loaded) {\n",
              "        if (root.Bokeh) {\n",
              "          root.Bokeh = undefined;\n",
              "        }\n",
              "        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "      }\n",
              "      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n",
              "        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "        run_inline_js();\n",
              "      });\n",
              "    }\n",
              "  }\n",
              "  // Give older versions of the autoload script a head-start to ensure\n",
              "  // they initialize before we start loading newer version.\n",
              "  setTimeout(load_or_wait, 100)\n",
              "}(window));"
            ],
            "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.7.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.8.2/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.holoviz.org/panel/1.8.2/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        let retries = 0;\n        const open = () => {\n          if (comm.active) {\n            comm.open();\n          } else if (retries > 3) {\n            console.warn('Comm target never activated')\n          } else {\n            retries += 1\n            setTimeout(open, 500)\n          }\n        }\n        if (comm.active) {\n          comm.open();\n        } else {\n          setTimeout(open, 500)\n        }\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
            "application/javascript": [
              "\n",
              "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
              "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
              "}\n",
              "\n",
              "\n",
              "    function JupyterCommManager() {\n",
              "    }\n",
              "\n",
              "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
              "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
              "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
              "        comm_manager.register_target(comm_id, function(comm) {\n",
              "          comm.on_msg(msg_handler);\n",
              "        });\n",
              "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
              "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
              "          comm.onMsg = msg_handler;\n",
              "        });\n",
              "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
              "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
              "          var messages = comm.messages[Symbol.asyncIterator]();\n",
              "          function processIteratorResult(result) {\n",
              "            var message = result.value;\n",
              "            var content = {data: message.data, comm_id};\n",
              "            var buffers = []\n",
              "            for (var buffer of message.buffers || []) {\n",
              "              buffers.push(new DataView(buffer))\n",
              "            }\n",
              "            var metadata = message.metadata || {};\n",
              "            var msg = {content, buffers, metadata}\n",
              "            msg_handler(msg);\n",
              "            return messages.next().then(processIteratorResult);\n",
              "          }\n",
              "          return messages.next().then(processIteratorResult);\n",
              "        })\n",
              "      }\n",
              "    }\n",
              "\n",
              "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
              "      if (comm_id in window.PyViz.comms) {\n",
              "        return window.PyViz.comms[comm_id];\n",
              "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
              "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
              "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
              "        if (msg_handler) {\n",
              "          comm.on_msg(msg_handler);\n",
              "        }\n",
              "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
              "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
              "        let retries = 0;\n",
              "        const open = () => {\n",
              "          if (comm.active) {\n",
              "            comm.open();\n",
              "          } else if (retries > 3) {\n",
              "            console.warn('Comm target never activated')\n",
              "          } else {\n",
              "            retries += 1\n",
              "            setTimeout(open, 500)\n",
              "          }\n",
              "        }\n",
              "        if (comm.active) {\n",
              "          comm.open();\n",
              "        } else {\n",
              "          setTimeout(open, 500)\n",
              "        }\n",
              "        if (msg_handler) {\n",
              "          comm.onMsg = msg_handler;\n",
              "        }\n",
              "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
              "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
              "        comm_promise.then((comm) => {\n",
              "          window.PyViz.comms[comm_id] = comm;\n",
              "          if (msg_handler) {\n",
              "            var messages = comm.messages[Symbol.asyncIterator]();\n",
              "            function processIteratorResult(result) {\n",
              "              var message = result.value;\n",
              "              var content = {data: message.data};\n",
              "              var metadata = message.metadata || {comm_id};\n",
              "              var msg = {content, metadata}\n",
              "              msg_handler(msg);\n",
              "              return messages.next().then(processIteratorResult);\n",
              "            }\n",
              "            return messages.next().then(processIteratorResult);\n",
              "          }\n",
              "        })\n",
              "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
              "          return comm_promise.then((comm) => {\n",
              "            comm.send(data, metadata, buffers, disposeOnDone);\n",
              "          });\n",
              "        };\n",
              "        var comm = {\n",
              "          send: sendClosure\n",
              "        };\n",
              "      }\n",
              "      window.PyViz.comms[comm_id] = comm;\n",
              "      return comm;\n",
              "    }\n",
              "    window.PyViz.comm_manager = new JupyterCommManager();\n",
              "    \n",
              "\n",
              "\n",
              "var JS_MIME_TYPE = 'application/javascript';\n",
              "var HTML_MIME_TYPE = 'text/html';\n",
              "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
              "var CLASS_NAME = 'output';\n",
              "\n",
              "/**\n",
              " * Render data to the DOM node\n",
              " */\n",
              "function render(props, node) {\n",
              "  var div = document.createElement(\"div\");\n",
              "  var script = document.createElement(\"script\");\n",
              "  node.appendChild(div);\n",
              "  node.appendChild(script);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle when a new output is added\n",
              " */\n",
              "function handle_add_output(event, handle) {\n",
              "  var output_area = handle.output_area;\n",
              "  var output = handle.output;\n",
              "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
              "    return\n",
              "  }\n",
              "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "  if (id !== undefined) {\n",
              "    var nchildren = toinsert.length;\n",
              "    var html_node = toinsert[nchildren-1].children[0];\n",
              "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "    var scripts = [];\n",
              "    var nodelist = html_node.querySelectorAll(\"script\");\n",
              "    for (var i in nodelist) {\n",
              "      if (nodelist.hasOwnProperty(i)) {\n",
              "        scripts.push(nodelist[i])\n",
              "      }\n",
              "    }\n",
              "\n",
              "    scripts.forEach( function (oldScript) {\n",
              "      var newScript = document.createElement(\"script\");\n",
              "      var attrs = [];\n",
              "      var nodemap = oldScript.attributes;\n",
              "      for (var j in nodemap) {\n",
              "        if (nodemap.hasOwnProperty(j)) {\n",
              "          attrs.push(nodemap[j])\n",
              "        }\n",
              "      }\n",
              "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
              "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
              "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
              "    });\n",
              "    if (JS_MIME_TYPE in output.data) {\n",
              "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
              "    }\n",
              "    output_area._hv_plot_id = id;\n",
              "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
              "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
              "    } else {\n",
              "      window.PyViz.plot_index[id] = null;\n",
              "    }\n",
              "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "    var bk_div = document.createElement(\"div\");\n",
              "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "    var script_attrs = bk_div.children[0].attributes;\n",
              "    for (var i = 0; i < script_attrs.length; i++) {\n",
              "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "    }\n",
              "    // store reference to server id on output_area\n",
              "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "  }\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle when an output is cleared or removed\n",
              " */\n",
              "function handle_clear_output(event, handle) {\n",
              "  var id = handle.cell.output_area._hv_plot_id;\n",
              "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
              "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
              "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
              "  if (server_id !== null) {\n",
              "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
              "    return;\n",
              "  } else if (comm !== null) {\n",
              "    comm.send({event_type: 'delete', 'id': id});\n",
              "  }\n",
              "  delete PyViz.plot_index[id];\n",
              "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
              "    var doc = window.Bokeh.index[id].model.document\n",
              "    doc.clear();\n",
              "    const i = window.Bokeh.documents.indexOf(doc);\n",
              "    if (i > -1) {\n",
              "      window.Bokeh.documents.splice(i, 1);\n",
              "    }\n",
              "  }\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle kernel restart event\n",
              " */\n",
              "function handle_kernel_cleanup(event, handle) {\n",
              "  delete PyViz.comms[\"hv-extension-comm\"];\n",
              "  window.PyViz.plot_index = {}\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle update_display_data messages\n",
              " */\n",
              "function handle_update_output(event, handle) {\n",
              "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
              "  handle_add_output(event, handle)\n",
              "}\n",
              "\n",
              "function register_renderer(events, OutputArea) {\n",
              "  function append_mime(data, metadata, element) {\n",
              "    // create a DOM node to render to\n",
              "    var toinsert = this.create_output_subarea(\n",
              "    metadata,\n",
              "    CLASS_NAME,\n",
              "    EXEC_MIME_TYPE\n",
              "    );\n",
              "    this.keyboard_manager.register_events(toinsert);\n",
              "    // Render to node\n",
              "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "    render(props, toinsert[0]);\n",
              "    element.append(toinsert);\n",
              "    return toinsert\n",
              "  }\n",
              "\n",
              "  events.on('output_added.OutputArea', handle_add_output);\n",
              "  events.on('output_updated.OutputArea', handle_update_output);\n",
              "  events.on('clear_output.CodeCell', handle_clear_output);\n",
              "  events.on('delete.Cell', handle_clear_output);\n",
              "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
              "\n",
              "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "    safe: true,\n",
              "    index: 0\n",
              "  });\n",
              "}\n",
              "\n",
              "if (window.Jupyter !== undefined) {\n",
              "  try {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  } catch(err) {\n",
              "  }\n",
              "}\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div id='e5ec018d-b4ed-457c-8335-e2d28121b4c1'>\n",
              "  <div id=\"e63eaa7a-941f-4729-b13c-4f5c542824bc\" data-root-id=\"e5ec018d-b4ed-457c-8335-e2d28121b4c1\" style=\"display: contents;\"></div>\n",
              "</div>\n",
              "<script type=\"application/javascript\">(function(root) {\n",
              "  var docs_json = {\"b3e0add4-d00f-4582-8bb4-3ef687c889ea\":{\"version\":\"3.7.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"e5ec018d-b4ed-457c-8335-e2d28121b4c1\",\"attributes\":{\"name\":\"Row00129\",\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 300px));\\n  -webkit-mask-size: auto calc(min(50%, 300px));\\n}\",{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"13d091a8-89fd-473e-8d29-22e879089400\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.8.2/dist/css/loading.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"146f4296-e293-41cf-9496-870479731aa3\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.8.2/dist/css/listpanel.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"d9e235ce-b39e-4c9d-8109-02e674e2fe37\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.8.2/dist/bundled/theme/default.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"b3a35d5d-63bc-4c59-ae2a-49b948a0f59f\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.8.2/dist/bundled/theme/native.css\"}}],\"margin\":0,\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"GridPlot\",\"id\":\"06aa726c-921c-422c-bcfd-f8da320f5d28\",\"attributes\":{\"rows\":null,\"cols\":null,\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"12cb9233-f86c-4bf4-a894-72b04db36004\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"0c265831-b74f-4269-9e55-f71c74b3dae2\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"1a764aab-1abb-4bf1-9c94-91f50bbdcfa6\",\"attributes\":{\"dimensions\":\"width\"}},{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"23a636e1-587f-48ec-86f0-d198df126d0a\",\"attributes\":{\"dimensions\":\"width\"}}]}},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"be83cb10-f8d9-45cf-b0c6-863f9e5a09c4\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"138a2794-f2ae-49c5-91bc-ab8a0637d97a\",\"attributes\":{\"dimensions\":\"width\",\"renderers\":\"auto\"}}]}},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"47feac84-db5a-4f03-ad07-d163d1129197\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"661ccbc0-310d-4f17-a109-1181b4d74d77\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"2cf653ef-6e72-473f-acd0-e0427326e312\"}]}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"48ec916c-68f9-49bf-937c-cb6e5f86ac71\"}]}},\"toolbar_location\":\"below\",\"children\":[[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"5e893097-e824-47f8-84da-fdedf04d9d9d\",\"attributes\":{\"width\":900,\"height\":50,\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"ec166e9c-0b44-4211-a118-1c4cb63a2661\",\"attributes\":{\"end\":189,\"bounds\":\"auto\"}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"945b5203-d972-4b4f-a5e7-e0d2faf5934d\",\"attributes\":{\"end\":2}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"41309b8b-9123-41c6-b2ee-31e58546e90b\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"0a87d7e9-2375-4940-ab44-29e6bc263bf4\"},\"title\":null,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"618eb87a-6792-467c-a344-e42316cba2b0\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"a9643d14-51fa-46ab-845f-2d737c35b62f\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"2dcaae30-8c0c-48c3-8ffa-b479974e391b\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"90763cc6-2deb-487a-8e70-7b9b5952890b\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"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\"},\"shape\":[376],\"dtype\":\"int32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAAEAAAABAAAAAQAAAA==\"},\"shape\":[376],\"dtype\":\"int32\",\"order\":\"little\"}],[\"recty\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"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\"},\"shape\":[376],\"dtype\":\"float64\",\"order\":\"little\"}],[\"text\",[\"M\",\"K\",\"G\",\"L\",\"I\",\"I\",\"I\",\"G\",\"S\",\"A\",\"Q\",\"V\",\"N\",\"S\",\"H\",\"T\",\"S\",\"A\",\"L\",\"A\",\"R\",\"Y\",\"L\",\"T\",\"E\",\"H\",\"F\",\"K\",\"T\",\"H\",\"D\",\"I\",\"E\",\"A\",\"E\",\"I\",\"F\",\"D\",\"L\",\"A\",\"E\",\"K\",\"P\",\"L\",\"N\",\"Q\",\"L\",\"D\",\"F\",\"S\",\"G\",\"T\",\"T\",\"P\",\"S\",\"I\",\"D\",\"E\",\"I\",\"K\",\"Q\",\"N\",\"M\",\"K\",\"D\",\"L\",\"K\",\"E\",\"K\",\"A\",\"M\",\"A\",\"A\",\"D\",\"F\",\"L\",\"I\",\"L\",\"G\",\"T\",\"P\",\"N\",\"Y\",\"H\",\"G\",\"S\",\"Y\",\"S\",\"G\",\"I\",\"L\",\"K\",\"N\",\"A\",\"L\",\"D\",\"H\",\"L\",\"N\",\"M\",\"D\",\"Y\",\"F\",\"K\",\"M\",\"K\",\"P\",\"V\",\"G\",\"L\",\"I\",\"G\",\"N\",\"S\",\"G\",\"G\",\"I\",\"V\",\"S\",\"S\",\"E\",\"P\",\"L\",\"S\",\"H\",\"L\",\"R\",\"V\",\"I\",\"V\",\"R\",\"S\",\"L\",\"L\",\"G\",\"I\",\"A\",\"V\",\"P\",\"T\",\"Q\",\"I\",\"A\",\"T\",\"H\",\"D\",\"S\",\"D\",\"F\",\"A\",\"K\",\"N\",\"E\",\"D\",\"G\",\"S\",\"Y\",\"Y\",\"L\",\"N\",\"D\",\"S\",\"E\",\"F\",\"Q\",\"L\",\"R\",\"A\",\"R\",\"L\",\"F\",\"V\",\"D\",\"Q\",\"I\",\"V\",\"S\",\"F\",\"V\",\"N\",\"N\",\"S\",\"P\",\"Y\",\"E\",\"H\",\"L\",\"K\",\"M\",\"K\",\"G\",\"L\",\"I\",\"I\",\"V\",\"G\",\"S\",\"A\",\"Q\",\"V\",\"N\",\"S\",\"H\",\"T\",\"S\",\"A\",\"L\",\"A\",\"R\",\"Y\",\"L\",\"S\",\"E\",\"I\",\"F\",\"K\",\"E\",\"H\",\"D\",\"I\",\"E\",\"A\",\"E\",\"I\",\"F\",\"D\",\"L\",\"A\",\"E\",\"H\",\"P\",\"L\",\"N\",\"P\",\"P\",\"D\",\"S\",\"K\",\"G\",\"T\",\"Q\",\"P\",\"K\",\"I\",\"P\",\"E\",\"K\",\"E\",\"K\",\"N\",\"M\",\"K\",\"L\",\"L\",\"K\",\"E\",\"K\",\"A\",\"K\",\"E\",\"A\",\"D\",\"F\",\"I\",\"I\",\"L\",\"A\",\"T\",\"P\",\"N\",\"Y\",\"H\",\"G\",\"S\",\"Y\",\"S\",\"G\",\"I\",\"L\",\"K\",\"N\",\"A\",\"L\",\"D\",\"H\",\"L\",\"D\",\"K\",\"S\",\"H\",\"F\",\"K\",\"M\",\"K\",\"P\",\"V\",\"G\",\"L\",\"I\",\"G\",\"N\",\"S\",\"G\",\"G\",\"I\",\"R\",\"S\",\"S\",\"K\",\"P\",\"L\",\"E\",\"H\",\"L\",\"R\",\"K\",\"I\",\"V\",\"R\",\"E\",\"L\",\"L\",\"G\",\"I\",\"A\",\"I\",\"P\",\"T\",\"Q\",\"I\",\"A\",\"T\",\"H\",\"D\",\"S\",\"D\",\"F\",\"A\",\"K\",\"N\",\"S\",\"D\",\"G\",\"T\",\"Y\",\"Y\",\"L\",\"S\",\"D\",\"S\",\"S\",\"F\",\"K\",\"K\",\"R\",\"A\",\"R\",\"E\",\"Y\",\"V\",\"D\",\"E\",\"I\",\"V\",\"D\",\"A\",\"V\",\"K\",\"N\",\"S\",\"P\",\"Y\",\"E\",\"H\",\"L\",\"K\"]],[\"colors\",[\"blue\",\"red\",\"orange\",\"blue\",\"blue\",\"blue\",\"blue\",\"orange\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"orange\",\"blue\",\"blue\",\"green\",\"red\",\"orange\",\"red\",\"blue\",\"red\",\"green\",\"blue\",\"green\",\"blue\",\"green\",\"blue\",\"orange\",\"green\",\"blue\",\"blue\",\"green\",\"red\",\"yellow\",\"blue\",\"blue\",\"blue\",\"blue\",\"green\",\"orange\",\"blue\",\"orange\",\"blue\",\"blue\",\"yellow\",\"blue\",\"blue\",\"green\",\"green\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"green\",\"blue\",\"red\",\"green\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"green\",\"orange\",\"blue\",\"blue\",\"blue\",\"orange\",\"blue\",\"yellow\",\"blue\",\"orange\",\"red\",\"orange\",\"blue\",\"orange\",\"blue\",\"orange\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"green\",\"red\",\"blue\",\"blue\",\"blue\",\"green\",\"orange\",\"orange\",\"red\",\"blue\",\"red\",\"yellow\",\"blue\",\"orange\",\"blue\",\"blue\",\"orange\",\"blue\",\"blue\",\"orange\",\"orange\",\"blue\",\"blue\",\"blue\",\"blue\",\"green\",\"yellow\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"orange\",\"blue\",\"blue\",\"blue\",\"yellow\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"green\",\"blue\",\"green\",\"orange\",\"blue\",\"red\",\"blue\",\"green\",\"green\",\"orange\",\"blue\",\"orange\",\"orange\",\"blue\",\"blue\",\"green\",\"blue\",\"green\",\"orange\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"orange\",\"blue\",\"green\",\"blue\",\"blue\",\"blue\",\"blue\",\"orange\",\"blue\",\"blue\",\"blue\",\"blue\",\"yellow\",\"orange\",\"green\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"orange\",\"blue\",\"blue\",\"blue\",\"blue\",\"orange\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"orange\",\"blue\",\"blue\",\"green\",\"blue\",\"orange\",\"red\",\"green\",\"red\",\"green\",\"blue\",\"green\",\"blue\",\"green\",\"blue\",\"orange\",\"green\",\"blue\",\"blue\",\"green\",\"red\",\"yellow\",\"blue\",\"blue\",\"yellow\",\"yellow\",\"green\",\"blue\",\"red\",\"orange\",\"blue\",\"blue\",\"yellow\",\"red\",\"blue\",\"yellow\",\"green\",\"red\",\"green\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"green\",\"red\",\"blue\",\"red\",\"green\",\"blue\",\"green\",\"orange\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"yellow\",\"blue\",\"orange\",\"red\",\"orange\",\"blue\",\"orange\",\"blue\",\"orange\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"green\",\"red\",\"blue\",\"green\",\"red\",\"blue\",\"red\",\"orange\",\"red\",\"blue\",\"red\",\"yellow\",\"blue\",\"orange\",\"blue\",\"blue\",\"orange\",\"blue\",\"blue\",\"orange\",\"orange\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"yellow\",\"blue\",\"green\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"green\",\"blue\",\"blue\",\"orange\",\"blue\",\"blue\",\"blue\",\"yellow\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"green\",\"blue\",\"green\",\"orange\",\"blue\",\"red\",\"blue\",\"blue\",\"green\",\"orange\",\"blue\",\"orange\",\"orange\",\"blue\",\"blue\",\"green\",\"blue\",\"blue\",\"orange\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"green\",\"orange\",\"blue\",\"green\",\"green\",\"blue\",\"blue\",\"green\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"yellow\",\"orange\",\"green\",\"red\",\"blue\",\"red\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"fb7b36a0-86da-4cda-a501-a30a08fcfe3f\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"5f82e27c-01f0-410d-9977-daaf727c3e05\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"98c64201-036d-42b1-93b2-18e4af31d400\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"recty\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":null},\"fill_color\":{\"type\":\"field\",\"field\":\"colors\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.6}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"9d21471a-7062-43b8-9f73-5e3ab80d196a\",\"attributes\":{\"tools\":[{\"id\":\"1a764aab-1abb-4bf1-9c94-91f50bbdcfa6\"},{\"id\":\"138a2794-f2ae-49c5-91bc-ab8a0637d97a\"},{\"id\":\"661ccbc0-310d-4f17-a109-1181b4d74d77\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"bd67faa7-2ec5-4d2d-b2d8-e4d9e807b108\"}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"15b490da-fb6e-4225-9fe9-12e3ba7687b2\",\"attributes\":{\"visible\":false,\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"fd976b42-c999-46e8-a83c-c752a4d630f2\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"f864124c-d7ac-4c51-9c9b-4b0462bd92d8\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"6e362823-aa49-4054-8ac2-e88d76f642e2\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"470b443e-602b-4521-a0b0-fa32cdb9b618\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"7bf54e23-dbed-4669-8891-155cc2315882\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"a95d7f13-17fc-4e09-9a7a-a5cbba87521d\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"4b4c88ca-f645-43a4-8390-32abb61e21b6\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"ff0b0bc4-eae5-4b3f-9552-28e4fb8ebb28\",\"attributes\":{\"visible\":false,\"axis\":{\"id\":\"470b443e-602b-4521-a0b0-fa32cdb9b618\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"c51cf32a-3d87-4b8a-a972-da359fdd1a21\",\"attributes\":{\"visible\":false,\"dimension\":1,\"axis\":{\"id\":\"15b490da-fb6e-4225-9fe9-12e3ba7687b2\"}}}],\"min_border\":0}},0,0],[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"4d4b53a3-abb1-4ab4-9d78-40f1eddb6ebc\",\"attributes\":{\"width\":900,\"height\":80,\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"d5c2e02c-9573-4a13-bce8-d8825c994800\",\"attributes\":{\"end\":100}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"4326b111-e246-4a01-9586-70016a6eab72\",\"attributes\":{\"factors\":[\"native,\",\"sample1,\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"9a62e438-b09b-41da-8f1e-53a6751588a3\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"d467a450-738f-4b0a-83cd-30cb8934cc38\"},\"title\":null,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"d2290f61-c708-4ff6-af9f-b00d9dd72b32\",\"attributes\":{\"data_source\":{\"id\":\"a9643d14-51fa-46ab-845f-2d737c35b62f\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"97b42adf-ac04-479a-95dd-8f451840def5\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"c67ba62e-3d2b-4397-bf37-2f921e9af74d\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Text\",\"id\":\"836e6664-6c20-4351-8994-5b46f63b6103\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"text\":{\"type\":\"field\",\"field\":\"text\"},\"text_color\":{\"type\":\"value\",\"value\":\"black\"},\"text_font\":{\"type\":\"field\",\"field\":\"monospace\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"9pt\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"7561b46e-0645-42b5-a4a4-d61577bce5cf\",\"attributes\":{\"data_source\":{\"id\":\"a9643d14-51fa-46ab-845f-2d737c35b62f\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"363abca0-f311-491b-8804-58c28cefb6ce\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"3131dd42-89a3-4bef-8a5b-ecff3f57ee79\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Rect\",\"id\":\"30889c7e-dfa5-4bc3-98a9-483f45a05d0d\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"recty\"},\"width\":{\"type\":\"value\",\"value\":1},\"height\":{\"type\":\"value\",\"value\":1},\"line_color\":{\"type\":\"value\",\"value\":null},\"fill_color\":{\"type\":\"field\",\"field\":\"colors\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.4}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"c6788ba2-cad1-42b5-b292-2dde08a636d9\",\"attributes\":{\"tools\":[{\"id\":\"23a636e1-587f-48ec-86f0-d198df126d0a\"},{\"id\":\"2cf653ef-6e72-473f-acd0-e0427326e312\"}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"93953cdf-5e55-4c63-9e9a-70f0767004d6\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"097a4fe5-1405-4ee8-9a54-981ad9c5fe71\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"29543191-b5ee-4fa9-84ea-a5509b92f5a1\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"81efe972-191a-423e-9b01-0400d7c3b795\"},\"major_tick_line_width\":0,\"minor_tick_line_width\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"b92a88e5-710b-4680-8666-e304f5164354\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"89f11c26-21cf-4a52-9ce9-37154e1c3055\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"65b98569-ad32-4afb-b3aa-8ffa742fd6a2\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"41c68025-aaa4-42a6-ad06-461fd2a554d0\"},\"major_label_text_font_style\":\"bold\"}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"abd1d796-ad24-40e2-93ba-7d88b7219edf\",\"attributes\":{\"visible\":false,\"axis\":{\"id\":\"b92a88e5-710b-4680-8666-e304f5164354\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"230a43b1-7c5f-4d30-85d6-f86804ab94ff\",\"attributes\":{\"visible\":false,\"dimension\":1,\"axis\":{\"id\":\"93953cdf-5e55-4c63-9e9a-70f0767004d6\"}}}],\"min_border\":0}},1,0]]}}]}},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"126a978d-79e6-47e8-97af-bb28832775a0\",\"attributes\":{\"plot_id\":\"e5ec018d-b4ed-457c-8335-e2d28121b4c1\",\"comm_id\":\"464cbea02922466ea78168e091de8a7c\",\"client_comm_id\":\"be139a90ebf041af8e397de55409325e\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"start\",\"kind\":\"Any\",\"default\":0},{\"name\":\"end\",\"kind\":\"Any\",\"default\":100},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"max_notifications\",\"kind\":\"Any\",\"default\":5},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
              "  var render_items = [{\"docid\":\"b3e0add4-d00f-4582-8bb4-3ef687c889ea\",\"roots\":{\"e5ec018d-b4ed-457c-8335-e2d28121b4c1\":\"e63eaa7a-941f-4729-b13c-4f5c542824bc\"},\"root_ids\":[\"e5ec018d-b4ed-457c-8335-e2d28121b4c1\"]}];\n",
              "  var docs = Object.values(docs_json)\n",
              "  if (!docs) {\n",
              "    return\n",
              "  }\n",
              "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
              "  async function embed_document(root) {\n",
              "    var Bokeh = get_bokeh(root)\n",
              "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "    for (const render_item of render_items) {\n",
              "      for (const root_id of render_item.root_ids) {\n",
              "\tconst id_el = document.getElementById(root_id)\n",
              "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
              "\t  const root_el = id_el.children[0]\n",
              "\t  root_el.id = root_el.id + '-rendered'\n",
              "\t  for (const child of root_el.children) {\n",
              "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
              "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
              "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
              "\t  }\n",
              "\t}\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  function get_bokeh(root) {\n",
              "    if (root.Bokeh === undefined) {\n",
              "      return null\n",
              "    } else if (root.Bokeh.version !== py_version) {\n",
              "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
              "\treturn null\n",
              "      }\n",
              "      return root.Bokeh.versions.get(py_version);\n",
              "    } else if (root.Bokeh.version === py_version) {\n",
              "      return root.Bokeh\n",
              "    }\n",
              "    return null\n",
              "  }\n",
              "  function is_loaded(root) {\n",
              "    var Bokeh = get_bokeh(root)\n",
              "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
              "  }\n",
              "  if (is_loaded(root)) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (is_loaded(root)) {\n",
              "        clearInterval(timer);\n",
              "        embed_document(root);\n",
              "      } else if (document.readyState == \"complete\") {\n",
              "        attempts++;\n",
              "        if (attempts > 200) {\n",
              "          clearInterval(timer);\n",
              "\t  var Bokeh = get_bokeh(root)\n",
              "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
              "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
              "\t  } else {\n",
              "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
              "\t    embed_document(root)\n",
              "\t  }\n",
              "        }\n",
              "      }\n",
              "    }, 25, root)\n",
              "  }\n",
              "})(window);</script>"
            ],
            "application/vnd.holoviews_exec.v0+json": "",
            "text/plain": [
              "Bokeh(GridPlot)"
            ]
          },
          "metadata": {
            "application/vnd.holoviews_exec.v0+json": {
              "id": "e5ec018d-b4ed-457c-8335-e2d28121b4c1"
            }
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3. Predict  structures of the designed sequences with AF2"
      ],
      "metadata": {
        "id": "74-F6pY_Ey1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Run ColabFold using custom MSAs and a single model\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Input / Output Folders\n",
        "msa_dir = 'msa' #@param {type:\"string\"}\n",
        "predictions_dir = 'predictions' #@param {type:\"string\"}\n",
        "csv_output_file = 'confidence_metrics.csv' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Model Settings\n",
        "#@markdown Specify model number(s) to run (e.g., \"3\" or \"1,2,3,4,5\")\n",
        "model_order = \"4\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. Setup directories ---\n",
        "result_dir_path = Path(predictions_dir)\n",
        "msa_dir_path = Path(msa_dir)\n",
        "os.makedirs(result_dir_path, exist_ok=True)\n",
        "\n",
        "# --- 2. Run ColabFold using the simple batch command ---\n",
        "print(f\"🚀 Starting ColabFold batch run...\")\n",
        "print(f\"   Input: {msa_dir}\")\n",
        "print(f\"   Output: {predictions_dir}\")\n",
        "print(f\"   Models: {model_order}\")\n",
        "\n",
        "!colabfold_batch \\\n",
        "  --model-order {model_order} \\\n",
        "  --model-type alphafold2_ptm \\\n",
        "  {msa_dir} \\\n",
        "  {predictions_dir}\n",
        "\n",
        "print(\"\\n✅ Prediction run complete.\")\n",
        "\n",
        "# --- 3. Gather confidence results ---\n",
        "print(f\"\\n📊 Parsing results to create {csv_output_file}...\")\n",
        "results = []\n",
        "\n",
        "# --- FIX 1 ---\n",
        "# Search for \"_scores_rank_001_*.json\" instead of \"_unrelaxed_rank_001_*.json\"\n",
        "# Also removed the extra \"*/\" since there are no sub-folders.\n",
        "json_files = sorted(result_dir_path.glob(\"*_scores_rank_001_*.json\"))\n",
        "# --- End FIX 1 ---\n",
        "\n",
        "if not json_files:\n",
        "    print(f\"🔥 Error: No JSON result files found in {predictions_dir}.\")\n",
        "    print(\"   Please check if the predictions ran correctly and produced output.\")\n",
        "    print(f\"   (Was looking for files like: *_scores_rank_001_*.json)\")\n",
        "else:\n",
        "    print(f\"   Found {len(json_files)} result files to parse.\")\n",
        "    for jf in json_files:\n",
        "        try:\n",
        "            # --- FIX 2 ---\n",
        "            # Split the filename by \"_scores\" to get the jobname\n",
        "            jobname = jf.name.split(\"_scores\")[0]\n",
        "            # --- End FIX 2 ---\n",
        "\n",
        "            data = json.load(open(jf))\n",
        "\n",
        "            # This part is correct: it calculates the average from the list\n",
        "            avg_plddt = np.mean(data.get(\"plddt\", [])) if data.get(\"plddt\") else None\n",
        "            ptm = data.get(\"ptm\", None)\n",
        "\n",
        "            model_name = \"unknown\"\n",
        "            if \"model_name\" in data:\n",
        "                model_name = data[\"model_name\"]\n",
        "            elif \"model\" in data:\n",
        "                model_name = data[\"model\"]\n",
        "            else:\n",
        "                for i in model_order.split(','):\n",
        "                    if f\"model_{i}\" in jf.name:\n",
        "                        model_name = f\"model_{i}\"\n",
        "                        break\n",
        "\n",
        "            results.append({\n",
        "                \"sequence_name\": jobname,\n",
        "                \"avg_plddt\": avg_plddt,\n",
        "                \"ptm\": ptm,\n",
        "                \"model_used\": model_name\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error parsing {jf.name}: {e}\")\n",
        "\n",
        "    if results:\n",
        "        df = pd.DataFrame(results).sort_values(\"avg_plddt\", ascending=False)\n",
        "        out_path = result_dir_path / csv_output_file\n",
        "        df.to_csv(out_path, index=False)\n",
        "        print(f\"\\n✅ Successfully saved confidence metrics to: {out_path}\")\n",
        "        print(\"\\n--- Top Results ---\")\n",
        "        print(df.head())\n",
        "    else:\n",
        "        print(\"🔥 No results were successfully parsed. Check predictions directory.\")\n",
        "\n",
        "print(\"\\n🎉 All done.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uWdgcQcDEuG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada67c2c-52f2-4189-c551-669c2d3f37ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting ColabFold batch run...\n",
            "   Input: msa\n",
            "   Output: predictions\n",
            "   Models: 4\n",
            "2025-11-07 01:31:56,753 Running colabfold 1.5.5 (31518c387dcb56c38cfa4f8e3c338a79ab3ff684)\n",
            "2025-11-07 01:32:02,366 Running on GPU\n",
            "2025-11-07 01:32:02,518 Found 5 citations for tools or databases\n",
            "2025-11-07 01:32:02,518 Query 1/11: protein_native (length 188)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1762479137.129182    4495 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Calculate RMSD & Save Aligned Structures\n",
        "#@markdown ---\n",
        "#@markdown ### 1. File Locations\n",
        "#@markdown ---\n",
        "#@markdown Path to your single experimental/reference PDB file:\n",
        "experimental_pdb = \"AZO1.pdb\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown Folder where ColabFold saved the predictions and CSV:\n",
        "predictions_dir = \"predictions\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown Name of the CSV file to read and update:\n",
        "csv_output_file = \"confidence_metrics.csv\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Folder to save all aligned PDBs for visualization:**\n",
        "aligned_pdb_folder = \"aligned\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Alignment Options\n",
        "#@markdown ---\n",
        "#@markdown Select the method for structural superposition:\n",
        "alignment_mode = \"Specific Residues\" #@param [\"All Atoms\", \"Iterative Exclusion\", \"Specific Residues\"]\n",
        "#@markdown ---\n",
        "#@markdown **For \"Iterative Exclusion\" mode:**\n",
        "#@markdown Cutoff in Å. Residues with Cα distance > cutoff after alignment will be excluded.\n",
        "iterative_rmsd_cutoff = 2.0 #@param {type:\"number\"}\n",
        "#@markdown Maximum number of iterations to run.\n",
        "iterative_max_cycles = 5 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "#@markdown **For \"Specific Residues\" mode:**\n",
        "#@markdown Provide a comma-separated list of residues or ranges (e.g., \"10-50, 80, 91-100\").\n",
        "residue_list_to_align = \"1-188\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# Try to import Biopython, which is needed for RMSD\n",
        "try:\n",
        "    from Bio.PDB import PDBParser, Superimposer, PDBIO\n",
        "except ImportError:\n",
        "    print(\" Biopython not found. Installing...\")\n",
        "    !pip install biopython\n",
        "    from Bio.PDB import PDBParser, Superimposer, PDBIO\n",
        "\n",
        "# --- Helper Function to Parse Residue List ---\n",
        "def parse_residue_list(res_string):\n",
        "    \"\"\"Parses a residue string like \"10-50, 80, 91-100\" into a set of integers.\"\"\"\n",
        "    residue_set = set()\n",
        "    if not res_string:\n",
        "        return residue_set\n",
        "\n",
        "    parts = res_string.split(',')\n",
        "    for part in parts:\n",
        "        part = part.strip()\n",
        "        if not part:\n",
        "            continue\n",
        "        if '-' in part:\n",
        "            try:\n",
        "                start, end = part.split('-')\n",
        "                start_res = int(start.strip())\n",
        "                end_res = int(end.strip())\n",
        "                residue_set.update(range(start_res, end_res + 1))\n",
        "            except ValueError:\n",
        "                print(f\"⚠️ Warning: Could not parse range '{part}'. Skipping.\")\n",
        "        else:\n",
        "            try:\n",
        "                residue_set.add(int(part.strip()))\n",
        "            except ValueError:\n",
        "                print(f\"⚠️ Warning: Could not parse residue number '{part}'. Skipping.\")\n",
        "    return residue_set\n",
        "\n",
        "# --- 1. Load Reference Structure & Setup Folders ---\n",
        "print(f\"Loading reference structure: {experimental_pdb}\")\n",
        "pdb_parser = PDBParser(QUIET=True)\n",
        "io = PDBIO() # Initialize PDB saver\n",
        "\n",
        "# Create the output folder for aligned PDBs\n",
        "os.makedirs(aligned_pdb_folder, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    ref_structure = pdb_parser.get_structure(\"reference\", experimental_pdb)\n",
        "except FileNotFoundError:\n",
        "    print(f\"🔥 Error: Experimental PDB not found at: {experimental_pdb}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Get all C-alpha atoms as a dictionary, keyed by residue number\n",
        "try:\n",
        "    ref_ca_dict = {\n",
        "        atom.get_parent().id[1]: atom\n",
        "        for atom in ref_structure[0].get_atoms()\n",
        "        if atom.name == \"CA\" and atom.get_parent().id[0] == ' ' # Ensure it's a standard residue\n",
        "    }\n",
        "except Exception as e:\n",
        "    print(f\"🔥 Error parsing reference PDB: {e}\")\n",
        "    print(\"   Make sure it's a valid PDB file.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if not ref_ca_dict:\n",
        "    print(f\"🔥 Error: No standard C-alpha atoms (CA) found in {experimental_pdb}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"Loaded {len(ref_ca_dict)} C-alpha atoms from reference.\")\n",
        "\n",
        "# --- Save a copy of the reference PDB to the aligned folder ---\n",
        "print(f\"\\nSaving reference structure to {aligned_pdb_folder}...\")\n",
        "io.set_structure(ref_structure)\n",
        "ref_output_name = f\"{Path(experimental_pdb).stem}_ref.pdb\"\n",
        "ref_output_path = os.path.join(aligned_pdb_folder, ref_output_name)\n",
        "io.save(ref_output_path)\n",
        "print(f\"  Saved: {ref_output_path}\")\n",
        "\n",
        "# --- 2. Load CSV File ---\n",
        "csv_path = os.path.join(predictions_dir, csv_output_file)\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"🔥 Error: CSV file not found at: {csv_path}\")\n",
        "    print(\"   Please run the previous cell to generate the CSV.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 3. Parse alignment residues if needed ---\n",
        "alignment_residue_set = set()\n",
        "if alignment_mode == \"Specific Residues\":\n",
        "    alignment_residue_set = parse_residue_list(residue_list_to_align)\n",
        "    if not alignment_residue_set:\n",
        "        print(f\"🔥 Error: 'Specific Residues' mode selected, but no valid residues found in '{residue_list_to_align}'.\")\n",
        "        sys.exit(1)\n",
        "    print(f\"Running in 'Specific Residues' mode. Aligning on {len(alignment_residue_set)} specified residues.\")\n",
        "elif alignment_mode == \"Iterative Exclusion\":\n",
        "    print(f\"Running in 'Iterative Exclusion' mode (Cutoff={iterative_rmsd_cutoff} Å, Max Cycles={iterative_max_cycles}).\")\n",
        "else:\n",
        "    print(\"Running in 'All Atoms' mode.\")\n",
        "\n",
        "# --- 4. Loop Through Predictions and Calculate RMSD ---\n",
        "rmsd_list = []\n",
        "aligned_residues_list = []\n",
        "super_imposer = Superimposer()\n",
        "\n",
        "print(\"\\nCalculating RMSD and saving aligned structures...\")\n",
        "for index, row in df.iterrows():\n",
        "    jobname = row['sequence_name']\n",
        "\n",
        "    # Find the corresponding PDB file\n",
        "    pdb_pattern = os.path.join(predictions_dir, f\"{jobname}_unrelaxed_rank_001_*.pdb\")\n",
        "    pdb_files = glob.glob(pdb_pattern)\n",
        "\n",
        "    if not pdb_files:\n",
        "        print(f\"⚠️ Warning: No PDB file found for {jobname}. Skipping.\")\n",
        "        rmsd_list.append(np.nan)\n",
        "        aligned_residues_list.append(np.nan)\n",
        "        continue\n",
        "\n",
        "    predicted_pdb_path = pdb_files[0]\n",
        "\n",
        "    try:\n",
        "        # Load the predicted structure and get its C-alpha dict\n",
        "        sample_structure = pdb_parser.get_structure(jobname, predicted_pdb_path)\n",
        "        sample_ca_dict = {\n",
        "            atom.get_parent().id[1]: atom\n",
        "            for atom in sample_structure[0].get_atoms()\n",
        "            if atom.name == \"CA\" and atom.get_parent().id[0] == ' '\n",
        "        }\n",
        "\n",
        "        if not sample_ca_dict:\n",
        "            print(f\"⚠️ Warning: No C-alpha atoms found in {jobname}. Skipping.\")\n",
        "            rmsd_list.append(np.nan)\n",
        "            aligned_residues_list.append(np.nan)\n",
        "            continue\n",
        "\n",
        "        # --- Start Alignment Logic ---\n",
        "        rmsd_val = np.nan\n",
        "        num_aligned = 0\n",
        "\n",
        "        if alignment_mode == \"Specific Residues\":\n",
        "            common_res_ids = alignment_residue_set & set(ref_ca_dict.keys()) & set(sample_ca_dict.keys())\n",
        "            if len(common_res_ids) < len(alignment_residue_set):\n",
        "                print(f\"  Info for {jobname}: Using {len(common_res_ids)} common residues out of {len(alignment_residue_set)} requested.\")\n",
        "\n",
        "            if not common_res_ids:\n",
        "                print(f\"  ⚠️ Warning: No common residues for alignment in {jobname}. Skipping.\")\n",
        "                rmsd_list.append(np.nan)\n",
        "                aligned_residues_list.append(0)\n",
        "                continue\n",
        "\n",
        "            ref_atoms = [ref_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            sample_atoms = [sample_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            num_aligned = len(common_res_ids)\n",
        "\n",
        "            super_imposer.set_atoms(ref_atoms, sample_atoms)\n",
        "            # Apply transformation to the *entire* structure\n",
        "            super_imposer.apply(sample_structure[0].get_atoms())\n",
        "            rmsd_val = super_imposer.rms\n",
        "\n",
        "            print_msg = f\"  ✅ {jobname}: RMSD = {rmsd_val:.3f} Å (on {num_aligned} specified residues)\"\n",
        "\n",
        "\n",
        "        elif alignment_mode == \"Iterative Exclusion\":\n",
        "            current_res_ids = set(ref_ca_dict.keys()) & set(sample_ca_dict.keys())\n",
        "\n",
        "            for i in range(iterative_max_cycles):\n",
        "                if not current_res_ids:\n",
        "                    print(f\"  🔥 Error for {jobname}: No atoms left to align during iteration.\")\n",
        "                    rmsd_val = np.nan\n",
        "                    break\n",
        "\n",
        "                ref_atoms_subset = [ref_ca_dict[res_id] for res_id in current_res_ids]\n",
        "                sample_atoms_subset = [sample_ca_dict[res_id] for res_id in current_res_ids]\n",
        "\n",
        "                super_imposer.set_atoms(ref_atoms_subset, sample_atoms_subset)\n",
        "                super_imposer.apply(sample_structure[0].get_atoms())\n",
        "                rmsd_val = super_imposer.rms\n",
        "\n",
        "                new_res_ids = set()\n",
        "                for res_id in current_res_ids:\n",
        "                    dist = ref_ca_dict[res_id] - sample_ca_dict[res_id]\n",
        "                    if dist < iterative_rmsd_cutoff:\n",
        "                        new_res_ids.add(res_id)\n",
        "\n",
        "                if len(new_res_ids) == len(current_res_ids):\n",
        "                    print_msg = f\"  ✅ {jobname}: Converged. RMSD = {rmsd_val:.3f} Å (on {len(new_res_ids)} core atoms)\"\n",
        "                    num_aligned = len(new_res_ids)\n",
        "                    break\n",
        "\n",
        "                print(f\"    Iter {i+1} for {jobname}: RMSD={rmsd_val:.3f} ({len(current_res_ids)} atoms) -> Removing {len(current_res_ids) - len(new_res_ids)} outliers.\")\n",
        "                current_res_ids = new_res_ids\n",
        "            else:\n",
        "                print_msg = f\"  ⚠️ {jobname}: Max iterations reached. Final RMSD = {rmsd_val:.3f} Å (on {len(current_res_ids)} core atoms)\"\n",
        "                num_aligned = len(current_res_ids)\n",
        "\n",
        "            rmsd_list.append(rmsd_val)\n",
        "            aligned_residues_list.append(num_aligned)\n",
        "\n",
        "        else: # \"All Atoms\" mode (default)\n",
        "            common_res_ids = set(ref_ca_dict.keys()) & set(sample_ca_dict.keys())\n",
        "\n",
        "            ref_atoms = [ref_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            sample_atoms = [sample_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            num_aligned = len(common_res_ids)\n",
        "\n",
        "            if len(ref_atoms) != len(ref_ca_dict) or len(sample_atoms) != len(sample_ca_dict):\n",
        "                 print(f\"  Info for {jobname}: Found {num_aligned} common atoms for alignment.\")\n",
        "\n",
        "            super_imposer.set_atoms(ref_atoms, sample_atoms)\n",
        "            # Apply transformation to the *entire* structure\n",
        "            super_imposer.apply(sample_structure[0].get_atoms())\n",
        "            rmsd_val = super_imposer.rms\n",
        "\n",
        "            print_msg = f\"  ✅ {jobname}: RMSD = {rmsd_val:.3f} Å (on {num_aligned} atoms)\"\n",
        "\n",
        "        # --- Save the aligned structure ---\n",
        "        if not np.isnan(rmsd_val):\n",
        "            output_filename = os.path.join(aligned_pdb_folder, f\"{jobname}_aligned.pdb\")\n",
        "            io.set_structure(sample_structure)\n",
        "            io.save(output_filename)\n",
        "            print(print_msg + f\" -> Saved to {output_filename}\")\n",
        "        else:\n",
        "            print(print_msg) # Print error/warning message from loop\n",
        "\n",
        "        rmsd_list.append(rmsd_val)\n",
        "        aligned_residues_list.append(num_aligned)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🔥 Error processing {jobname}: {e}\")\n",
        "        rmsd_list.append(np.nan)\n",
        "        aligned_residues_list.append(np.nan)\n",
        "\n",
        "# --- 5. Update DataFrame and Save ---\n",
        "df['rmsd_to_exp (Å)'] = rmsd_list\n",
        "df['rmsd_aligned_residues'] = aligned_residues_list\n",
        "df = df.sort_values(\"rmsd_to_exp (Å)\", ascending=True)\n",
        "\n",
        "# Save the updated CSV\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"\\n✅ Successfully calculated RMSD, saved aligned PDBs to '{aligned_pdb_folder}', and updated {csv_path}.\")\n",
        "print(f\"   Mode used: {alignment_mode}\")\n",
        "print(\"\\n--- Results (Sorted by RMSD) ---\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iSRF6nd0jw0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Plot pLDDT, pTM, and RMSD (Interactive, Wide)\n",
        "#@markdown ---\n",
        "#@markdown ### 1. File Locations\n",
        "#@markdown ---\n",
        "#@markdown Folder where your CSV file is located:\n",
        "predictions_dir = \"predictions\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown Name of the CSV file to read:\n",
        "csv_output_file = \"confidence_metrics.csv\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Name of your native/reference sequence:**\n",
        "#@markdown (This must match the 'sequence_name' in the CSV exactly for red coloring)\n",
        "native_sequence_name = \"1LVM_A_native\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Output\n",
        "#@markdown ---\n",
        "#@markdown Name of the interactive plot file to save (must be .json):\n",
        "plot_output_file_json = \"all_metrics_plot.json\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import altair as alt\n",
        "import sys\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "csv_path = os.path.join(predictions_dir, csv_output_file)\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"🔥 Error: CSV file not found at: {csv_path}\")\n",
        "    print(\"   Please run the previous cells to generate and update the CSV.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Check if required columns exist\n",
        "required_cols = ['sequence_name', 'avg_plddt', 'ptm', 'rmsd_to_exp (Å)']\n",
        "if not all(col in df.columns for col in required_cols):\n",
        "    print(f\"🔥 Error: The CSV file is missing one or more required columns.\")\n",
        "    print(f\"   Required: {required_cols}\")\n",
        "    print(f\"   Found: {list(df.columns)}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 2. Prepare Data for Plotting ---\n",
        "\n",
        "# --- NEW: Divide pLDDT by 100 ---\n",
        "print(\"Applying pLDDT / 100 transformation...\")\n",
        "df['avg_plddt'] = df['avg_plddt'] / 100.0\n",
        "# ---\n",
        "\n",
        "# Create the 'model_type' column for coloring\n",
        "if native_sequence_name not in df['sequence_name'].values:\n",
        "    print(f\"⚠️ Warning: Native sequence name '{native_sequence_name}' not found in CSV.\")\n",
        "    print(\"   All points will be colored orange.\")\n",
        "    df['model_type'] = \"Designed\"\n",
        "else:\n",
        "    df['model_type'] = np.where(\n",
        "        df['sequence_name'] == native_sequence_name,\n",
        "        'Native',\n",
        "        'Designed'\n",
        "    )\n",
        "\n",
        "# \"Melt\" the DataFrame from wide to long format\n",
        "df_melted = df.melt(\n",
        "    id_vars=['sequence_name', 'model_type'],\n",
        "    value_vars=['avg_plddt', 'ptm', 'rmsd_to_exp (Å)'],\n",
        "    var_name='Metric',\n",
        "    value_name='Value'\n",
        ")\n",
        "\n",
        "# --- UPDATED: Clean up metric names for better plot titles ---\n",
        "df_melted['Metric'] = df_melted['Metric'].replace({\n",
        "    'avg_plddt': 'pLDDT / 100',  # <-- Title changed here\n",
        "    'ptm': 'pTM Score',\n",
        "    'rmsd_to_exp (Å)': 'RMSD (Å)'\n",
        "})\n",
        "\n",
        "# --- 3. Create the Interactive Altair Plot ---\n",
        "print(f\"Generating interactive plot...\")\n",
        "\n",
        "# Define the custom color scale\n",
        "color_scale = alt.Scale(domain=['Native', 'Designed'],\n",
        "                        range=['red', 'orange'])\n",
        "\n",
        "# Create the base chart\n",
        "base = alt.Chart(df_melted).mark_circle(size=80, opacity=0.7).encode(\n",
        "    # X-axis: Native vs. Designed (no title, labels at bottom)\n",
        "    x=alt.X('model_type:N', title=None, axis=alt.Axis(labels=True, ticks=False, title=\"\")),\n",
        "\n",
        "    # Y-axis: The metric's value\n",
        "    y=alt.Y('Value:Q', title='Value'),\n",
        "\n",
        "    # Color based on type\n",
        "    color=alt.Color('model_type:N', scale=color_scale, legend=alt.Legend(title=\"Model Type\")),\n",
        "\n",
        "    # Show this information on hover\n",
        "    tooltip=[\n",
        "        alt.Tooltip('sequence_name:N', title='Model'),\n",
        "        alt.Tooltip('Metric:N', title='Metric'),\n",
        "        alt.Tooltip('Value:Q', title='Value', format='.3f') # Use .3f for 0-1 scale\n",
        "    ]\n",
        ").properties(\n",
        "    # --- NEW: Make each plot wider ---\n",
        "    width=100\n",
        ").interactive() # Make the chart interactive (zoom/pan)\n",
        "\n",
        "# Create the final faceted chart\n",
        "chart = base.facet(\n",
        "    # Create one column for each \"Metric\".\n",
        "    # The header for each facet will be the metric's name\n",
        "    column=alt.Column('Metric:N', header=alt.Header(\n",
        "        titleOrient=\"top\",\n",
        "        labelOrient=\"top\"\n",
        "    ))\n",
        ").resolve_scale(\n",
        "    # Make the Y-axis independent for each plot\n",
        "    y='independent'\n",
        ")\n",
        "\n",
        "# --- 4. Save and Display the Plot ---\n",
        "json_path = os.path.join(predictions_dir, plot_output_file_json)\n",
        "chart.save(json_path)\n",
        "\n",
        "print(f\"✅ Successfully saved interactive plot to: {json_path}\")\n",
        "\n",
        "# Display the chart in the Colab output\n",
        "chart"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8ifkiA22okZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK7X9T44pWb7",
        "cellView": "form"
      },
      "source": [
        "#@title 4) Display the Aligned 3D Structure {run: \"auto\"}\n",
        "import py3Dmol\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from colabfold.colabfold import plot_plddt_legend\n",
        "from colabfold.colabfold import pymol_color_list, alphabet_list\n",
        "import sys # Added for error checking\n",
        "from pathlib import Path\n",
        "\n",
        "#@markdown ### 1. PDB Location\n",
        "#@markdown ---\n",
        "#@markdown Folder where the aligned structures were saved:\n",
        "aligned_structures_folder = \"aligned\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Jobname of the sequence to display:**\n",
        "#@markdown (e.g., \"1LVM_A_native\", \"1LVM_A_sample1\")\n",
        "jobname = \"1LVM_A_sample1\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Display Options\n",
        "#@markdown ---\n",
        "color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
        "show_sidechains = False #@param {type:\"boolean\"}\n",
        "show_mainchains = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown **Overlay the reference structure?**\n",
        "show_reference = True #@param {type:\"boolean\"}\n",
        "#@markdown Path to the saved reference PDB:\n",
        "reference_pdb_path = \"aligned/1LVM_A_ref.pdb\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "# --- Find the aligned PDB file ---\n",
        "pdb_pattern = f\"{aligned_structures_folder}/{jobname}_aligned.pdb\"\n",
        "pdb_file_list = glob.glob(pdb_pattern)\n",
        "\n",
        "def show_pdb(\n",
        "    predicted_pdb_path,\n",
        "    show_reference=False,\n",
        "    reference_pdb_path=None,\n",
        "    show_sidechains=False,\n",
        "    show_mainchains=False,\n",
        "    color=\"lDDT\"\n",
        "):\n",
        "\n",
        "    view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "\n",
        "    # --- 1. Add Reference Structure (if requested) ---\n",
        "    if show_reference:\n",
        "        try:\n",
        "            view.addModel(open(reference_pdb_path,'r').read(),'pdb')\n",
        "            # Style the *first* model (index 0) as gray\n",
        "            view.setStyle({'model': 0}, {'cartoon': {'color': 'gray'}})\n",
        "        except FileNotFoundError:\n",
        "            print(f\"⚠️ Warning: Reference PDB not found at {reference_pdb_path}. Skipping.\")\n",
        "\n",
        "    # --- 2. Add Predicted Structure ---\n",
        "    view.addModel(open(predicted_pdb_path,'r').read(),'pdb')\n",
        "    # Style the *last added* model (index -1)\n",
        "    model_style = {'model': -1}\n",
        "\n",
        "    if color == \"lDDT\":\n",
        "        view.setStyle(model_style, {'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "    elif color == \"rainbow\":\n",
        "        view.setStyle(model_style, {'cartoon': {'color':'spectrum'}})\n",
        "    elif color == \"chain\":\n",
        "        # Simple chain coloring\n",
        "        view.setStyle(model_style, {'cartoon': {'color':'chain'}})\n",
        "\n",
        "    if show_sidechains:\n",
        "        BB = ['C','O','N']\n",
        "        view.addStyle({'and':[model_style, {'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                            {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "        view.addStyle({'and':[model_style, {'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                            {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "        view.addStyle({'and':[model_style, {'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                            {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    if show_mainchains:\n",
        "        BB = ['C','O','N','CA']\n",
        "        view.addStyle({'and':[model_style, {'atom':BB}]},\n",
        "                            {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "    view.zoomTo()\n",
        "    return view\n",
        "\n",
        "# --- Display the structure ---\n",
        "if not pdb_file_list:\n",
        "    print(f\"🔥 Error: Could not find aligned PDB file.\")\n",
        "    print(f\"   Searched for pattern: {pdb_pattern}\")\n",
        "    print(f\"   Please check 'aligned_structures_folder' and 'jobname'.\")\n",
        "    print(f\"   (Did you run the RMSD script to generate the aligned files?)\")\n",
        "else:\n",
        "    pdb_to_show = pdb_file_list[0]\n",
        "    print(f\"Displaying: {pdb_to_show}\")\n",
        "    if show_reference:\n",
        "        print(f\"Overlaying: {reference_pdb_path}\")\n",
        "\n",
        "    view = show_pdb(\n",
        "        pdb_to_show,\n",
        "        show_reference=show_reference,\n",
        "        reference_pdb_path=reference_pdb_path,\n",
        "        show_sidechains=show_sidechains,\n",
        "        show_mainchains=show_mainchains,\n",
        "        color=color\n",
        "    )\n",
        "    view.show()\n",
        "\n",
        "    if color == \"lDDT\":\n",
        "        plot_plddt_legend().show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}