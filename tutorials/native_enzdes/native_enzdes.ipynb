{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4yBrceuFbf3"
      },
      "source": [
        "# **Native sequence redesign using ProteinMPNN and evolutionary information**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pb3lab/AI4PD_2025/blob/main/tutorials/native_enzdes/native_enzdes.ipynb)"
      ],
      "metadata": {
        "id": "_49KeNzjry_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we will exemplify how to combine ProteinMPNN with experimental information about active site residues and evolutionary information â€“ in the form conservation of residues at difference sequence identity percentages from a multiple sequence alignment (MSA) â€“ to perform **native sequence redesign of natural enzymes**.\n",
        "\n",
        "This tutorial is largely based on the breakthough article by **Kiera Sumida**, published in [JACS in 2024](https://pubs.acs.org/doi/10.1021/jacs.3c10941), with some key differences and similarities:\n",
        "1) As in the original article, active site residues are defined as residues containing backbone atoms within 7 Ã… or sidechains atoms within 6 Ã… of the ligand.\n",
        "2) We are using an MSA generated using [MMseqs2](https://github.com/soedinglab/MMseqs2) instead of [HHblits](https://github.com/soedinglab/hh-suite) for retrieven homologous enzymes to generate an MSA. The primary reason for using MMseqs2 over HHblits is its massive speed advantage.\n",
        "3) In the original article, four iterative HHblits searches were performed against the UniRef30 database at E-value cutoffs of 1e-50, 1e-30, 1e-10 and 1e-4. Here, the database being used by MMseqs2 are built from extensive sequence sets like UniRef30 and other environmental sequences\n",
        "4) We maintained the filtering of the sequences in the MSA at 90% sequence identity, 50% coverage and 30% minimum query identity.\n",
        "5) In the original article, each position in the MSA was ranked based on how highly conserved the most frequent amino acid identity was, selecting the top 30%, 50%, and 70% most conserved positions to fix. Here, we are only fixing the top 70% most conserved positions.\n",
        "6) We are using the same ProteinMPNN model, trained with 0.2 Ã… applied to the training set of protein bacbones.\n",
        "7) Three sampling temperatures were tried for ProteinMPNN during the protein sequence generation stage (0.1, 0.2, 0.3), whereas only one 0.2 is used in this tutorial.\n",
        "8) Only 4 sequences are being generated, whereas 144 sequences were generated in the original article.\n",
        "9) Only model 4 is being used for the AlphaFold predictions, as in the original article, but we are only filtering candidates based on pLDDT >85, and not on CÎ± RMSD < 2 Ã…. This will be added in the future."
      ],
      "metadata": {
        "id": "7NPqtfGm5jgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0. Install the different packages required to run this tutorial\n",
        "\n",
        "### **Please install all the different dependencies at the beginning of the tutorial in the order they are indicated in this notebook.**"
      ],
      "metadata": {
        "id": "bB9h6Pn8BAzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Install PyRosetta\n",
        "#@markdown This installation will require you to authorize linking Google Drive to Google Colab, to expedite the installation of PyRosetta in future works. Please accept this, before moving onto the next steps.\n",
        "%%time\n",
        "import os\n",
        "!pip install jupyter_bokeh --quiet\n",
        "!pip install pyrosettacolabsetup --quiet\n",
        "import pyrosettacolabsetup; pyrosettacolabsetup.install_pyrosetta()\n",
        "import pyrosetta; pyrosetta.init()"
      ],
      "metadata": {
        "id": "1sUnd4p9F8IC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Install ProteinMPNN\n",
        "import json, time, os, sys, glob\n",
        "\n",
        "if not os.path.isdir(\"ProteinMPNN\"):\n",
        "  os.system(\"git clone -q https://github.com/dauparas/ProteinMPNN.git\")\n",
        "sys.path.append('/content/ProteinMPNN')"
      ],
      "metadata": {
        "id": "sEi7JwX4dWaV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Setup ProteinMPNN model\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split, Subset\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import os.path\n",
        "from protein_mpnn_utils import loss_nll, loss_smoothed, gather_edges, gather_nodes, gather_nodes_t, cat_neighbors_nodes, _scores, _S_to_seq, tied_featurize, parse_PDB\n",
        "from protein_mpnn_utils import StructureDataset, StructureDatasetPDB, ProteinMPNN\n",
        "\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "#v_48_010=version with 48 edges 0.10A noise\n",
        "model_name = \"v_48_020\" #@param [\"v_48_002\", \"v_48_010\", \"v_48_020\", \"v_48_030\"]\n",
        "\n",
        "\n",
        "backbone_noise=0.00               # Standard deviation of Gaussian noise to add to backbone atoms\n",
        "\n",
        "path_to_model_weights='/content/ProteinMPNN/vanilla_model_weights'\n",
        "hidden_dim = 128\n",
        "num_layers = 3\n",
        "model_folder_path = path_to_model_weights\n",
        "if model_folder_path[-1] != '/':\n",
        "    model_folder_path = model_folder_path + '/'\n",
        "checkpoint_path = model_folder_path + f'{model_name}.pt'\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "print('Number of edges:', checkpoint['num_edges'])\n",
        "noise_level_print = checkpoint['noise_level']\n",
        "print(f'Training noise level: {noise_level_print}A')\n",
        "model = ProteinMPNN(num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
        "model.to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(\"Model loaded\")"
      ],
      "metadata": {
        "id": "HNvZ8PSydflj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4) Helper functions for ProteinMPNN\n",
        "def make_tied_positions_for_homomers(pdb_dict_list):\n",
        "    my_dict = {}\n",
        "    for result in pdb_dict_list:\n",
        "        all_chain_list = sorted([item[-1:] for item in list(result) if item[:9]=='seq_chain']) #A, B, C, ...\n",
        "        tied_positions_list = []\n",
        "        chain_length = len(result[f\"seq_chain_{all_chain_list[0]}\"])\n",
        "        for i in range(1,chain_length+1):\n",
        "            temp_dict = {}\n",
        "            for j, chain in enumerate(all_chain_list):\n",
        "                temp_dict[chain] = [i] #needs to be a list\n",
        "            tied_positions_list.append(temp_dict)\n",
        "        my_dict[result['name']] = tied_positions_list\n",
        "    return my_dict"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p8UCmlCnvIah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5) Setup ColabFold without AMBER (i.e. without relax)\n",
        "\n",
        "from sys import version_info\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "use_amber = False\n",
        "use_templates = True\n",
        "python_version = python_version"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uubxVRG9JjHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6) Install dependencies for running MMseqs2 and ColabFold\n",
        "%%time\n",
        "%%bash -s $use_amber $use_templates $python_version\n",
        "\n",
        "set -e\n",
        "\n",
        "USE_AMBER=$1\n",
        "USE_TEMPLATES=$2\n",
        "PYTHON_VERSION=$3\n",
        "\n",
        "if [ ! -f COLABFOLD_READY ]; then\n",
        "  # install dependencies\n",
        "  # We have to use \"--no-warn-conflicts\" because colab already has a lot preinstalled with requirements different to ours\n",
        "  pip install -q --no-warn-conflicts \"colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold\"\n",
        "  if [ -n \"${TPU_NAME}\" ]; then\n",
        "    pip install -q --no-warn-conflicts -U dm-haiku==0.0.10 jax==0.3.25\n",
        "  fi\n",
        "  ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\n",
        "  ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\n",
        "  # hack to fix TF crash\n",
        "  rm -f /usr/local/lib/python3.*/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so\n",
        "  touch COLABFOLD_READY\n",
        "fi\n",
        "\n",
        "# Download params (~1min)\n",
        "python -m colabfold.download\n",
        "\n",
        "# setup conda\n",
        "if [ ${USE_AMBER} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
        "  if [ ! -f CONDA_READY ]; then\n",
        "    wget -qnc https://github.com/conda-forge/miniforge/releases/download/25.3.1-0/Miniforge3-25.3.1-0-Linux-x86_64.sh\n",
        "    bash Miniforge3-25.3.1-0-Linux-x86_64.sh -bfp /usr/local 2>&1 1>/dev/null\n",
        "    rm Miniforge3-25.3.1-0-Linux-x86_64.sh\n",
        "    conda config --set auto_update_conda false\n",
        "    touch CONDA_READY\n",
        "  fi\n",
        "fi\n",
        "# setup template search\n",
        "if [ ${USE_TEMPLATES} == \"True\" ] && [ ! -f HH_READY ]; then\n",
        "  conda install -y -q -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python=\"${PYTHON_VERSION}\" 2>&1 1>/dev/null\n",
        "  touch HH_READY\n",
        "fi\n",
        "# setup openmm for amber refinement\n",
        "if [ ${USE_AMBER} == \"True\" ] && [ ! -f AMBER_READY ]; then\n",
        "  conda install -y -q -c conda-forge openmm=8.2.0 python=\"${PYTHON_VERSION}\" pdbfixer 2>&1 1>/dev/null\n",
        "  touch AMBER_READY\n",
        "fi"
      ],
      "metadata": {
        "id": "AzIKiDiCaHAn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Once you have finished installing these dependencies, we are ready to perform the tutorial**"
      ],
      "metadata": {
        "id": "zps1RWW0BlKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. Determine the active site residues and highly conserved residues of the enzyme to sequence redesign\n",
        "\n",
        "### Please follow the steps in the order they are presented in this tutorial"
      ],
      "metadata": {
        "id": "7WZL0b3nCZ4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Download Structure, Extract Sequence, and Split Protein and Ligand Chains\n",
        "#@markdown #### This is useful when working with small molecules, which must be extracted from the protein structure and parameterized before using them on PyRosetta. This will be added in a future version of this tutorial.\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Enter PDB and Chain Information\n",
        "import pyrosetta; pyrosetta.init()\n",
        "pdb_id = '1LVM' #@param {type:\"string\"}\n",
        "target_chain_letter = 'A' #@param {type:\"string\"}\n",
        "#@markdown (Optional) Provide a second chain letter to save its structure.\n",
        "ligand_chain_letter = 'C' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import pyrosetta\n",
        "# No toolbox imports needed\n",
        "\n",
        "# --- 1. Define all filenames based on form input ---\n",
        "full_pdb_filename = f\"{pdb_id}.pdb\"\n",
        "target_fasta_filename = f\"{pdb_id}_{target_chain_letter}.fasta\"\n",
        "target_pdb_filename = f\"{pdb_id}_{target_chain_letter}.pdb\"\n",
        "\n",
        "if ligand_chain_letter:\n",
        "    ligand_pdb_filename = f\"{pdb_id}_{ligand_chain_letter}.pdb\"\n",
        "\n",
        "print(f\"Starting process for PDB: {pdb_id}\")\n",
        "\n",
        "# --- 2. Download the full PDB file ---\n",
        "if not os.path.isfile(full_pdb_filename):\n",
        "    print(f\"Downloading {full_pdb_filename} from RCSB PDB...\")\n",
        "    !wget -q -O {full_pdb_filename} https://files.rcsb.org/download/{full_pdb_filename}\n",
        "    print(f\"Successfully downloaded {full_pdb_filename}.\")\n",
        "else:\n",
        "    print(f\"{full_pdb_filename} already exists. Using local file.\")\n",
        "\n",
        "# --- 3. Load Pose and Process Chains ---\n",
        "try:\n",
        "    print(f\"\\nLoading full pose from {full_pdb_filename}...\")\n",
        "    pose = pyrosetta.pose_from_pdb(full_pdb_filename)\n",
        "\n",
        "    # --- 3A. Process Target Chain (Sequence AND Structure) ---\n",
        "    print(f\"Processing target chain: {target_chain_letter}\")\n",
        "    target_chain_index = -1\n",
        "\n",
        "    # Find the PyRosetta chain index (1, 2, 3...) from the PDB chain letter (A, B, C...)\n",
        "    for i in range(1, pose.num_chains() + 1):\n",
        "        first_res_num = pose.chain_begin(i)\n",
        "\n",
        "        # --- THIS LINE IS FIXED ---\n",
        "        # Get the chain letter for that residue from PDB info\n",
        "        chain_letter = pose.pdb_info().chain(first_res_num)\n",
        "\n",
        "        if chain_letter == target_chain_letter:\n",
        "            target_chain_index = i\n",
        "            break\n",
        "\n",
        "    if target_chain_index != -1:\n",
        "        # Get sequence\n",
        "        sequence = pose.chain_sequence(target_chain_index)\n",
        "        with open(target_fasta_filename, 'w') as f:\n",
        "            f.write(f\">{pdb_id}_{target_chain_letter}\\n\")\n",
        "            f.write(sequence)\n",
        "        print(f\"âœ… Successfully saved sequence to {target_fasta_filename}\")\n",
        "\n",
        "        # --- Manually build a new pose for the chain ---\n",
        "        target_chain_pose = pyrosetta.Pose()\n",
        "        start_res = pose.chain_begin(target_chain_index)\n",
        "        end_res = pose.chain_end(target_chain_index)\n",
        "\n",
        "        target_chain_pose.append_residue_by_jump(pose.residue(start_res), 1)\n",
        "        for res_num in range(start_res + 1, end_res + 1):\n",
        "            target_chain_pose.append_residue_by_bond(pose.residue(res_num))\n",
        "\n",
        "        target_chain_pose.dump_pdb(target_pdb_filename)\n",
        "        print(f\"âœ… Successfully saved structure to {target_pdb_filename}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"âš ï¸ Error: Target chain '{target_chain_letter}' not found in PDB info.\")\n",
        "\n",
        "    # --- 3B. Process Ligand Chain (Structure ONLY) ---\n",
        "    if ligand_chain_letter:\n",
        "        print(f\"\\nProcessing ligand chain: {ligand_chain_letter}\")\n",
        "        ligand_chain_index = -1\n",
        "\n",
        "        for i in range(1, pose.num_chains() + 1):\n",
        "            first_res_num = pose.chain_begin(i)\n",
        "\n",
        "            # --- THIS LINE IS FIXED ---\n",
        "            # Get the chain letter for that residue from PDB info\n",
        "            chain_letter = pose.pdb_info().chain(first_res_num)\n",
        "\n",
        "            if chain_letter == ligand_chain_letter:\n",
        "                ligand_chain_index = i\n",
        "                break\n",
        "\n",
        "        if ligand_chain_index != -1:\n",
        "            # --- Manually build a new pose for this chain ---\n",
        "            ligand_chain_pose = pyrosetta.Pose()\n",
        "            start_res = pose.chain_begin(ligand_chain_index)\n",
        "            end_res = pose.chain_end(ligand_chain_index)\n",
        "\n",
        "            ligand_chain_pose.append_residue_by_jump(pose.residue(start_res), 1)\n",
        "            for res_num in range(start_res + 1, end_res + 1):\n",
        "                ligand_chain_pose.append_residue_by_bond(pose.residue(res_num))\n",
        "\n",
        "            ligand_chain_pose.dump_pdb(ligand_pdb_filename)\n",
        "            print(f\"âœ… Successfully saved structure to {ligand_pdb_filename}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ Error: Ligand chain '{ligand_chain_letter}' not found in PDB info.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ”¥ Error during processing: {e}\")\n",
        "\n",
        "print(\"\\nProcess complete.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Kt2nJuU0HbOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Define Active Site Residues Around Ligand using PyRosetta\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Input Files and Chains\n",
        "#@markdown Enter the filenames of your separated protein and ligand chains.\n",
        "protein_pdb_file = '1LVM_A.pdb' #@param {type:\"string\"}\n",
        "protein_chain_letter = 'A' #@param {type:\"string\"}\n",
        "ligand_pdb_file = '1LVM_C.pdb' #@param {type:\"string\"}\n",
        "ligand_chain_letter = 'C' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### 2. Ligand Parameters\n",
        "#@markdown (This section has been removed as .params files are not needed for peptide substrates)\n",
        "merged_pdb_file = 'protein_ligand_merged.pdb' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### 3. Active Site Cutoffs\n",
        "cutoff_CA = 7.0 #@param {type:\"number\"}\n",
        "cutoff_sc = 6.0 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pyrosetta\n",
        "# Import the helper scripts you uploaded\n",
        "!wget -q -O parser_tools.py https://raw.githubusercontent.com/pb3lab/AI4PD_2025/refs/heads/main/tutorials/native_enzdes/parser_tools.py\n",
        "!wget -q -O setup_fixed_positions_around_target.py https://raw.githubusercontent.com/pb3lab/AI4PD_2025/refs/heads/main/tutorials/native_enzdes/setup_fixed_positions_around_target.py\n",
        "import parser_tools\n",
        "import setup_fixed_positions_around_target\n",
        "\n",
        "# List to hold paths to any .params files (will be empty)\n",
        "params = []\n",
        "\n",
        "# --- 1A. Fix Ligand Chain ID ---\n",
        "# This step is still necessary to fix the chain ID from the previous script\n",
        "fixed_ligand_pdb_file = ligand_pdb_file.replace(\".pdb\", \"_fixed.pdb\")\n",
        "print(f\"Fixing chain ID in {ligand_pdb_file} -> {fixed_ligand_pdb_file}...\")\n",
        "try:\n",
        "    with open(ligand_pdb_file, 'r') as infile, open(fixed_ligand_pdb_file, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            if line.startswith(\"ATOM\") or line.startswith(\"HETATM\"):\n",
        "                if line[21] != ligand_chain_letter:\n",
        "                    line = line[:21] + ligand_chain_letter + line[22:]\n",
        "            outfile.write(line)\n",
        "    print(f\"âœ… Ligand PDB chain ID corrected.\")\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ”¥ Error fixing ligand PDB file: {e}. Using original file.\")\n",
        "    fixed_ligand_pdb_file = ligand_pdb_file # Revert to original if fix fails\n",
        "\n",
        "\n",
        "# --- 1B. Remove .params file generation ---\n",
        "# This entire step is removed as it's not necessary for a peptide substrate\n",
        "print(\"Skipping .params generation (not needed for peptide substrate).\")\n",
        "\n",
        "\n",
        "# --- 2. Merge Protein and Ligand PDBs (with TER card) ---\n",
        "print(f\"Merging {protein_pdb_file} and {fixed_ligand_pdb_file} into {merged_pdb_file}...\")\n",
        "# --- THIS IS THE FIX ---\n",
        "# We add an \"TER\" card to explicitly separate the chains\n",
        "!cat {protein_pdb_file} > {merged_pdb_file}\n",
        "!echo \"TER\" >> {merged_pdb_file}\n",
        "!cat {fixed_ligand_pdb_file} >> {merged_pdb_file}\n",
        "# --- END FIX ---\n",
        "\n",
        "if not os.path.isfile(merged_pdb_file):\n",
        "    print(f\"ðŸ”¥ Error: Failed to create merged file {merged_pdb_file}\")\n",
        "else:\n",
        "    print(f\"âœ… Merged file created with TER card.\")\n",
        "\n",
        "\n",
        "# --- 3. Initialize PyRosetta ---\n",
        "# No 'extra_res_fa' needed, as we have no .params files\n",
        "init_flags = f\"-beta -run:preserve_header -ignore_unrecognized_res\"\n",
        "print(f\"Initializing PyRosetta with flags: {init_flags}\")\n",
        "pyrosetta.init(init_flags)\n",
        "\n",
        "\n",
        "# --- 4. Load Pose and Find Active Site ---\n",
        "try:\n",
        "    input_pdb = merged_pdb_file\n",
        "    print(f\"Loading pose from {input_pdb}...\")\n",
        "    pose = pyrosetta.pose_from_file(input_pdb)\n",
        "\n",
        "    target_name = protein_pdb_file.split('.')[0]\n",
        "    print(f\"Parsing pose for target: {target_name}\")\n",
        "    parsed_pdb = parser_tools.parse_pose(pose, target_name, params) # params is empty, which is fine\n",
        "\n",
        "    # --- Manually find \"ligand\" (substrate) residues by chain letter ---\n",
        "    print(f\"Manually finding substrate residues for chain: {ligand_chain_letter}...\")\n",
        "    ligand_resno = []\n",
        "    for res in pose.residues:\n",
        "        res_chain_letter = pose.pdb_info().chain(res.seqpos())\n",
        "        if res_chain_letter == ligand_chain_letter:\n",
        "            ligand_resno.append(res.seqpos())\n",
        "\n",
        "    if not ligand_resno:\n",
        "         print(f\"ðŸ”¥ Error: No residues found on chain '{ligand_chain_letter}'.\")\n",
        "    else:\n",
        "         print(f\"âœ… Substrate residues (from chain {ligand_chain_letter}) found at: {ligand_resno}\")\n",
        "    # --- End ---\n",
        "\n",
        "    # Have rosetta find your active site for you\n",
        "    print(f\"Finding active site (CA cutoff: {cutoff_CA}, SC cutoff: {cutoff_sc})...\")\n",
        "    # This function will now work because ligand_resno is correctly populated\n",
        "    fixed_res_per_chain = setup_fixed_positions_around_target.get_fixed_positions(\n",
        "        input_pdb, pose, ligand_resno,\n",
        "        cutoff_CA=cutoff_CA, cutoff_sc=cutoff_sc\n",
        "    )\n",
        "\n",
        "    # Get the active site residues from the *protein chain*\n",
        "    active_site = np.array(fixed_res_per_chain[protein_chain_letter])\n",
        "\n",
        "    print(\"\\n--- Active Site Analysis Complete ---\")\n",
        "    print(f\"Active site residues found in chain {protein_chain_letter}:\")\n",
        "    print(active_site)\n",
        "    print(f\"Total: {len(active_site)} residues.\")\n",
        "\n",
        "except KeyError as e:\n",
        "    print(f\"ðŸ”¥ A KeyError occurred. This likely means the protein chain letter '{protein_chain_letter}' was not found in the results.\")\n",
        "    print(f\"   Chains found in results: {list(fixed_res_per_chain.keys())}\")\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ”¥ An error occurred during pose processing: {e}\")"
      ],
      "metadata": {
        "id": "7Q3yqLnD0o_M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Generate MSA (.a3m) and optionally filter based on different parameters\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Specify Input\n",
        "#@markdown Enter the name of the FASTA file you generated.\n",
        "input_fasta_file = '1LVM_A.fasta' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 2. HHfilter Options\n",
        "#@markdown Check the box to run hhfilter on the generated .a3m file.\n",
        "run_hhfilter = True #@param {type:\"boolean\"}\n",
        "id_redundancy = 90 #@param {type:\"integer\"}\n",
        "coverage = 50 #@param {type:\"integer\"}\n",
        "query_identity = 30 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. Check if the input FASTA file exists\n",
        "if not os.path.isfile(input_fasta_file):\n",
        "    print(f\"ðŸ”¥ Error: Input file not found: {input_fasta_file}\")\n",
        "    print(\"Please make sure the filename matches the one from the previous step.\")\n",
        "    sys.exit(f\"File not found: {input_fasta_file}\")\n",
        "else:\n",
        "    print(f\"Found input file: {input_fasta_file}\")\n",
        "\n",
        "# 2. Define the output directory as the current folder\n",
        "output_dir = \".\" # This will save files to /content/\n",
        "\n",
        "# 3. Run the colabfold_batch command\n",
        "print(f\"Running colabfold_batch on {input_fasta_file}...\")\n",
        "print(f\"This will generate the .a3m file and then stop.\")\n",
        "\n",
        "# Run the alignment generation\n",
        "!colabfold_batch {input_fasta_file} {output_dir} --msa-mode \"mmseqs2_uniref_env\" --msa-only\n",
        "\n",
        "# 4. Check the output files from colabfold_batch\n",
        "print(\"\\nAlignment generation complete.\")\n",
        "\n",
        "# --- 5. Run HHfilter (New Step) ---\n",
        "base_name = os.path.splitext(input_fasta_file)[0]\n",
        "original_a3m = f\"{base_name}.a3m\"\n",
        "filtered_a3m = f\"{base_name}.filtered.a3m\"\n",
        "\n",
        "if run_hhfilter:\n",
        "    print(f\"\\nRunning hhfilter on {original_a3m}...\")\n",
        "    if not os.path.isfile(original_a3m):\n",
        "        print(f\"ðŸ”¥ Error: The original alignment file {original_a3m} was not found. Skipping filter.\")\n",
        "    else:\n",
        "        # Build and run the hhfilter command\n",
        "        !hhfilter -i {original_a3m} -o {filtered_a3m} -id {id_redundancy} -cov {coverage} -qid {query_identity}\n",
        "        print(f\"Filtering complete. Filtered file saved as: {filtered_a3m}\")\n",
        "else:\n",
        "    print(\"\\nSkipping hhfilter step.\")\n",
        "\n",
        "# --- 6. Find and report the final .a3m file ---\n",
        "print(\"\\n--- Final Output ---\")\n",
        "try:\n",
        "    if run_hhfilter and os.path.isfile(filtered_a3m):\n",
        "        print(f\"âœ… Your FINAL filtered alignment file is ready: {filtered_a3m}\")\n",
        "    elif os.path.isfile(original_a3m):\n",
        "        print(f\"âœ… Your original (unfiltered) alignment file is ready: {original_a3m}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Error: No .a3m file ({original_a3m} or {filtered_a3m}) was found.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâš ï¸ Error finding .a3m file: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SFSdQBD4Aw3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4) Find Conserved Residues based on the MSA\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Specify Input Files\n",
        "#@markdown Enter the name of the filtered .a3m file (from previous step).\n",
        "filtered_a3m_file = '1LVM_A.filtered.a3m' #@param {type:\"string\"}\n",
        "#@markdown\n",
        "#@markdown ### 2. Conservation Settings\n",
        "#@markdown Fraction of most-conserved residues to select (e.g., 0.5 = 50%).\n",
        "frac_conserved = 0.7 #@param {type:\"number\"}\n",
        "#@markdown\n",
        "#@markdown ### 3. Output File\n",
        "#@markdown Name for the final .txt file containing the fixed positions.\n",
        "output_txt_file = \"fixed_positions.txt\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Self-contained A3M Parser (Replaces tools.py) ---\n",
        "def robust_parse_a3m(a3m_file_path):\n",
        "    \"\"\"\n",
        "    Parses an .a3m file, removes insertions (lowercase), and converts to numbers.\n",
        "    This version correctly skips header lines before the first sequence.\n",
        "    \"\"\"\n",
        "    # Mapping from AA to number (0-19 are AA, 20 is gap)\n",
        "    aa_to_num = {\n",
        "        'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9,\n",
        "        'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19,\n",
        "        '-': 20, 'X': 20, 'B': 20, 'Z': 20 # Treat unknowns as gaps\n",
        "    }\n",
        "\n",
        "    msa_sequences = []\n",
        "    seq = \"\"\n",
        "    found_first_header = False # Flag to skip junk lines at the start\n",
        "\n",
        "    with open(a3m_file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith('>'):\n",
        "                found_first_header = True # We've found the first sequence, start parsing\n",
        "                if seq: # save previous sequence\n",
        "                    msa_sequences.append(seq)\n",
        "                seq = \"\" # start new sequence\n",
        "            elif found_first_header and line: # Only append if we're parsing and line is not empty\n",
        "                seq += line\n",
        "\n",
        "    if seq: # save last sequence\n",
        "        msa_sequences.append(seq)\n",
        "\n",
        "    if not msa_sequences:\n",
        "        raise ValueError(f\"No sequences found in {a3m_file_path}. File might be malformed.\")\n",
        "\n",
        "    # Get query length from the first sequence (which is 1LVM_A)\n",
        "    query_seq = msa_sequences[0]\n",
        "    query_L = len([c for c in query_seq if c.isupper() or c == '-'])\n",
        "\n",
        "    if query_L == 0:\n",
        "        raise ValueError(f\"Query sequence found, but it has no match/delete characters (Length is 0).\")\n",
        "\n",
        "    msa_aligned = []\n",
        "    for seq in msa_sequences:\n",
        "        aligned_seq = \"\"\n",
        "        for char in seq:\n",
        "            # Keep only uppercase (match) and gaps (delete)\n",
        "            if char.isupper() or char == '-':\n",
        "                aligned_seq += char\n",
        "\n",
        "        # Ensure all sequences have the same length as the query\n",
        "        if len(aligned_seq) == query_L:\n",
        "            msa_aligned.append(aligned_seq)\n",
        "\n",
        "    if not msa_aligned:\n",
        "         raise ValueError(f\"No valid, aligned sequences found in {a3m_file_path}.\")\n",
        "\n",
        "    # Convert to numbers\n",
        "    msa_numeric = []\n",
        "    for seq in msa_aligned:\n",
        "        num_seq = [aa_to_num.get(char.upper(), 20) for char in seq] # .upper() for safety\n",
        "        msa_numeric.append(num_seq)\n",
        "\n",
        "    return {\n",
        "        'msa': np.array(msa_aligned),\n",
        "        'msa_num': np.array(msa_numeric, dtype=int)\n",
        "    }\n",
        "# --- End of Parser ---\n",
        "\n",
        "\n",
        "# --- 2. Check for 'active_site' variable ---\n",
        "if 'active_site' not in locals():\n",
        "    print(\"ðŸ”¥ Error: The 'active_site' list was not found.\")\n",
        "    print(\"   Please re-run the 'Define Active Site Around Ligand' cell first.\")\n",
        "    sys.exit(1)\n",
        "else:\n",
        "    print(f\"Found active_site list with {len(active_site)} residues.\")\n",
        "\n",
        "# --- 3. Run Conservation Analysis ---\n",
        "try:\n",
        "    if not os.path.isfile(filtered_a3m_file):\n",
        "        print(f\"ðŸ”¥ Error: The alignment file '{filtered_a3m_file}' was not found.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Parsing alignment file: {filtered_a3m_file}...\")\n",
        "    # Use our new robust parser\n",
        "    aln = robust_parse_a3m(filtered_a3m_file)\n",
        "\n",
        "    msa_num = aln['msa_num']\n",
        "    L = msa_num.shape[1] # Get length (L) from the numeric array\n",
        "    num_seqs = msa_num.shape[0]\n",
        "\n",
        "    if num_seqs == 0 or L == 0:\n",
        "        print(f\"ðŸ”¥ Error: The alignment file '{filtered_a3m_file}' contains no valid sequences or has length 0.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"Alignment loaded. Length: {L}, Number of sequences: {num_seqs}\")\n",
        "\n",
        "    # Your code snippet, now safer\n",
        "    counts = np.stack([np.bincount(column, minlength=21) for column in msa_num.T]).T\n",
        "    max_count = np.max(counts, axis=0)\n",
        "\n",
        "    freq = counts / num_seqs\n",
        "\n",
        "    # Handle columns that are 100% gaps (to avoid divide-by-zero)\n",
        "    freq_sum = freq[:20].sum(axis=0)\n",
        "    freq_sum[freq_sum == 0] = 1.0 # Set sum to 1.0 to prevent error\n",
        "\n",
        "    freq_norm = freq[:20] / freq_sum\n",
        "    max_freq_norm = np.max(freq_norm, axis=0)\n",
        "\n",
        "    # Only apply low-count penalty if we have more than one sequence\n",
        "    if num_seqs > 1:\n",
        "        max_freq_norm[max_count < 10] = 0 # Don't choose positions with low counts\n",
        "    else:\n",
        "        print(\"Only 1 sequence found, skipping low-count filter.\")\n",
        "\n",
        "    # Save the conserved residues as a list\n",
        "    num_conserved = int(L * frac_conserved)\n",
        "    conserved_residues = np.argsort(max_freq_norm)[::-1][:num_conserved] + 1 # make 1-indexed\n",
        "    conserved_residues.sort()\n",
        "\n",
        "    print(f\"Found {len(conserved_residues)} conserved residues (top {frac_conserved*100}%).\")\n",
        "\n",
        "    # --- 4. Intersect with Active Site ---\n",
        "    fixed_positions = np.union1d(conserved_residues, active_site)\n",
        "    fixed_positions.sort()\n",
        "\n",
        "    print(\"\\n--- Final Results ---\")\n",
        "    print(f\"Active Site Residues ({len(active_site)}):\")\n",
        "    print(active_site)\n",
        "    print(f\"Conserved Residues ({len(conserved_residues)}):\")\n",
        "    print(conserved_residues)\n",
        "    print(f\"All Fixed Positions) ({len(fixed_positions)}):\")\n",
        "    print(fixed_positions)\n",
        "\n",
        "    # --- 5. Save Final List to TXT File ---\n",
        "    print(f\"\\nSaving final fixed positions to {output_txt_file}...\")\n",
        "    fixed_positions_str = [str(res) for res in fixed_positions]\n",
        "\n",
        "    with open(output_txt_file, 'w') as f:\n",
        "        f.write(' '.join(fixed_positions_str))\n",
        "\n",
        "    print(f\"âœ… Successfully saved fixed positions to {output_txt_file}.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ðŸ”¥ Error: The alignment file '{filtered_a3m_file}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ”¥ An error occurred: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VvF_lX9PZwBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. Run ProteinMPNN with evolutionary information"
      ],
      "metadata": {
        "id": "wekZ4-aIEVtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#@markdown ## 1) Setting up ProteinMPNN with fixed positions\n",
        "#@markdown ### 1. Input PDB\n",
        "#@markdown Path to the PDB file (must be in your Colab folder).\n",
        "pdb_path = \"1LVM_A.pdb\" #@param {type:\"string\"}\n",
        "\n",
        "homomer = False #@param {type:\"boolean\"}\n",
        "designed_chain = \"A\" #@param {type:\"string\"}\n",
        "fixed_chain = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if designed_chain == \"\":\n",
        "  designed_chain_list = []\n",
        "else:\n",
        "  designed_chain_list = re.sub(\"[^A-Za-z]+\",\",\", designed_chain).split(\",\")\n",
        "\n",
        "if fixed_chain == \"\":\n",
        "  fixed_chain_list = []\n",
        "else:\n",
        "  fixed_chain_list = re.sub(\"[^A-Za-z]+\",\",\", fixed_chain).split(\",\")\n",
        "\n",
        "chain_list = list(set(designed_chain_list + fixed_chain_list))\n",
        "\n",
        "#@markdown - `designed_chain`: Chain(s) to design (e.g., \"A\").\n",
        "#@markdown - `fixed_chain`: Chain(s) to keep fixed (e.g., \"C\").\n",
        "\n",
        "#@markdown ### 2. Design Options\n",
        "#@markdown Number of sequences to generate.\n",
        "num_seqs = 4 #@param {type:\"integer\"}\n",
        "num_seq_per_target = num_seqs\n",
        "\n",
        "#@markdown - Sampling temperature for amino acids, T=0.0 means taking argmax, T>>1.0 means sample randomly.\n",
        "sampling_temp = \"0.2\" #@param [\"0.0001\", \"0.1\", \"0.15\", \"0.2\", \"0.25\", \"0.3\", \"0.5\"]\n",
        "\n",
        "#@markdown - `omit_AAs`: Specify amino acids to omit (e.g., \"XC\" to omit Cys and Unknown).\n",
        "omit_AAs = \"XC\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### 3. Fixed Positions (Optional)\n",
        "#@markdown Provide the .txt file of conserved/active site residues to fix.\n",
        "fixed_positions_file = \"fixed_positions.txt\" #@param {type:\"string\"}\n",
        "#@markdown The chain these fixed positions apply to.\n",
        "fixed_positions_chain = \"A\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "# --- (Rest of your script's parameters) ---\n",
        "save_score=0\n",
        "save_probs=0\n",
        "score_only=0\n",
        "conditional_probs_only=0\n",
        "conditional_probs_only_backbone=0\n",
        "batch_size=1\n",
        "max_length=20000\n",
        "out_folder='.'\n",
        "jsonl_path=''\n",
        "# omit_AAs='X'  <--- THIS LINE IS NOW DELETED (replaced by the form variable)\n",
        "pssm_multi=0.0\n",
        "pssm_threshold=0.0\n",
        "pssm_log_odds_flag=0\n",
        "pssm_bias_flag=0\n",
        "\n",
        "##############################################################\n",
        "\n",
        "folder_for_outputs = out_folder\n",
        "\n",
        "NUM_BATCHES = num_seq_per_target//batch_size\n",
        "BATCH_COPIES = batch_size\n",
        "temperatures = [float(item) for item in sampling_temp.split()]\n",
        "omit_AAs_list = omit_AAs\n",
        "alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
        "\n",
        "omit_AAs_np = np.array([AA in omit_AAs_list for AA in alphabet]).astype(np.float32)\n",
        "\n",
        "chain_id_dict = None\n",
        "fixed_positions_dict = None\n",
        "pssm_dict = None\n",
        "omit_AA_dict = None\n",
        "bias_AA_dict = None\n",
        "tied_positions_dict = None\n",
        "bias_by_res_dict = None\n",
        "bias_AAs_np = np.zeros(len(alphabet))\n",
        "\n",
        "# --- New code to read fixed_positions.txt ---\n",
        "if fixed_positions_file and fixed_positions_chain:\n",
        "    try:\n",
        "        with open(fixed_positions_file, 'r') as f:\n",
        "            # Read the space-separated list of numbers\n",
        "            residues_to_fix = [int(res) for res in f.read().split()]\n",
        "\n",
        "        if residues_to_fix:\n",
        "            # Get the PDB name (basename without .pdb)\n",
        "            pdb_name = os.path.basename(pdb_path).replace('.pdb', '')\n",
        "\n",
        "            # Build the dictionary in the format MPNN expects\n",
        "            fixed_positions_dict = {\n",
        "                pdb_name: {\n",
        "                    fixed_positions_chain: residues_to_fix\n",
        "                }\n",
        "            }\n",
        "            print(f\"âœ… Successfully read {len(residues_to_fix)} fixed positions for chain {fixed_positions_chain} from {fixed_positions_file}.\")\n",
        "            print(f\"Fixed positions: {residues_to_fix}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ {fixed_positions_file} was found but is empty. No specific residues fixed.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âš ï¸ {fixed_positions_file} not found. No specific residues fixed (besides 'fixed_chain').\")\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸ”¥ Error reading {fixed_positions_file}: {e}\")\n",
        "else:\n",
        "    print(\"No fixed positions file provided. Only 'fixed_chain' (if any) will be fixed.\")\n",
        "# --- End new code ---\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# Check if PDB file exists before proceeding\n",
        "if not os.path.isfile(pdb_path):\n",
        "    print(f\"ðŸ”¥ Error: PDB file not found at {pdb_path}\")\n",
        "    print(\"Please make sure the file is in your Colab folder.\")\n",
        "else:\n",
        "    pdb_dict_list = parse_PDB(pdb_path, input_chain_list=chain_list)\n",
        "    dataset_valid = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=max_length)\n",
        "\n",
        "    chain_id_dict = {}\n",
        "    chain_id_dict[pdb_dict_list[0]['name']]= (designed_chain_list, fixed_chain_list)\n",
        "\n",
        "    print(f\"\\nChain assignments: {chain_id_dict}\")\n",
        "    for chain in chain_list:\n",
        "      l = len(pdb_dict_list[0][f\"seq_chain_{chain}\"])\n",
        "      print(f\"Length of chain {chain} is {l}\")\n",
        "\n",
        "    if homomer:\n",
        "      tied_positions_dict = make_tied_positions_for_homomers(pdb_dict_list)\n",
        "    else:\n",
        "      tied_positions_dict = None"
      ],
      "metadata": {
        "cellView": "form",
        "id": "agHE5Szc-yFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Run ProteinMPNN with fixed positions\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Output FASTA File\n",
        "output_fasta_file = \"generated_sequences.fasta\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 2. A3M Generation\n",
        "source_a3m_file = \"1LVM_A.filtered.a3m\"          #@param {type:\"string\"}\n",
        "output_msa_folder = \"msa\"                        #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os, copy, torch, numpy as np\n",
        "\n",
        "# --- 1. Read base alignment ---\n",
        "os.makedirs(output_msa_folder, exist_ok=True)\n",
        "alignment_body_lines = []\n",
        "try:\n",
        "    with open(source_a3m_file, 'r') as f:\n",
        "        found_first_seq = False\n",
        "        for line in f:\n",
        "            line = line.rstrip('\\n\\r') + '\\n'  # normalize newlines\n",
        "            if line.startswith('>'):\n",
        "                if not found_first_seq:\n",
        "                    found_first_seq = True\n",
        "                    continue  # skip query header\n",
        "                alignment_body_lines.append(line)\n",
        "            elif found_first_seq:\n",
        "                alignment_body_lines.append(line)\n",
        "    print(f\"Read {len(alignment_body_lines)//2} alignment hits from {source_a3m_file}.\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ WARNING reading {source_a3m_file}: {e}\")\n",
        "\n",
        "def _alignment_tail_to_write(alignment_lines):\n",
        "    \"\"\"Return alignment hits excluding the query sequence.\"\"\"\n",
        "    if not alignment_lines:\n",
        "        return []\n",
        "    if not alignment_lines[0].startswith('>'):\n",
        "        return alignment_lines[1:]\n",
        "    return alignment_lines\n",
        "\n",
        "# --- 2. Generate sequences ---\n",
        "with torch.no_grad():\n",
        "    print('Generating sequences...')\n",
        "    print(f\"Saving generated sequences to: {output_fasta_file}\")\n",
        "    with open(output_fasta_file, 'w') as fasta_f:\n",
        "        for ix, protein in enumerate(dataset_valid):\n",
        "            score_list, all_probs_list, all_log_probs_list, S_sample_list = [], [], [], []\n",
        "            batch_clones = [copy.deepcopy(protein) for _ in range(BATCH_COPIES)]\n",
        "            X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta = tied_featurize(\n",
        "                batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict,\n",
        "                pssm_dict, bias_by_res_dict)\n",
        "            pssm_log_odds_mask = (pssm_log_odds_all > pssm_threshold).float()\n",
        "            name_ = batch_clones[0]['name']\n",
        "\n",
        "            randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
        "            log_probs = model(X, S, mask, chain_M * chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
        "            mask_for_loss = mask * chain_M * chain_M_pos\n",
        "            native_score = _scores(S, log_probs, mask_for_loss).cpu().data.numpy()\n",
        "\n",
        "            for temp in temperatures:\n",
        "                for j in range(NUM_BATCHES):\n",
        "                    randn_2 = torch.randn(chain_M.shape, device=X.device)\n",
        "                    if tied_positions_dict is None:\n",
        "                        sample_dict = model.sample(\n",
        "                            X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp,\n",
        "                            omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos,\n",
        "                            omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias,\n",
        "                            pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag),\n",
        "                            pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag),\n",
        "                            bias_by_res=bias_by_res_all)\n",
        "                        S_sample = sample_dict[\"S\"]\n",
        "                    else:\n",
        "                        sample_dict = model.tied_sample(\n",
        "                            X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp,\n",
        "                            omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos,\n",
        "                            omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias,\n",
        "                            pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag),\n",
        "                            pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag),\n",
        "                            tied_pos=tied_pos_list_of_lists_list[0], tied_beta=tied_beta,\n",
        "                            bias_by_res=bias_by_res_all)\n",
        "                        S_sample = sample_dict[\"S\"]\n",
        "\n",
        "                    log_probs = model(X, S_sample, mask, chain_M * chain_M_pos,\n",
        "                                      residue_idx, chain_encoding_all, randn_2,\n",
        "                                      use_input_decoding_order=True,\n",
        "                                      decoding_order=sample_dict[\"decoding_order\"])\n",
        "                    mask_for_loss = mask * chain_M * chain_M_pos\n",
        "                    scores = _scores(S_sample, log_probs, mask_for_loss).cpu().data.numpy()\n",
        "\n",
        "                    for b_ix in range(BATCH_COPIES):\n",
        "                        masked_chain_length_list = masked_chain_length_list_list[b_ix]\n",
        "                        masked_list = masked_list_list[b_ix]\n",
        "                        seq_recovery_rate = torch.sum(\n",
        "                            torch.sum(torch.nn.functional.one_hot(S[b_ix],21)\n",
        "                                      * torch.nn.functional.one_hot(S_sample[b_ix],21), axis=-1)\n",
        "                            * mask_for_loss[b_ix]) / torch.sum(mask_for_loss[b_ix])\n",
        "                        seq = _S_to_seq(S_sample[b_ix], chain_M[b_ix])\n",
        "                        score = scores[b_ix]\n",
        "                        native_seq = _S_to_seq(S[b_ix], chain_M[b_ix])\n",
        "\n",
        "                        # --- Native written once ---\n",
        "                        if b_ix == 0 and j == 0 and temp == temperatures[0]:\n",
        "                            native_seq = \"\".join(native_seq.split(\"/\")).strip()\n",
        "                            native_score_print = np.format_float_positional(np.float32(native_score.mean()), unique=False, precision=4)\n",
        "                            line = f\">native, score={native_score_print}\\n{native_seq}\\n\"\n",
        "                            fasta_f.write(line)\n",
        "                            print(line.rstrip())\n",
        "\n",
        "                            a3m_filename = os.path.join(output_msa_folder, f\"{name_}_native.a3m\")\n",
        "                            try:\n",
        "                                with open(a3m_filename, 'w') as a3m_f:\n",
        "                                    native_len = len(native_seq)\n",
        "                                    a3m_f.write(f\"#{native_len}\\t1\\n>native\\n{native_seq}\\n\")\n",
        "                                    tail = _alignment_tail_to_write(alignment_body_lines)\n",
        "                                    a3m_f.writelines(tail)\n",
        "                                print(f\"Wrote native A3M: {a3m_filename}\")\n",
        "                            except Exception as e:\n",
        "                                print(f\"âš ï¸ Warning: Could not write native A3M {a3m_filename}. Error: {e}\")\n",
        "\n",
        "                        # --- Generated sequences ---\n",
        "                        seq = \"\".join(seq.split(\"/\")).strip()\n",
        "                        score_print = np.format_float_positional(np.float32(score), unique=False, precision=4)\n",
        "                        seq_rec_print = np.format_float_positional(np.float32(seq_recovery_rate.detach().cpu().numpy()), unique=False, precision=4)\n",
        "                        sample_index = j * BATCH_COPIES + b_ix + 1\n",
        "                        line = f\">sample{sample_index}, T={temp}, score={score_print}, seq_recovery={seq_rec_print}\\n{seq}\\n\"\n",
        "                        fasta_f.write(line)\n",
        "                        print(line.rstrip())\n",
        "\n",
        "                        sample_name = f\"sample{sample_index}\"\n",
        "                        a3m_filename = os.path.join(output_msa_folder, f\"{name_}_{sample_name}.a3m\")\n",
        "                        try:\n",
        "                            with open(a3m_filename, 'w') as a3m_f:\n",
        "                                seq_len = len(seq)\n",
        "                                a3m_f.write(f\"#{seq_len}\\t1\\n>{sample_name}\\n{seq}\\n\")\n",
        "                                tail = _alignment_tail_to_write(alignment_body_lines)\n",
        "                                a3m_f.writelines(tail)\n",
        "                            print(f\"Wrote sample A3M: {a3m_filename}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"âš ï¸ Warning: Could not write sample A3M {a3m_filename}. Error: {e}\")\n",
        "\n",
        "    print(f\"\\nâœ… All sequences saved to {output_fasta_file} and A3Ms in '{output_msa_folder}'.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w1qviwvLAyim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Visualize the generated sequences using an MSA Viewer in Google Colab\n",
        "#The following code is modified from the wonderful viewer developed by Damien Farrell\n",
        "#https://dmnfarrell.github.io/bioinformatics/bokeh-sequence-aligner\n",
        "\n",
        "#Importing all modules first\n",
        "import os, io, random\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from Bio.Seq import Seq\n",
        "from Bio.Align import MultipleSeqAlignment\n",
        "from Bio import AlignIO, SeqIO\n",
        "\n",
        "import panel as pn\n",
        "import panel.widgets as pnw\n",
        "pn.extension()\n",
        "\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.models import ColumnDataSource, Plot, Grid, Range1d\n",
        "from bokeh.models.glyphs import Text, Rect\n",
        "from bokeh.layouts import gridplot\n",
        "\n",
        "#Setting up the amino color code according to Zappo color scheme\n",
        "def get_colors(seqs):\n",
        "    #make colors for bases in sequence\n",
        "    text = [i for s in list(seqs) for i in s]\n",
        "    #Use Zappo color scheme\n",
        "    clrs =  {'K':'red',\n",
        "             'R':'red',\n",
        "             'H':'red',\n",
        "             'D':'green',\n",
        "             'E':'green',\n",
        "             'Q':'blue',\n",
        "             'N':'blue',\n",
        "             'S':'blue',\n",
        "             'T':'blue',\n",
        "             'A':'blue',\n",
        "             'I':'blue',\n",
        "             'L':'blue',\n",
        "             'M':'blue',\n",
        "             'V':'blue',\n",
        "             'F':'orange',\n",
        "             'Y':'orange',\n",
        "             'W':'orange',\n",
        "             'C':'blue',\n",
        "             'P':'yellow',\n",
        "             'G':'orange',\n",
        "             '-':'white'}\n",
        "    colors = [clrs[i] for i in text]\n",
        "    return colors\n",
        "\n",
        "#Setting up the MSA viewer\n",
        "def view_alignment(aln, fontsize=\"9pt\", plot_width=800):\n",
        "    \"\"\"Bokeh sequence alignment view\"\"\"\n",
        "\n",
        "    #make sequence and id lists from the aln object\n",
        "    seqs = [rec.seq for rec in (aln)]\n",
        "    ids = [rec.id for rec in aln]\n",
        "    text = [i for s in list(seqs) for i in s]\n",
        "    colors = get_colors(seqs)\n",
        "    N = len(seqs[0])\n",
        "    S = len(seqs)\n",
        "    width = .4\n",
        "\n",
        "    x = np.arange(1,N+1)\n",
        "    y = np.arange(0,S,1)\n",
        "    #creates a 2D grid of coords from the 1D arrays\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    #flattens the arrays\n",
        "    gx = xx.ravel()\n",
        "    gy = yy.flatten()\n",
        "    #use recty for rect coords with an offset\n",
        "    recty = gy+.5\n",
        "    h= 1/S\n",
        "    #now we can create the ColumnDataSource with all the arrays\n",
        "    source = ColumnDataSource(dict(x=gx, y=gy, recty=recty, text=text, colors=colors))\n",
        "    plot_height = len(seqs)*15+50\n",
        "    x_range = Range1d(0,N+1, bounds='auto')\n",
        "    if N>100:\n",
        "        viewlen=100\n",
        "    else:\n",
        "        viewlen=N\n",
        "    #view_range is for the close up view\n",
        "    view_range = (0,viewlen)\n",
        "    tools=\"xpan, xwheel_zoom, reset, save\"\n",
        "\n",
        "    #entire sequence view (no text, with zoom)\n",
        "    p = figure(title=None, width= plot_width, height=50,\n",
        "               x_range=x_range, y_range=(0,S), tools=tools,\n",
        "               min_border=0, toolbar_location='below')\n",
        "    rects = Rect(x=\"x\", y=\"recty\",  width=1, height=1, fill_color=\"colors\",\n",
        "                 line_color=None, fill_alpha=0.6)\n",
        "    p.add_glyph(source, rects)\n",
        "    p.yaxis.visible = False\n",
        "    p.grid.visible = False\n",
        "\n",
        "    #sequence text view with ability to scroll along x axis\n",
        "    p1 = figure(title=None, width=plot_width, height=plot_height,\n",
        "                x_range=view_range, y_range=ids, tools=\"xpan,reset\",\n",
        "                min_border=0, toolbar_location='below')#, lod_factor=1)\n",
        "    glyph = Text(x=\"x\", y=\"y\", text=\"text\", text_align='center',text_color=\"black\",\n",
        "                text_font=\"monospace\",text_font_size=fontsize)\n",
        "    rects = Rect(x=\"x\", y=\"recty\",  width=1, height=1, fill_color=\"colors\",\n",
        "                line_color=None, fill_alpha=0.4)\n",
        "    p1.add_glyph(source, glyph)\n",
        "    p1.add_glyph(source, rects)\n",
        "\n",
        "    p1.grid.visible = False\n",
        "    p1.xaxis.major_label_text_font_style = \"bold\"\n",
        "    p1.yaxis.minor_tick_line_width = 0\n",
        "    p1.yaxis.major_tick_line_width = 0\n",
        "\n",
        "    p = gridplot([[p],[p1]], toolbar_location='below')\n",
        "    return p\n",
        "\n",
        "#Loading the viewer by indicating the MSA file and format to read\n",
        "#@markdown Name of the MSA file (including the filetype)\n",
        "MSAfile = 'generated_sequences.fasta' #@param {type:\"string\"}\n",
        "MSAformat = 'fasta' #@param {type:\"string\"}\n",
        "aln = AlignIO.read(MSAfile,MSAformat)\n",
        "p = view_alignment(aln, plot_width=900)\n",
        "pn.pane.Bokeh(p)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cmDekMMXBjpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3. Predict  structures of the designed sequences with AF2"
      ],
      "metadata": {
        "id": "74-F6pY_Ey1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Run ColabFold using custom MSAs and a single model\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Input / Output Folders\n",
        "msa_dir = 'msa' #@param {type:\"string\"}\n",
        "predictions_dir = 'predictions' #@param {type:\"string\"}\n",
        "csv_output_file = 'confidence_metrics.csv' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Model Settings\n",
        "#@markdown Specify model number(s) to run (e.g., \"3\" or \"1,2,3,4,5\")\n",
        "model_order = \"4\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. Setup directories ---\n",
        "result_dir_path = Path(predictions_dir)\n",
        "msa_dir_path = Path(msa_dir)\n",
        "os.makedirs(result_dir_path, exist_ok=True)\n",
        "\n",
        "# --- 2. Run ColabFold using the simple batch command ---\n",
        "print(f\"ðŸš€ Starting ColabFold batch run...\")\n",
        "print(f\"   Input: {msa_dir}\")\n",
        "print(f\"   Output: {predictions_dir}\")\n",
        "print(f\"   Models: {model_order}\")\n",
        "\n",
        "!colabfold_batch \\\n",
        "  --model-order {model_order} \\\n",
        "  --model-type alphafold2_ptm \\\n",
        "  {msa_dir} \\\n",
        "  {predictions_dir}\n",
        "\n",
        "print(\"\\nâœ… Prediction run complete.\")\n",
        "\n",
        "# --- 3. Gather confidence results ---\n",
        "print(f\"\\nðŸ“Š Parsing results to create {csv_output_file}...\")\n",
        "results = []\n",
        "\n",
        "# --- FIX 1 ---\n",
        "# Search for \"_scores_rank_001_*.json\" instead of \"_unrelaxed_rank_001_*.json\"\n",
        "# Also removed the extra \"*/\" since there are no sub-folders.\n",
        "json_files = sorted(result_dir_path.glob(\"*_scores_rank_001_*.json\"))\n",
        "# --- End FIX 1 ---\n",
        "\n",
        "if not json_files:\n",
        "    print(f\"ðŸ”¥ Error: No JSON result files found in {predictions_dir}.\")\n",
        "    print(\"   Please check if the predictions ran correctly and produced output.\")\n",
        "    print(f\"   (Was looking for files like: *_scores_rank_001_*.json)\")\n",
        "else:\n",
        "    print(f\"   Found {len(json_files)} result files to parse.\")\n",
        "    for jf in json_files:\n",
        "        try:\n",
        "            # --- FIX 2 ---\n",
        "            # Split the filename by \"_scores\" to get the jobname\n",
        "            jobname = jf.name.split(\"_scores\")[0]\n",
        "            # --- End FIX 2 ---\n",
        "\n",
        "            data = json.load(open(jf))\n",
        "\n",
        "            # This part is correct: it calculates the average from the list\n",
        "            avg_plddt = np.mean(data.get(\"plddt\", [])) if data.get(\"plddt\") else None\n",
        "            ptm = data.get(\"ptm\", None)\n",
        "\n",
        "            model_name = \"unknown\"\n",
        "            if \"model_name\" in data:\n",
        "                model_name = data[\"model_name\"]\n",
        "            elif \"model\" in data:\n",
        "                model_name = data[\"model\"]\n",
        "            else:\n",
        "                for i in model_order.split(','):\n",
        "                    if f\"model_{i}\" in jf.name:\n",
        "                        model_name = f\"model_{i}\"\n",
        "                        break\n",
        "\n",
        "            results.append({\n",
        "                \"sequence_name\": jobname,\n",
        "                \"avg_plddt\": avg_plddt,\n",
        "                \"ptm\": ptm,\n",
        "                \"model_used\": model_name\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error parsing {jf.name}: {e}\")\n",
        "\n",
        "    if results:\n",
        "        df = pd.DataFrame(results).sort_values(\"avg_plddt\", ascending=False)\n",
        "        out_path = result_dir_path / csv_output_file\n",
        "        df.to_csv(out_path, index=False)\n",
        "        print(f\"\\nâœ… Successfully saved confidence metrics to: {out_path}\")\n",
        "        print(\"\\n--- Top Results ---\")\n",
        "        print(df.head())\n",
        "    else:\n",
        "        print(\"ðŸ”¥ No results were successfully parsed. Check predictions directory.\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ All done.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uWdgcQcDEuG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2) Calculate RMSD & Save Aligned Structures\n",
        "#@markdown ---\n",
        "#@markdown ### 1. File Locations\n",
        "#@markdown ---\n",
        "#@markdown Path to your single experimental/reference PDB file:\n",
        "experimental_pdb = \"1LVM_A.pdb\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown Folder where ColabFold saved the predictions and CSV:\n",
        "predictions_dir = \"predictions\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown Name of the CSV file to read and update:\n",
        "csv_output_file = \"confidence_metrics.csv\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Folder to save all aligned PDBs for visualization:**\n",
        "aligned_pdb_folder = \"aligned\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Alignment Options\n",
        "#@markdown ---\n",
        "#@markdown Select the method for structural superposition:\n",
        "alignment_mode = \"Specific Residues\" #@param [\"All Atoms\", \"Iterative Exclusion\", \"Specific Residues\"]\n",
        "#@markdown ---\n",
        "#@markdown **For \"Iterative Exclusion\" mode:**\n",
        "#@markdown Cutoff in Ã…. Residues with CÎ± distance > cutoff after alignment will be excluded.\n",
        "iterative_rmsd_cutoff = 2.0 #@param {type:\"number\"}\n",
        "#@markdown Maximum number of iterations to run.\n",
        "iterative_max_cycles = 5 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "#@markdown **For \"Specific Residues\" mode:**\n",
        "#@markdown Provide a comma-separated list of residues or ranges (e.g., \"10-50, 80, 91-100\").\n",
        "residue_list_to_align = \"20-229\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# Try to import Biopython, which is needed for RMSD\n",
        "try:\n",
        "    from Bio.PDB import PDBParser, Superimposer, PDBIO\n",
        "except ImportError:\n",
        "    print(\" Biopython not found. Installing...\")\n",
        "    !pip install biopython\n",
        "    from Bio.PDB import PDBParser, Superimposer, PDBIO\n",
        "\n",
        "# --- Helper Function to Parse Residue List ---\n",
        "def parse_residue_list(res_string):\n",
        "    \"\"\"Parses a residue string like \"10-50, 80, 91-100\" into a set of integers.\"\"\"\n",
        "    residue_set = set()\n",
        "    if not res_string:\n",
        "        return residue_set\n",
        "\n",
        "    parts = res_string.split(',')\n",
        "    for part in parts:\n",
        "        part = part.strip()\n",
        "        if not part:\n",
        "            continue\n",
        "        if '-' in part:\n",
        "            try:\n",
        "                start, end = part.split('-')\n",
        "                start_res = int(start.strip())\n",
        "                end_res = int(end.strip())\n",
        "                residue_set.update(range(start_res, end_res + 1))\n",
        "            except ValueError:\n",
        "                print(f\"âš ï¸ Warning: Could not parse range '{part}'. Skipping.\")\n",
        "        else:\n",
        "            try:\n",
        "                residue_set.add(int(part.strip()))\n",
        "            except ValueError:\n",
        "                print(f\"âš ï¸ Warning: Could not parse residue number '{part}'. Skipping.\")\n",
        "    return residue_set\n",
        "\n",
        "# --- 1. Load Reference Structure & Setup Folders ---\n",
        "print(f\"Loading reference structure: {experimental_pdb}\")\n",
        "pdb_parser = PDBParser(QUIET=True)\n",
        "io = PDBIO() # Initialize PDB saver\n",
        "\n",
        "# Create the output folder for aligned PDBs\n",
        "os.makedirs(aligned_pdb_folder, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    ref_structure = pdb_parser.get_structure(\"reference\", experimental_pdb)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ðŸ”¥ Error: Experimental PDB not found at: {experimental_pdb}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Get all C-alpha atoms as a dictionary, keyed by residue number\n",
        "try:\n",
        "    ref_ca_dict = {\n",
        "        atom.get_parent().id[1]: atom\n",
        "        for atom in ref_structure[0].get_atoms()\n",
        "        if atom.name == \"CA\" and atom.get_parent().id[0] == ' ' # Ensure it's a standard residue\n",
        "    }\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ”¥ Error parsing reference PDB: {e}\")\n",
        "    print(\"   Make sure it's a valid PDB file.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if not ref_ca_dict:\n",
        "    print(f\"ðŸ”¥ Error: No standard C-alpha atoms (CA) found in {experimental_pdb}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"Loaded {len(ref_ca_dict)} C-alpha atoms from reference.\")\n",
        "\n",
        "# --- Save a copy of the reference PDB to the aligned folder ---\n",
        "print(f\"\\nSaving reference structure to {aligned_pdb_folder}...\")\n",
        "io.set_structure(ref_structure)\n",
        "ref_output_name = f\"{Path(experimental_pdb).stem}_ref.pdb\"\n",
        "ref_output_path = os.path.join(aligned_pdb_folder, ref_output_name)\n",
        "io.save(ref_output_path)\n",
        "print(f\"  Saved: {ref_output_path}\")\n",
        "\n",
        "# --- 2. Load CSV File ---\n",
        "csv_path = os.path.join(predictions_dir, csv_output_file)\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ðŸ”¥ Error: CSV file not found at: {csv_path}\")\n",
        "    print(\"   Please run the previous cell to generate the CSV.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 3. Parse alignment residues if needed ---\n",
        "alignment_residue_set = set()\n",
        "if alignment_mode == \"Specific Residues\":\n",
        "    alignment_residue_set = parse_residue_list(residue_list_to_align)\n",
        "    if not alignment_residue_set:\n",
        "        print(f\"ðŸ”¥ Error: 'Specific Residues' mode selected, but no valid residues found in '{residue_list_to_align}'.\")\n",
        "        sys.exit(1)\n",
        "    print(f\"Running in 'Specific Residues' mode. Aligning on {len(alignment_residue_set)} specified residues.\")\n",
        "elif alignment_mode == \"Iterative Exclusion\":\n",
        "    print(f\"Running in 'Iterative Exclusion' mode (Cutoff={iterative_rmsd_cutoff} Ã…, Max Cycles={iterative_max_cycles}).\")\n",
        "else:\n",
        "    print(\"Running in 'All Atoms' mode.\")\n",
        "\n",
        "# --- 4. Loop Through Predictions and Calculate RMSD ---\n",
        "rmsd_list = []\n",
        "aligned_residues_list = []\n",
        "super_imposer = Superimposer()\n",
        "\n",
        "print(\"\\nCalculating RMSD and saving aligned structures...\")\n",
        "for index, row in df.iterrows():\n",
        "    jobname = row['sequence_name']\n",
        "\n",
        "    # Find the corresponding PDB file\n",
        "    pdb_pattern = os.path.join(predictions_dir, f\"{jobname}_unrelaxed_rank_001_*.pdb\")\n",
        "    pdb_files = glob.glob(pdb_pattern)\n",
        "\n",
        "    if not pdb_files:\n",
        "        print(f\"âš ï¸ Warning: No PDB file found for {jobname}. Skipping.\")\n",
        "        rmsd_list.append(np.nan)\n",
        "        aligned_residues_list.append(np.nan)\n",
        "        continue\n",
        "\n",
        "    predicted_pdb_path = pdb_files[0]\n",
        "\n",
        "    try:\n",
        "        # Load the predicted structure and get its C-alpha dict\n",
        "        sample_structure = pdb_parser.get_structure(jobname, predicted_pdb_path)\n",
        "        sample_ca_dict = {\n",
        "            atom.get_parent().id[1]: atom\n",
        "            for atom in sample_structure[0].get_atoms()\n",
        "            if atom.name == \"CA\" and atom.get_parent().id[0] == ' '\n",
        "        }\n",
        "\n",
        "        if not sample_ca_dict:\n",
        "            print(f\"âš ï¸ Warning: No C-alpha atoms found in {jobname}. Skipping.\")\n",
        "            rmsd_list.append(np.nan)\n",
        "            aligned_residues_list.append(np.nan)\n",
        "            continue\n",
        "\n",
        "        # --- Start Alignment Logic ---\n",
        "        rmsd_val = np.nan\n",
        "        num_aligned = 0\n",
        "\n",
        "        if alignment_mode == \"Specific Residues\":\n",
        "            common_res_ids = alignment_residue_set & set(ref_ca_dict.keys()) & set(sample_ca_dict.keys())\n",
        "            if len(common_res_ids) < len(alignment_residue_set):\n",
        "                print(f\"  Info for {jobname}: Using {len(common_res_ids)} common residues out of {len(alignment_residue_set)} requested.\")\n",
        "\n",
        "            if not common_res_ids:\n",
        "                print(f\"  âš ï¸ Warning: No common residues for alignment in {jobname}. Skipping.\")\n",
        "                rmsd_list.append(np.nan)\n",
        "                aligned_residues_list.append(0)\n",
        "                continue\n",
        "\n",
        "            ref_atoms = [ref_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            sample_atoms = [sample_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            num_aligned = len(common_res_ids)\n",
        "\n",
        "            super_imposer.set_atoms(ref_atoms, sample_atoms)\n",
        "            # Apply transformation to the *entire* structure\n",
        "            super_imposer.apply(sample_structure[0].get_atoms())\n",
        "            rmsd_val = super_imposer.rms\n",
        "\n",
        "            print_msg = f\"  âœ… {jobname}: RMSD = {rmsd_val:.3f} Ã… (on {num_aligned} specified residues)\"\n",
        "\n",
        "\n",
        "        elif alignment_mode == \"Iterative Exclusion\":\n",
        "            current_res_ids = set(ref_ca_dict.keys()) & set(sample_ca_dict.keys())\n",
        "\n",
        "            for i in range(iterative_max_cycles):\n",
        "                if not current_res_ids:\n",
        "                    print(f\"  ðŸ”¥ Error for {jobname}: No atoms left to align during iteration.\")\n",
        "                    rmsd_val = np.nan\n",
        "                    break\n",
        "\n",
        "                ref_atoms_subset = [ref_ca_dict[res_id] for res_id in current_res_ids]\n",
        "                sample_atoms_subset = [sample_ca_dict[res_id] for res_id in current_res_ids]\n",
        "\n",
        "                super_imposer.set_atoms(ref_atoms_subset, sample_atoms_subset)\n",
        "                super_imposer.apply(sample_structure[0].get_atoms())\n",
        "                rmsd_val = super_imposer.rms\n",
        "\n",
        "                new_res_ids = set()\n",
        "                for res_id in current_res_ids:\n",
        "                    dist = ref_ca_dict[res_id] - sample_ca_dict[res_id]\n",
        "                    if dist < iterative_rmsd_cutoff:\n",
        "                        new_res_ids.add(res_id)\n",
        "\n",
        "                if len(new_res_ids) == len(current_res_ids):\n",
        "                    print_msg = f\"  âœ… {jobname}: Converged. RMSD = {rmsd_val:.3f} Ã… (on {len(new_res_ids)} core atoms)\"\n",
        "                    num_aligned = len(new_res_ids)\n",
        "                    break\n",
        "\n",
        "                print(f\"    Iter {i+1} for {jobname}: RMSD={rmsd_val:.3f} ({len(current_res_ids)} atoms) -> Removing {len(current_res_ids) - len(new_res_ids)} outliers.\")\n",
        "                current_res_ids = new_res_ids\n",
        "            else:\n",
        "                print_msg = f\"  âš ï¸ {jobname}: Max iterations reached. Final RMSD = {rmsd_val:.3f} Ã… (on {len(current_res_ids)} core atoms)\"\n",
        "                num_aligned = len(current_res_ids)\n",
        "\n",
        "            rmsd_list.append(rmsd_val)\n",
        "            aligned_residues_list.append(num_aligned)\n",
        "\n",
        "        else: # \"All Atoms\" mode (default)\n",
        "            common_res_ids = set(ref_ca_dict.keys()) & set(sample_ca_dict.keys())\n",
        "\n",
        "            ref_atoms = [ref_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            sample_atoms = [sample_ca_dict[res_id] for res_id in common_res_ids]\n",
        "            num_aligned = len(common_res_ids)\n",
        "\n",
        "            if len(ref_atoms) != len(ref_ca_dict) or len(sample_atoms) != len(sample_ca_dict):\n",
        "                 print(f\"  Info for {jobname}: Found {num_aligned} common atoms for alignment.\")\n",
        "\n",
        "            super_imposer.set_atoms(ref_atoms, sample_atoms)\n",
        "            # Apply transformation to the *entire* structure\n",
        "            super_imposer.apply(sample_structure[0].get_atoms())\n",
        "            rmsd_val = super_imposer.rms\n",
        "\n",
        "            print_msg = f\"  âœ… {jobname}: RMSD = {rmsd_val:.3f} Ã… (on {num_aligned} atoms)\"\n",
        "\n",
        "        # --- Save the aligned structure ---\n",
        "        if not np.isnan(rmsd_val):\n",
        "            output_filename = os.path.join(aligned_pdb_folder, f\"{jobname}_aligned.pdb\")\n",
        "            io.set_structure(sample_structure)\n",
        "            io.save(output_filename)\n",
        "            print(print_msg + f\" -> Saved to {output_filename}\")\n",
        "        else:\n",
        "            print(print_msg) # Print error/warning message from loop\n",
        "\n",
        "        rmsd_list.append(rmsd_val)\n",
        "        aligned_residues_list.append(num_aligned)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸ”¥ Error processing {jobname}: {e}\")\n",
        "        rmsd_list.append(np.nan)\n",
        "        aligned_residues_list.append(np.nan)\n",
        "\n",
        "# --- 5. Update DataFrame and Save ---\n",
        "df['rmsd_to_exp (Ã…)'] = rmsd_list\n",
        "df['rmsd_aligned_residues'] = aligned_residues_list\n",
        "df = df.sort_values(\"rmsd_to_exp (Ã…)\", ascending=True)\n",
        "\n",
        "# Save the updated CSV\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Successfully calculated RMSD, saved aligned PDBs to '{aligned_pdb_folder}', and updated {csv_path}.\")\n",
        "print(f\"   Mode used: {alignment_mode}\")\n",
        "print(\"\\n--- Results (Sorted by RMSD) ---\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iSRF6nd0jw0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3) Plot pLDDT, pTM, and RMSD (Interactive, Wide)\n",
        "#@markdown ---\n",
        "#@markdown ### 1. File Locations\n",
        "#@markdown ---\n",
        "#@markdown Folder where your CSV file is located:\n",
        "predictions_dir = \"predictions\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown Name of the CSV file to read:\n",
        "csv_output_file = \"confidence_metrics.csv\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Name of your native/reference sequence:**\n",
        "#@markdown (This must match the 'sequence_name' in the CSV exactly for red coloring)\n",
        "native_sequence_name = \"1LVM_A_native\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Output\n",
        "#@markdown ---\n",
        "#@markdown Name of the interactive plot file to save (must be .json):\n",
        "plot_output_file_json = \"all_metrics_plot.json\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import altair as alt\n",
        "import sys\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "csv_path = os.path.join(predictions_dir, csv_output_file)\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ðŸ”¥ Error: CSV file not found at: {csv_path}\")\n",
        "    print(\"   Please run the previous cells to generate and update the CSV.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Check if required columns exist\n",
        "required_cols = ['sequence_name', 'avg_plddt', 'ptm', 'rmsd_to_exp (Ã…)']\n",
        "if not all(col in df.columns for col in required_cols):\n",
        "    print(f\"ðŸ”¥ Error: The CSV file is missing one or more required columns.\")\n",
        "    print(f\"   Required: {required_cols}\")\n",
        "    print(f\"   Found: {list(df.columns)}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 2. Prepare Data for Plotting ---\n",
        "\n",
        "# --- NEW: Divide pLDDT by 100 ---\n",
        "print(\"Applying pLDDT / 100 transformation...\")\n",
        "df['avg_plddt'] = df['avg_plddt'] / 100.0\n",
        "# ---\n",
        "\n",
        "# Create the 'model_type' column for coloring\n",
        "if native_sequence_name not in df['sequence_name'].values:\n",
        "    print(f\"âš ï¸ Warning: Native sequence name '{native_sequence_name}' not found in CSV.\")\n",
        "    print(\"   All points will be colored orange.\")\n",
        "    df['model_type'] = \"Designed\"\n",
        "else:\n",
        "    df['model_type'] = np.where(\n",
        "        df['sequence_name'] == native_sequence_name,\n",
        "        'Native',\n",
        "        'Designed'\n",
        "    )\n",
        "\n",
        "# \"Melt\" the DataFrame from wide to long format\n",
        "df_melted = df.melt(\n",
        "    id_vars=['sequence_name', 'model_type'],\n",
        "    value_vars=['avg_plddt', 'ptm', 'rmsd_to_exp (Ã…)'],\n",
        "    var_name='Metric',\n",
        "    value_name='Value'\n",
        ")\n",
        "\n",
        "# --- UPDATED: Clean up metric names for better plot titles ---\n",
        "df_melted['Metric'] = df_melted['Metric'].replace({\n",
        "    'avg_plddt': 'pLDDT / 100',  # <-- Title changed here\n",
        "    'ptm': 'pTM Score',\n",
        "    'rmsd_to_exp (Ã…)': 'RMSD (Ã…)'\n",
        "})\n",
        "\n",
        "# --- 3. Create the Interactive Altair Plot ---\n",
        "print(f\"Generating interactive plot...\")\n",
        "\n",
        "# Define the custom color scale\n",
        "color_scale = alt.Scale(domain=['Native', 'Designed'],\n",
        "                        range=['red', 'orange'])\n",
        "\n",
        "# Create the base chart\n",
        "base = alt.Chart(df_melted).mark_circle(size=80, opacity=0.7).encode(\n",
        "    # X-axis: Native vs. Designed (no title, labels at bottom)\n",
        "    x=alt.X('model_type:N', title=None, axis=alt.Axis(labels=True, ticks=False, title=\"\")),\n",
        "\n",
        "    # Y-axis: The metric's value\n",
        "    y=alt.Y('Value:Q', title='Value'),\n",
        "\n",
        "    # Color based on type\n",
        "    color=alt.Color('model_type:N', scale=color_scale, legend=alt.Legend(title=\"Model Type\")),\n",
        "\n",
        "    # Show this information on hover\n",
        "    tooltip=[\n",
        "        alt.Tooltip('sequence_name:N', title='Model'),\n",
        "        alt.Tooltip('Metric:N', title='Metric'),\n",
        "        alt.Tooltip('Value:Q', title='Value', format='.3f') # Use .3f for 0-1 scale\n",
        "    ]\n",
        ").properties(\n",
        "    # --- NEW: Make each plot wider ---\n",
        "    width=100\n",
        ").interactive() # Make the chart interactive (zoom/pan)\n",
        "\n",
        "# Create the final faceted chart\n",
        "chart = base.facet(\n",
        "    # Create one column for each \"Metric\".\n",
        "    # The header for each facet will be the metric's name\n",
        "    column=alt.Column('Metric:N', header=alt.Header(\n",
        "        titleOrient=\"top\",\n",
        "        labelOrient=\"top\"\n",
        "    ))\n",
        ").resolve_scale(\n",
        "    # Make the Y-axis independent for each plot\n",
        "    y='independent'\n",
        ")\n",
        "\n",
        "# --- 4. Save and Display the Plot ---\n",
        "json_path = os.path.join(predictions_dir, plot_output_file_json)\n",
        "chart.save(json_path)\n",
        "\n",
        "print(f\"âœ… Successfully saved interactive plot to: {json_path}\")\n",
        "\n",
        "# Display the chart in the Colab output\n",
        "chart"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8ifkiA22okZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK7X9T44pWb7",
        "cellView": "form"
      },
      "source": [
        "#@title 4) Display the Aligned 3D Structure {run: \"auto\"}\n",
        "import py3Dmol\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from colabfold.colabfold import plot_plddt_legend\n",
        "from colabfold.colabfold import pymol_color_list, alphabet_list\n",
        "import sys # Added for error checking\n",
        "from pathlib import Path\n",
        "\n",
        "#@markdown ### 1. PDB Location\n",
        "#@markdown ---\n",
        "#@markdown Folder where the aligned structures were saved:\n",
        "aligned_structures_folder = \"aligned\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Jobname of the sequence to display:**\n",
        "#@markdown (e.g., \"1LVM_A_native\", \"1LVM_A_sample1\")\n",
        "jobname = \"1LVM_A_sample1\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Display Options\n",
        "#@markdown ---\n",
        "color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
        "show_sidechains = False #@param {type:\"boolean\"}\n",
        "show_mainchains = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown **Overlay the reference structure?**\n",
        "show_reference = True #@param {type:\"boolean\"}\n",
        "#@markdown Path to the saved reference PDB:\n",
        "reference_pdb_path = \"aligned/1LVM_A_ref.pdb\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "# --- Find the aligned PDB file ---\n",
        "pdb_pattern = f\"{aligned_structures_folder}/{jobname}_aligned.pdb\"\n",
        "pdb_file_list = glob.glob(pdb_pattern)\n",
        "\n",
        "def show_pdb(\n",
        "    predicted_pdb_path,\n",
        "    show_reference=False,\n",
        "    reference_pdb_path=None,\n",
        "    show_sidechains=False,\n",
        "    show_mainchains=False,\n",
        "    color=\"lDDT\"\n",
        "):\n",
        "\n",
        "    view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "\n",
        "    # --- 1. Add Reference Structure (if requested) ---\n",
        "    if show_reference:\n",
        "        try:\n",
        "            view.addModel(open(reference_pdb_path,'r').read(),'pdb')\n",
        "            # Style the *first* model (index 0) as gray\n",
        "            view.setStyle({'model': 0}, {'cartoon': {'color': 'gray'}})\n",
        "        except FileNotFoundError:\n",
        "            print(f\"âš ï¸ Warning: Reference PDB not found at {reference_pdb_path}. Skipping.\")\n",
        "\n",
        "    # --- 2. Add Predicted Structure ---\n",
        "    view.addModel(open(predicted_pdb_path,'r').read(),'pdb')\n",
        "    # Style the *last added* model (index -1)\n",
        "    model_style = {'model': -1}\n",
        "\n",
        "    if color == \"lDDT\":\n",
        "        view.setStyle(model_style, {'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
        "    elif color == \"rainbow\":\n",
        "        view.setStyle(model_style, {'cartoon': {'color':'spectrum'}})\n",
        "    elif color == \"chain\":\n",
        "        # Simple chain coloring\n",
        "        view.setStyle(model_style, {'cartoon': {'color':'chain'}})\n",
        "\n",
        "    if show_sidechains:\n",
        "        BB = ['C','O','N']\n",
        "        view.addStyle({'and':[model_style, {'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                            {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "        view.addStyle({'and':[model_style, {'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                            {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "        view.addStyle({'and':[model_style, {'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                            {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    if show_mainchains:\n",
        "        BB = ['C','O','N','CA']\n",
        "        view.addStyle({'and':[model_style, {'atom':BB}]},\n",
        "                            {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "\n",
        "    view.zoomTo()\n",
        "    return view\n",
        "\n",
        "# --- Display the structure ---\n",
        "if not pdb_file_list:\n",
        "    print(f\"ðŸ”¥ Error: Could not find aligned PDB file.\")\n",
        "    print(f\"   Searched for pattern: {pdb_pattern}\")\n",
        "    print(f\"   Please check 'aligned_structures_folder' and 'jobname'.\")\n",
        "    print(f\"   (Did you run the RMSD script to generate the aligned files?)\")\n",
        "else:\n",
        "    pdb_to_show = pdb_file_list[0]\n",
        "    print(f\"Displaying: {pdb_to_show}\")\n",
        "    if show_reference:\n",
        "        print(f\"Overlaying: {reference_pdb_path}\")\n",
        "\n",
        "    view = show_pdb(\n",
        "        pdb_to_show,\n",
        "        show_reference=show_reference,\n",
        "        reference_pdb_path=reference_pdb_path,\n",
        "        show_sidechains=show_sidechains,\n",
        "        show_mainchains=show_mainchains,\n",
        "        color=color\n",
        "    )\n",
        "    view.show()\n",
        "\n",
        "    if color == \"lDDT\":\n",
        "        plot_plddt_legend().show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}